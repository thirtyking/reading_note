---
tags: []
parent: '[Megatron-LM] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism'
collections:
    - 并行计算
$version: 2256
$libraryID: 1
$itemKey: YS6U6GSX

---
\[2019-09]\[Megatron-LM] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

# \[Megatron-LM] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

## 概述

你好！很高兴能为你解读这篇在 AI 大模型时代具有**里程碑意义**的论文——**Megatron-LM**。

你可以把 Megatron-LM 看作是**让 AI 模型突破体型限制的“扩容手术刀”**。在它之前，由于硬件限制，我们很难训练超级巨大的模型；在它之后，训练千亿甚至万亿参数的模型（如 GPT-3）才在工程上成为了可能。

下面我按照你的要求，用通俗的语言为你拆解这篇论文的核心。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**核心问题：显存不够用了，无法训练超大模型。**

*   **背景：** 在深度学习领域，有一个公认的定律：**模型越大，效果越好**（Scaling Laws）。比如从 BERT 到 GPT-2，参数量翻了好几倍，智能程度也直线上升。

*   **瓶颈：** 想要模型更智能，就要增加参数（Parameters，即模型里的权重矩阵）。但是，参数是需要存放在显卡内存（显存）里的。

    *   当模型大到几十亿（Multi-Billion）参数时，**单张显卡的显存根本装不下整个模型**。
    *   一旦装不下，连“启动”都做不到，更别提训练了。

**为什么重要？** 如果不能解决“装不下”的问题，AI 的发展就会被硬件显存锁死，我们永远无法得到像 GPT-3、GPT-4 这样强大的智能体。这篇论文就是为了打破这个天花板。

***

### 2. 解决这个问题的难点在哪里？之前有哪些方案？为什么不行？

**难点：** 如何把一个巨大的模型“切开”，塞进多张显卡里，还能保证它们高效地一起工作？

在 Megatron-LM 之前，主要有两种主流方案，但都有明显缺陷：

*   **方案 A：数据并行 (Data Parallelism)**

    *   **做法：** 每张显卡都复制一份**完整**的模型，然后大家分头读不同的数据（比如显卡1读第一本书，显卡2读第二本书），最后汇总学习成果。
    *   **缺陷：** 这是最常用的方法，但前提是**单张显卡必须能装下整个模型**。如果模型本身比显卡内存还大，这个方案直接失效。

*   **方案 B：流水线并行 (Pipeline Parallelism)**

    *   **做法：** 把模型按“层”切分。比如模型有 100 层，显卡1负责前 50 层，显卡2负责后 50 层。数据像流水线一样流过。
    *   **缺陷：** 会产生**气泡 (Bubbles)**。当显卡1在算前 50 层时，显卡2是闲着的（因为它在等显卡1的结果）；反之亦然。这就导致显卡经常“磨洋工”，效率低，且代码实现非常复杂。

**总结之前的困境：** 数据并行装不下，流水线并行效率低且难实现。

***

### 3. 本论文提出了何种解决方案？方案为什么能解决这个问题？

**核心方案：张量模型并行 (Tensor Model Parallelism)**

Megatron-LM 没有像“流水线并行”那样按**层**（纵向）切分，而是选择在**层内部**（横向）把矩阵切开。

**通俗解释：** 想象模型中的一次计算是**两个巨大的矩阵相乘**（$A \times B = C$）。

*   之前的方案是：这就好比一个人算不过来，我们找两个人，但这两人还得互相等待。

*   **Megatron-LM 的方案：** 它把矩阵 $A$ 切成两半，矩阵 $B$ 也切成两半，分给两张显卡。

    *   显卡 1 算左半边。
    *   显卡 2 算右半边。
    *   **关键点：** 它们是**同时计算**的！谁都不用等谁。

**具体到 Transformer 架构（大模型的主流架构）：** Transformer 主要由两部分组成：**自注意力机制 (Self-Attention)** 和 **前馈神经网络 (MLP)**。 Megatron-LM 针对这两部分设计了巧妙的切分方法：

*   **按列切分 (Column Parallelism)：** 把权重矩阵按列竖着切开，分给不同显卡。
*   **按行切分 (Row Parallelism)：** 把权重矩阵按行横着切开，分给不同显卡。

**为什么能解决问题？**

1.  **显存压力平摊了：** 一个 10GB 的矩阵，如果切分到 2 张卡上，每张卡只占 5GB。只要卡够多，多大的模型都能装下。
2.  **没有闲置等待：** 因为大家是在算同一层的不同部分，可以同时开工，避免了流水线并行那种“你干活我看着”的闲置时间。

***

### 4. 方案的核心创新点在哪里？

你可能会问：“切矩阵谁不会啊？为什么这篇论文这么牛？” 核心创新在于\*\*“怎么切”才能让通信代价最小\*\*。

**创新点：针对 Transformer 结构的“通信优化切分”**

在多显卡训练中，显卡之间传输数据（通信）是非常慢的，远比显卡自己计算要慢。如果切分得不好，显卡就会把大量时间花在“互相通气”上，而不是计算上。

Megatron-LM 的天才之处在于它设计了一种\*\*“三明治”式的切分策略\*\*：

1.  **第一步（列切分）：** 第一个矩阵做**按列切分**。显卡算出结果后，**不需要**立刻告诉对方。
2.  **第二步（行切分）：** 第二个矩阵做**按行切分**。显卡继续接着算，依然**不需要**通信。
3.  **最后一步（All-Reduce）：** 只有在这一层计算彻底结束时，两张显卡才进行一次**全归约 (All-Reduce)**，把结果加在一起。

**一句话总结创新点：** 它并没有在每一步计算后都去同步数据，而是利用线性代数的性质，**把多次同步合并成了一次**。这极大地减少了显卡之间的“废话时间”，让多卡并行训练的效率几乎随着卡数线性增长（加一倍卡，速度就快近一倍）。

***

### 总结给学生听

*   **问题：** 就像要搬一块巨石（大模型），一辆卡车（单GPU）装不下。
*   **旧方法：** 要么每辆车都装一块石头（装不下），要么把石头切成几段先后运（不仅慢，后面车还得等前面车）。
*   **Megatron-LM：** 它把石头磨成粉（矩阵切分），铺在所有车厢里同时运。
*   **绝招：** 它通过数学技巧，安排了装载顺序，使得司机之间不需要频繁打电话确认位置（极简通信），从而让整个车队跑得飞快。

这就是 Megatron-LM 成为现在训练 GPT 系列、LLaMA 等大模型“标配”技术的根本原因。

## 思考一

第一步（列切分）： 第一个矩阵做按列切分。显卡算出结果后，不需要立刻告诉对方。 第二步（行切分）： 第二个矩阵做按行切分。显卡继续接着算，依然不需要通信。 最后一步（All-Reduce）： 只有在这一层计算彻底结束时，两张显卡才进行一次全归约 (All-Reduce)，把结果加在一起。 问题：

1.  这里的第一步和第二步是指什么？
2.  这里的一层是指什么？
3.  Transformer里自注意力和前馈神经网络两部分的切分方法是一致的吗？
4.  为什么可以做这样的切分？从线性代数的原理上解释一下，要求通俗易懂。

***

这是一个非常敏锐的问题！你能关注到“列切分”和“行切分”的顺序以及中间的“无通信”，说明你已经触及到了 Megatron-LM 的灵魂。

为了让你彻底明白，我们需要深入到 Transformer 模型的**微观结构**里去看一看。我们以 Transformer中最常见的**MLP（前馈神经网络）层**为例来拆解。

***

### 1. 这里的第一步和第二步是指什么？

在 Transformer 的 MLP 层中，数据的处理逻辑通常是这样的： 输入 $X \rightarrow$ **\[矩阵乘法 A]** $\rightarrow$ \[激活函数 (GeLU)] $\rightarrow$ **\[矩阵乘法 B]** $\rightarrow$ 输出 $Z$

这是一个“先变胖，再变瘦”的过程。

*   **第一步（第一个矩阵 A）：** 这是一个把特征维度放大的过程（通常放大4倍）。

    *   Megatron 选择**按列切分**这个矩阵 A。
    *   **结果：** 显卡 1 算出了一部分中间特征，显卡 2 算出了另一部分中间特征。

*   **中间插曲（激活函数）：** 显卡各自对自己的结果做激活（GeLU）。因为激活函数是针对每个数单独操作的，所以显卡之间**不需要商量**，各干各的。

*   **第二步（第二个矩阵 B）：** 这是一个把特征维度还原回去的过程。

    *   Megatron 选择**按行切分**这个矩阵 B。
    *   **结果：** 显卡 1 算出的结果和显卡 2 算出的结果，维度是一样的，但是数值只是最终结果的一部分（Partial Sum）。

### 2. 这里的一层是指什么？

在论文的语境下，这里的“一层”指的不是整个 Transformer Block（包含注意力+MLP），而是指**其中的某一个子模块**。

具体来说，Megatron-LM 把 Transformer Block 拆解成了两个独立的并行单元：

1.  **自注意力模块 (Self-Attention Block)：** 这是一个并行单元。
2.  **前馈神经网络模块 (MLP Block)：** 这是另一个并行单元。

你刚才看到的“列切分 -> 行切分 -> All-Reduce”这套动作，**在 MLP 模块里做了一遍，也在 Self-Attention 模块里做了一遍**。每做完一个模块，才进行一次通信（All-Reduce）。

***

### 3. Transformer里自注意力和前馈神经网络两部分的切分方法是一致的吗？

**答案是：逻辑一致（都是先列后行），但物理含义不同。**

虽然核心思想都是“为了减少通信，把两次矩阵乘法拼起来”，但针对的结构不一样：

#### **A. 在 MLP（前馈神经网络）中：**

*   **第一刀（列切分）：** 切的是第一个全连接层（fc1）。
*   **第二刀（行切分）：** 切的是第二个全连接层（fc2）。
*   **逻辑：** 把巨大的神经网络权重拆散了。

#### **B. 在 Self-Attention（自注意力机制）中：**

这是 Transformer 最特殊的地方，它天然就是“多头”的（Multi-Head）。

*   **第一刀（列切分）：** 切的是 $Q$ (Query), $K$ (Key), $V$ (Value) 的权重矩阵。

    *   **非常巧妙：** Transformer 有很多“头”（Head）。比如共有 32 个头，2 张显卡。Megatron 直接把**前 16 个头分给显卡 1，后 16 个头分给显卡 2**。
    *   这就天然构成了“列切分”。显卡 1 只负责算前 16 个头的注意力，显卡 2 算后 16 个的。

*   **第二刀（行切分）：** 切的是**输出投影矩阵 (Output Linear Layer)**。

    *   当各个头算完后，需要把结果拼起来经过一个线性层输出。这时采用行切分，直接处理各自显卡上的结果。

**结论：** 两者都是遵循 **“第一个矩阵列切 $\rightarrow$ 第二个矩阵行切 $\rightarrow$ 最后 All-Reduce”** 的黄金法则。

***

### 4. 为什么可以做这样的切分？从线性代数的原理上解释一下（通俗版）

这其实利用了矩阵乘法的一个名为**分块矩阵乘法 (Block Matrix Multiplication)** 的性质。

让我们把复杂的神经网络简化成最简单的数学公式： 假设我们要计算 $Y = X \times A \times B$。 （$X$是输入，$A$是第一层权重，$B$是第二层权重）。

#### **第一阶段：列切分矩阵 A**

我们将矩阵 $A$ 竖着切成两半：$A_1$ 和 $A_2$。 $A = [A_1, A_2]$ 我们将输入 $X$ 复制一份（或者广播）给两张显卡。

*   **显卡 1 计算：**  $Y_1 = X \times A_1$

*   **显卡 2 计算：**  $Y_2 = X \times A_2$

注意：此时 $Y_1$ 和 $Y_2$ 只是中间结果，它们是横向排列的。

#### **第二阶段：行切分矩阵 B**

为了能接上刚才的结果，我们将矩阵 $B$ 横着切成两半：$B_1$ 和 $B_2$。 $B = \begin{bmatrix} B_1 \\ B_2 \end{bmatrix}$

**最关键的魔法来了：** 我们要算的最终结果是 $Y = [Y_1, Y_2] \times \begin{bmatrix} B_1 \\ B_2 \end{bmatrix}$

根据线性代数规则，这个计算等价于： $\text{最终结果} = (Y_1 \times B_1) + (Y_2 \times B_2)$

你看清楚了吗？这个公式意味着：

1.  **显卡 1** 只需要拿自己刚才算出的  $Y_1$ ，去乘以  $B$  的上半部分  $B_1$ 。

2.  **显卡 2** 只需要拿自己刚才算出的  $Y_2$ ，去乘以  $B$  的下半部分  $B_2$ 。

3.  **这一步依然不需要通信！** 显卡 1 和 显卡 2 各算各的。

#### **最后阶段：加法 (All-Reduce)**

公式里的那个 **“+”** 号，就是最后一步。 显卡 1 手里有 $(Y_1 \times B_1)$，显卡 2 手里有 $(Y_2 \times B_2)$。 它们只需要在最后时刻，把这两个结果**加起来**，就得到了最终的正确答案。

**总结原理：** Megatron-LM 把一个连乘运算 $X \cdot A \cdot B$，利用分块性质变成了 $(X \cdot A_1) \cdot B_1 + (X \cdot A_2) \cdot B_2$。 这使得中间的计算可以完全独立，只有最后一步求和需要“见面”。

## 思考二

1.  解释一下数据并行、流水线并行、张量并行三种方法在attention层和FFN层的数据流，并分析各自的通讯量
2.  生成一个通讯量的比较表格

***

这是一个非常专业且切中要害的问题！理解这三种并行方式在\*\*数据流（Data Flow）**和**通讯（Communication）\*\*上的区别，是真正掌握大模型训练架构的关键。

为了方便理解，我们设定一个标准场景：

*   **输入数据**：一批文本（Batch Size  $B$ ，序列长度  $S$ ）。

*   **模型隐藏层维度**： $H$  (Hidden Size)。

*   **显卡数量**： $N$  张卡。

### 1. 三种并行方法的深度解析

#### **A. 数据并行 (Data Parallelism, DP)**

这是最基础、最直观的并行方式。

*   **形象比喻：** **影分身术**。 每个人（GPU）都复制一份完全一样的“大脑”（模型权重）。老师发试卷（数据）时，把试卷分成 $N$ 份，每人做一份。最后大家把答案的修正意见（梯度）汇总起来，统一修改大脑。

*   **Attention / FFN 层的数据流：**

    1.  **复制：** 每张卡显存里都有**完整**的 Attention 和 FFN 参数。
    2.  **独立计算：** 显卡 1 拿第 1-10 条数据，显卡 2 拿第 11-20 条数据。它们在各自的卡内完整跑完 Forward（前向）和 Backward（反向）过程。
    3.  **计算期间无通信：** 在计算 Attention 和 FFN 的过程中，显卡之间**完全互不理睬**。

*   **通讯分析（痛点）：**

    *   **何时通讯？** 在反向传播结束，准备更新参数（Optimizer Step）之前。
    *   **传什么？** **梯度 (Gradients)**。
    *   **通讯量：** 与**模型参数量 ($P$)** 成正比。
    *   **瓶颈：** 模型越大，参数越多，梯度就越大。当模型大到一定程度，通讯时间会超过计算时间，且单卡显存装不下完整模型时，此法失效。

#### **B. 流水线并行 (Pipeline Parallelism, PP)**

为了解决单卡装不下模型的问题，PP 把模型横着切（按层切）。

*   **形象比喻：** **工厂流水线**。 GPU 1 负责第 1-10 层，GPU 2 负责第 11-20 层。数据像半成品一样，处理完一道工序传给下一道。

*   **Attention / FFN 层的数据流：**

    1.  **分层驻扎：** 某些 Attention/FFN 层只存在于 GPU 1，另一些只存在于 GPU 2。

    2.  **接力跑：**

        *   GPU 1 计算完它的那些层，得到一个**中间结果（Activations）**。
        *   GPU 1 停下，把这个中间结果传给 GPU 2。
        *   GPU 2 接着算它的层。

*   **通讯分析：**

    *   **何时通讯？** 在两个 GPU 负责的层与层交界处（Stage Boundary）。
    *   **传什么？** **边界处的激活值 (Activations)**（前向时）和 **对激活值的梯度**（反向时）。
    *   **通讯量：** 与 **中间特征图的大小 ($B \times S \times H$)** 成正比。通常比模型参数量小得多。
    *   **瓶颈：** 通讯量其实不大，核心问题是**气泡（Bubble）**——GPU 2 等 GPU 1 结果时是闲着的。虽然不算通讯堵塞，但算力浪费严重。

#### **C. 张量并行 (Tensor Parallelism, TP) - Megatron-LM 的核心**

这是为了解决单层超大矩阵运算而生的。

*   **形象比喻：** **多人同抬一根原木**。 对于同一层的同一个矩阵乘法，大家同时出力，各抬一部分。

*   **Attention 层的数据流（以 Megatron 为例）：**

    1.  **输入：** 同一份数据输入  $X$  复制到所有卡（或已经是切分过的）。

    2.  **QKV 映射（列切）：** 卡 1 算前一半 Head，卡 2 算后一半 Head。**（无通信）**

    3.  **Attention Score：** 各自算各自 Head 的注意力分数。**（无通信）**

    4.  **输出投影（行切）：** 各自算结果的一部分。**（无通信）**

    5.  **All-Reduce：** 把两张卡的结果**加起来**，得到最终完整的 Attention 输出。**（发生通信）**

*   **FFN 层的数据流：**

    1.  **第一层 MLP（列切）：** 维度放大，各算一半特征。**（无通信）**
    2.  **GeLU：** 各自激活。**（无通信）**
    3.  **第二层 MLP（行切）：** 维度还原，各算一部分结果。**（无通信）**
    4.  **All-Reduce：** 把两张卡的结果**加起来**。**（发生通信）**

*   **通讯分析：**

    *   **何时通讯？** 在每一层的 Attention 结束时，和每一层的 FFN 结束时。**非常高频！**
    *   **传什么？** **中间激活值 (Activations)** 的部分和。
    *   **通讯量：** 与 **特征图大小 ($B \times S \times H$)** 成正比。
    *   **瓶颈：** 虽然单次通讯量不大，但因为每个 Transformer Block 都要通讯两次（Attn一次，FFN一次），如果 GPU 之间物理距离远（比如跨服务器），延迟会极高。因此 TP 通常只在\*\*单机内部（NVLink 连接）\*\*使用。

***

### 2. 通讯量比较表格

为了直观对比，假设我们要训练一个超大模型。

| 维度           | **数据并行 (DP)**                                                  | **流水线并行 (PP)**                   | **张量并行 (TP, Megatron)**                                                  |
| ------------ | -------------------------------------------------------------- | -------------------------------- | ------------------------------------------------------------------------ |
| **切分对象**     | **数据** (Batch Size)                                            | **模型层数** (Layers)                | **模型层内参数** (Weights)                                                     |
| **显存中存储的内容** | 完整模型副本                                                         | 模型的一部分层                          | 每层矩阵的一块碎片                                                                |
| **通讯发生时机**   | 反向传播结束后 (Update 前)                                             | 层与层交界处 (跨 Stage)                 | **每一层**的前向和反向计算内部                                                        |
| **通讯传输内容**   | **模型梯度 (Gradients)**                                           | **边界激活值 (Activations)**          | **中间激活值 (Activations)**                                                  |
| **单次通讯量级**   | 极大 (与参数量 $P$ 成正比)                                              | 较小 (与 $B \times S \times H$ 成正比) | 较小 (与 $B \times S \times H$ 成正比)                                         |
| **通讯频率**     | 低 (每 Batch 一次)                                                 | 中 (每 Micro-Batch 一次)             | **极高** (每层Layer都要通讯)                                                     |
| **对带宽的要求**   | <span style="background-color: #ffd40080">很高 (但可容忍一定延迟)</span> | 较低 (点对点传输)                       | **极高** (<span style="background-color: #ffd40080">极低延迟，必须用NVLink</span>) |
| **典型应用范围**   | 多机多卡 (最通用)                                                     | 多机多卡 (跨节点连接)                     | **单机多卡** (节点内连接)                                                         |
| **一句话总结**    | **各干各的，最后对答案**                                                 | **接力赛，棒交接时通讯**                   | **二人三足，时刻同步**                                                            |


### 核心结论（给学生的“作弊条”）：

1.  **DP (数据并行)** 传的是**梯度**（也就是权重更新量），跟模型多大有关。

2.  **TP (张量并行)** 和 **PP (流水线并行)** 传的都是**数据（激活值）**，跟由于 Batch Size 和 序列长度 有关。

3.  **最关键的区别：**

    *   **TP** 是把大家捆在一起，必须**同时**走，任何一步计算都要互相确认（All-Reduce），所以只能在通信超快的**单机内部**做。
    *   **PP** 是把大家分开，只要交接棒（边界数据）传过去就行，适合在**多台机器之间**做连接。

这也是为什么现在训练 GPT-4 这种大模型时，通常会混合使用这三种策略（即 **3D 并行**）：

*   在显卡内部切开矩阵用 **TP**。
*   在机器之间把层切开用 **PP**。
*   最后再复制几份整个流水线用 **DP** 来吃更多数据。
