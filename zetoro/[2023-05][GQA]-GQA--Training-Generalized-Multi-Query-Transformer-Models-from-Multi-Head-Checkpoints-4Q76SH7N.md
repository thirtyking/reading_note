---
tags: []
parent: '[GQA] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints'
collections:
    - 核心组件
    - 推理加速
$version: 2207
$libraryID: 1
$itemKey: 4Q76SH7N

---
\[2023-05]\[GQA] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints

# \[GQA] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints

## 概述

你好！很高兴能为你解读这就篇在当今大模型（LLM）领域非常实用且影响力巨大的论文：《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》。

现在的开源大模型（比如 Llama 2、Llama 3、Mistral）之所以跑得快、显存占用低，很大程度上都要归功于这篇论文提出的 **GQA** 技术。

我们把它拆解成四个部分来聊。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**核心问题：** 如何在保证模型**聪明程度**（性能/生成质量）几乎不下降的前提下，大幅提升模型**说话的速度**（推理速度），并降低**内存占用**（显存开销）。

**为什么重要？** 现在的 AI 模型（如 ChatGPT）是基于 Transformer 架构的。它们在生成回复时，是“一个字一个字”往外蹦的（这叫自回归生成）。

*   **痛点：** 模型生成每一个字，都需要回头去查阅之前所有内容的“记忆”。这部分记忆在计算机里被称为 **KV Cache（键值缓存）**。

*   **瓶颈：** 随着对话越来越长，这个“记忆库”会变得极其庞大。显卡（GPU）为了读取这些记忆，内存带宽会被占满。结果就是：**模型虽然算得快，但数据搬运太慢，导致生成速度变慢，且显存容易爆掉。**

如果解决不了这个问题，长文本对话或在手机端运行大模型就几乎不可能实现。

***

### 2. 解决这个问题的难点在哪里？之前有哪些方案？为什么不够好？

难点在于\*\*“速度”与“质量”的权衡\*\*。

在 GQA 出现之前，主要有两个极端的流派：

1.  **MHA（Multi-Head Attention，多头注意力机制）** —— **追求质量，但太慢**

    *   **原理：** 这是 Transformer 的原配方案。假设模型有 8 个“查询头”（Query Heads，负责提问），它就配备 8 个“键值头”（Key-Value Heads，负责存记忆）。也就是 **1对1** 的关系。

    *   **优点：** 模型“脑子”好使，能从不同角度捕捉信息。

    *   **缺点：** 存的记忆（KV Cache）太多了，显存占用大，读取慢。

2.  **MQA（Multi-Query Attention，多查询注意力机制）** —— **追求速度，但变笨**

    *   **原理：** 为了省地儿，不管你有多少个“查询头”（比如 8 个），我就只给你配 **1 个**“键值头”。也就是 **多对1** 的关系。

    *   **优点：** 记忆库瞬间缩小 8 倍（以上例为例），速度飞快！

    *   **缺点：** 模型“脑子”不够用了，因为所有头都共用同一份压缩的记忆，细节容易丢失，生成质量明显下降，甚至会胡言乱语。

**之前的困境：** 大家都以为只能二选一，要么选 MHA 忍受龟速，要么选 MQA 忍受降智。

***

### 3. 本论文提出了何种解决方案？方案为什么能解决这个问题？

**解决方案：GQA（Grouped-Query Attention，分组查询注意力机制）**

这篇论文提出了一个完美的**折中方案**。

**核心思路：** 既然 1对1（MHA）太慢，多对1（MQA）太笨，那我们为什么不搞个 **“几对1”** 呢？

**具体做法：**

*   GQA 把“查询头”分成几个**组（Group）**。

*   每个组内的查询头，共享同一个“键值头”。

*   **举例：** 假设原来是 8 个查询头。

    *   **MHA:** 配 8 个 KV 头。（显存占用 100%）

    *   **MQA:** 配 1 个 KV 头。（显存占用 12.5%）

    *   **GQA (分组):** 我们把 8 个查询头分成 **4 组**，每组 2 个。每组配 1 个 KV 头，总共 **4 个 KV 头**。（显存占用 50%）

**为什么能解决问题？** GQA 找到了一个\*\*“甜点位”（Sweet Spot）\*\*：

1.  **速度够快：** 相比 MHA，它的 KV Cache 大幅减小（通常减小到 1/8 或 1/4），内存带宽压力骤减。

2.  **质量够高：** 相比 MQA，它保留了更多的 KV 头，模型的“记忆能力”和“区分能力”比单纯共用一个头要强得多，性能几乎接近原始的 MHA。

***

### 4. 方案的核心创新点在哪里？

这篇论文除了提出 GQA 这个架构（其实架构改动很简单），更重要的是它解决了一个**工程难题**：

**创新点：如何把旧模型“变身”？（Uptraining / 上也就是“继续训练”）**

通常，如果你想换架构（比如从 MHA 换到 GQA），你得从头开始训练模型，这需要消耗数百万美元和几个月的时间。

这篇论文提出了一个\*\*“低成本改造”\*\*方案：

1.  **利用现有资产：** 拿着已经训练好的 MHA 模型（比如 BERT 或 T5 的检查点 Checkpoint）。

2.  **数学合并（Mean Pooling）：** 把原来的多个 KV 头的参数，通过数学平均的方法，“捏”成较少的 KV 头，作为 GQA 模型的初始参数。

3.  **少量微调（Uptraining）：** 在这个“捏”好的基础上，只用原训练量 **5%** 的计算资源再训练一下。

**结果：** 模型迅速适应了新的 GQA 架构，性能恢复到接近原始水平，但推理速度快了很多。

**总结一下：**

*   **GQA 架构** = MHA 和 MQA 的混血儿，兼顾速度与质量。

*   **Uptraining 方法** = 不需要从零练号，可以把满级的大号（MHA）通过微整形，快速改成新职业（GQA）。

这就是为什么现在的 Llama 3 70B 等模型都采用 GQA 的原因——它让大模型在你的电脑上跑得动，而且还依然聪明。

## 思考一

GQA (分组): 我们把 8 个查询头分成 4 组，每组 2 个。每组配 1 个 KV 头，总共 4 个 KV 头。（显存占用 50%） 速度够快： 相比 MHA，它的 KV Cache 大幅减小（通常减小到 1/8 或 1/4），内存带宽压力骤减。 问题：前面说4个KV头，显存占用50%，后面又说KV Cache大幅减少到1/8或1/4，似乎有些矛盾，怎么理解？

***

你看得很仔细！这是个非常好的问题。

确实，我在前面的举例中为了让你**理解“分组”的概念**，用了一个比较保守的数字（8 分成 4 组），而后面提到的“1/8”是**业界实际应用中的典型数值**。

这里不存在原理上的矛盾，而是\*\*“教学例子”**和**“实际生产配置”\*\*的区别。我来详细拆解一下这背后的数学逻辑，你就明白了。

### 1. 为什么例子里是 50%？（为了好懂）

在解释 GQA 原理时，我举例说：“8 个头，分成 4 组”。

*   **计算：** 原本 MHA 需要 8 个 KV 头，现在 GQA 只需要 4 个 KV 头。

*   **压缩率：**  $4 \div 8 = 0.5$  (50%)。

*   **目的：** 这个例子是为了让你直观看到“两两一组”是怎么运作的。但在实际应用中，只压缩一半通常不够“解渴”，我们往往下手更狠。

### 2. 为什么实际说是 1/8 或 1/4？（为了好用）

在真实的大模型（比如 Llama-2-70B 或论文中的实验设置）中，头数（Heads）通常非常多，比如 **64 个**甚至更多。

如果在这个规模下使用 GQA，我们通常会将分组比例设定得更高（比如 8 个查询头共用 1 个 KV 头）：

*   **原始 MHA 配置：**

    *   Query 头数：64 个

    *   KV 头数：**64 个** (1对1)

    *   KV Cache 大小：假设是 64GB（仅作比喻）

*   **实际 GQA 配置（典型设置）：**

    *   Query 头数：64 个（保持不变）

    *   **分组策略：** 每 **8** 个 Query 头分为一组。

    *   KV 头数： $64 \div 8 =$  **8 个**。

    *   **压缩率：**  $8 \div 64 =$  **1/8 (12.5%)**。

    *   KV Cache 大小：变成了 8GB。

**结论：**

*   当你把分组比例设为 **2:1** 时（我的教学例子），显存省一半。

*   当你把分组比例设为 **8:1** 时（业界主流做法），显存变成原来的 **1/8**。

### 3. GQA 的灵活性（想压多少压多少）

这正是 GQA 厉害的地方，它像是一个**滑动条**：

*   **滑动条推到最左边 (MHA):** 64 个 KV 头。 (最聪明，但最慢，显存 100%)

*   **滑动条推到最右边 (MQA):** 1 个 KV 头。 (最笨，但最快，显存 1/64)

*   **滑动条放在中间 (GQA):** 我们可以选 8 个头、4 个头或者 16 个头。

**论文的核心发现是：** 当我们把 KV 头数压缩到 **1/8** (比如从 64 压到 8) 时，模型的智商（准确率）几乎没有下降，但速度和显存的红利已经吃得差不多了。

所以，**“1/8”是一个最佳性价比（Sweet Spot）的经验值**，而“50%”只是我用来解释原理的简易数学题。

## 思考二

为什么只考虑压缩KV头，而不考虑压缩Q头

***

这是一个极其敏锐的问题！这触及到了 Transformer 架构最本质的运作机理。

简单来说，只压缩 KV（键值）而不压缩 Q（查询），是因为：**压缩 KV 是为了“省钱”（省显存），而保留 Q 是为了“保命”（保智商）。**

我们可以从\*\*“工程瓶颈”**和**“数学原理”\*\*两个维度来彻底讲清楚这个问题。

***

### 1. 工程维度：瓶颈在“记忆”而不在“当前”

我们要解决的核心痛点是 **推理速度慢** 和 **显存不够**。

在模型生成文字的时候（比如它正在写第 1000 个字）：

*   **Q (Query) 的状态：** 它是\*\*“当前”\*\*的想法。模型只处理这 **1 个** 新字。Q 的数据量非常小（只是一个向量），算完瞬间就扔掉了，**不占什么显存**。

*   **KV (Key-Value) 的状态：** 它是\*\*“过去”\*\*的记忆。模型必须把前面 999 个字的 KV 信息全部存在显卡里（这就是 **KV Cache**）。随着对话越长，这个库越大，动辄几十 GB。

**结论：**

*   压缩 Q：省不了多少显存（因为 Q 本来就不占地儿），且无法解决读取过去庞大记忆的带宽瓶颈。
*   压缩 KV：直接击中痛点！把那几十 GB 的庞大显存占用瞬间砍掉几倍。

所以，**压缩 Q 在工程上属于“捡了芝麻（计算量），丢了西瓜（模型能力）”，性价比极低。**

***

### 2. 原理维度：Q 代表“观察角度”，KV 代表“被观察的数据”

为了让你更好理解，我们用一个\*\*“多位专家面试一个候选人”\*\*的场景来打比方。

*   **场景：** 面试现场。

*   **Q (Query 头)：** 是 **面试官**。不同的 Q 头代表不同的面试官（有的考察技术，有的考察英语，有的考察情商）。

*   **KV (Key/Value 头)：** 是 **候选人的简历和档案**。

#### 为什么要保留很多 Q (面试官)？

如果你把 Q 压缩了（比如从 8 个 Q 变成 1 个 Q），就等于把 8 个不同领域的专家裁员成了 1 个通才。

*   **后果：** 你的考察角度变得单一了。模型失去了多角度思考问题的能力（Multi-Head 的本意就是多角度注意力）。**智商直接跳水。**

#### 为什么可以压缩 KV (简历)？

在 **MHA (多头注意力)** 中，传统的做法极其奢侈：

*   面试官 A 手里拿一份简历复印件。
*   面试官 B 手里拿一份简历复印件。
*   ...
*   8 个面试官，桌上放了 8 份一模一样的档案（虽然数学上会有不同投影，但本质信息源是冗余的）。这太浪费桌子空间（显存）了！

在 **GQA (分组查询)** 中，我们这样做：

*   我们把 8 个面试官分成 4 组。

*   每组 2 个面试官，**共看同一份简历**。

*   **结果：** 面试官（Q）的人数没变，大家依然可以从不同角度提问；但是桌上的简历（KV）数量少了很多，桌子空间（显存）省下来了。

### 总结

1.  **Q 不能少：** Q 决定了模型的**表达能力**（Feature extraction），减少 Q 会让模型变笨，看不懂复杂关系。

2.  **KV 可以少：** KV 主要是**信息载体**，多个 Q 共享一份 KV，往往也能提取出需要的信息（因为以前 MHA 的 KV 头之间存在大量的信息冗余）。

所以，GQA 的精髓就是：**保留“脑子”（Q），压缩“记忆体”（KV）。**
