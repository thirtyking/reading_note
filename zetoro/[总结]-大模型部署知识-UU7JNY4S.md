---
tags: []
parent: ""
collections:
    - '0. 综述'
$version: 2259
$libraryID: 1
$itemKey: UU7JNY4S

---
\[总结] 大模型部署知识

## 硬件选型与显存公式详解

你好！我是你的 AI 基础设施架构师。

你提的问题非常专业，直击大模型推理的“七寸”。很多企业在采购时只看算力（TFLOPS），结果买回来发现推理速度慢得像蜗牛，根本原因就是没搞懂**带宽**和**显存**的关系。

让我们深入底层，把这些“硬骨头”啃下来。

***

### 1. 核心参数：带宽 vs 算力 —— 谁是瓶颈？

在**大模型推理（Inference）场景下，尤其是生成阶段（Decoding）**，**显存带宽 (Memory Bandwidth)** 远比 **算力 (TFLOPS)** 重要。

#### 为什么是 "Memory-bound"（显存受限）？

想象一个工厂：

*   **显存 (VRAM)** 是仓库，存放着模型权重（几百亿个参数）。

*   **显存带宽** 是连接仓库和车间的运输公路。

*   **计算单元 (Tensor Cores)** 是车间里的超级工人。

大模型生成文本是\*\*自回归（Auto-regressive）\*\*的，也就是“一个字一个字蹦”。 **每生成 1 个 Token**，GPU 就必须把 **几百亿个参数（权重）** 完整地从显存仓库搬运到计算车间一次。

*   **算力（工人）：** 动作极快，0.0001秒就算完了。

*   **带宽（公路）：** 路太窄，搬运这几十 GB 的数据需要 0.05秒。

*   **结果：** 工人 99% 的时间都在等数据。这就是 **Memory-bound**。

> **结论：**
>
> *   **追求低延迟（单用户/小并发）：** 显存带宽决定推理速度上限。**带宽越高，字蹦得越快**。
>
> *   **追求高吞吐（高并发）：** 当并发量很大（Batch Size 大）时，一次搬运数据可以给几十个用户共用计算，此时算力才开始成为瓶颈。

***

### 2. 量化与显存：通用计算公式

要精准规划硬件，必须掌握这套数学。

#### 公式 A：<span style="background-color: #ffd40080">模型权重占用 (静态)</span>

这是模型加载后雷打不动的占用。 $\text{显存 (GB)} \approx \frac{\text{参数量 (Billion)} \times \text{每参数字节数}}{1}$

*   **FP16/BF16:** 2 Bytes

*   **INT8:** 1 Byte

*   **INT4:** 0.5 Byte

#### 公式 B：<span style="background-color: #ffd40080">KV Cache 占用 (动态)</span>

这是随上下文长度增长而增长的部分。Llama-3 等现代模型使用了 **GQA (Grouped Query Attention)**，大大节省了 KV Cache。

$\text{KV Cache (GB)} = \frac{2 \times \text{层数} \times \text{KV头数} \times \text{头维度} \times \text{上下文长度} \times \text{并发数} \times \text{精度字节}}{1024^3}$

*   **系数 2:** Key 和 Value 各一份。

*   **KV头数 (Num KV Heads):** 注意不是 Query 头数。Llama-3-70B 的 KV 头数是 8（GQA），而不是 64。

*   **头维度 (Head Dim):** 通常是  $\frac{\text{隐藏层维度}}{\text{Query头数}}$ 。

#### 公式 C：<span style="background-color: #ffd40080">总显存需求</span>

$\text{总显存} = \text{权重占用} + \text{KV Cache} + \text{激活值(通常预留 1-2GB)} + \text{框架开销}$

***

### 3. 实战演练：Llama-3-70B 的算账现场

**场景设定：**

*   **模型：** Llama-3-70B

*   **参数详情：** 70B参数，80层，KV头数=8，Head Dim=128。

*   **需求：** Context=8192 (8k)，Batch Size=1 (单人测试)，目标速度 20 Tokens/s。

#### 方案一：FP16 精度 (不量化)

1.  **权重占用：**$70 \times 2 = \mathbf{140 \text{ GB}}$。

2.  **KV Cache (8k)：**$2 \times 80 \text{(层)} \times 8 \text{(KV头)} \times 128 \text{(维)} \times 8192 \text{(长)} \times 2 \text{(Bytes)} \approx \mathbf{2.68 \text{ GB}}$。 *(注：如果是旧模型如 Llama-2-70B 没用 GQA，这个数字会翻 8 倍变成 21GB)*

3.  **总显存需求：**$140 + 2.7 + \text{Buffer} \approx \mathbf{145 \text{ GB}}$。

    *   **硬件推论：** 单张 A100 (80G) **放不下**。需要 **2张 A100 (80G)** 做张量并行 (TP)。

4.  **带宽需求计算 (Memory Bandwidth Bound)：**

    *   要每秒生成 20 个 Token。

    *   每生成 1 个 Token 需要读取 140 GB 权重。

    *   所需带宽 =  $140 \text{ GB} \times 20 = \mathbf{2800 \text{ GB/s}}$ 。

    *   **现实检验：** 单张 A100 带宽是 1935 GB/s。虽然我们用了双卡，总带宽理论翻倍，但受限于卡间通信，能跑满 2800 GB/s 是没问题的。所以 **双卡 A100 FP16 可以达标**。

#### 方案二：INT4 量化 (W4A16 - 权重4bit，激活16bit)

1.  **权重占用：**$70 \times 0.5 = \mathbf{35 \text{ GB}}$。

2.  **KV Cache (8k)：** 通常 KV Cache 保持 FP16 以保证精度，仍为 **2.68 GB**。

3.  **总显存需求：**$35 + 2.7 + \text{Buffer} \approx \mathbf{40 \text{ GB}}$。

    *   **硬件推论：** 刚好塞进 **一张 A100 (40G)** 或 **一张 RTX 6000 Ada (48G)**。如果是 RTX 4090 (24G) 则需要 **2张**。

4.  **带宽需求计算：**

    *   每生成 1 个 Token 读取 35 GB 权重。

    *   所需带宽 =$35 \text{ GB} \times 20 = \mathbf{700 \text{ GB/s}}$。

    *   **现实检验：**

        *   **RTX 4090 带宽：** \~1008 GB/s。**远超 700 GB/s**。

        *   **结论：** 使用 INT4 量化，即便是消费级的 RTX 4090，也能轻松实现 20+ Tokens/s 的流畅速度，甚至能达到 30-40 Tokens/s。

***

### 4. 显卡角斗场：A100 vs H100 vs 4090 vs L20

作为架构师，选卡不仅看性能，还要看**性价比**和**合规性**。

| 显卡型号               | 显存 (VRAM)    | 带宽 (Bandwidth) | FP16 算力      | 特性与适用场景                                                                                                                                 |
| ------------------ | ------------ | -------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------------------- |
| **H100 (80G)**     | 80 GB HBM3   | **3350 GB/s**  | ~1979 TFLOPS | **地表最强。**支持**FP8**精度（推理速度翻倍神器）。带宽极其恐怖，适合超高并发、超低延迟的生产环境。**缺点：贵，难买。**                                                                     |
| **A100 (80G)**     | 80 GB HBM2e  | 1935 GB/s      | 312 TFLOPS   | \*\*工业标准。\*\*生态最成熟。NVLink 速度快，适合多卡跑大模型（如 70B/405B）。性价比目前不如 H100，但依然是中流砥柱。                                                               |
| **L20 (48G)**      | 48 GB GDDR6  | **864 GB/s**   | ~239 TFLOPS  | \*\*中国特供卡。\*\*基于 Ada 架构（类似 4090）。\*\*优势：\*\*48G 大显存是亮点，能跑较大的量化模型。\*\*劣势：\*\*带宽被砍了一刀（比 4090 还低），导致推理速度受限。适合并发不高、但模型较大（需要大显存）的场景。         |
| **RTX 4090 (24G)** | 24 GB GDDR6X | 1008 GB/s      | 83 TFLOPS    | \*\*性价比之神。优势：\*\*单卡带宽超高，单卡跑 7B-14B 模型速度极快。\*\*劣势：\*\*显存太小（24G），跑 70B 需要 2-4 张卡。**不支持 NVLink**，多卡通信走 PCIe，速度会大打折扣。且严禁用于数据中心（License 限制）。 |


### 架构师总结 (Takeaway)

1.  **公式要背熟：**$P \times \text{精度}$算权重，GQA 之后的公式算 KV。

2.  **瓶颈判定：** 单人对话看**带宽**，万人并发看**算力**。

3.  **选型建议：**

    *   **土豪/生产环境：** 直接上 H100/A100，利用 NVLink 扩展。

    *   **高性价比/实验室：** RTX 4090 (双卡/四卡)，但要忍受 PCIe 通信带来的额外延迟（尤其是 PP 流水线并行时）。

    *   **特定受限环境：** L20 是无奈之选，用大显存换空间，但速度不会太快。

***

接下来，如果你想知道**如何把这些卡用好**，也就是软件层面的选择，可以发送 **Prompt 3**，我们将进入 **“推理框架的深度对比”**。

## 算力计算公式

你好！我是你的 AI 基础设施架构师。

这是一个非常棒的进阶问题。如果说**显存带宽（Bandwidth）**决定了**“单辆跑车能跑多快（最低延迟）”**，那么**算力（TFLOPS）**就决定了**“高速公路上能同时容纳多少辆车（最大吞吐量）”**以及**“起步加速有多猛（首字延迟）”**。

简单来说：**带宽决定下限，算力决定上限。**

下面我将从原理、场景和计算公式三个方面为你拆解 TFLOPS 的奥秘。

***

### 1. TFLOPS 在哪里起作用？

TFLOPS (Trillion Floating Point Operations Per Second) 是指 GPU 每秒能做多少万亿次浮点运算。

在大模型推理中，算力主要影响两个关键环节：

#### A. 首字延迟 (TTFT) —— 算力是主力

当用户发来一段 5000 字的长文档（Prompt）让你总结时，这就是 **Prefill（预填充）** 阶段。

*   **动作：** GPU 需要一次性把这 5000 个 token 全部算一遍，生成 KV Cache。
*   **特征：** 这是一个巨大的矩阵乘法任务，**完全是计算密集型（Compute-bound）**。
*   **影响：** 你的显卡 TFLOPS 越高，这 5000 字“读”得就越快，首字出来的等待时间就越短。

#### B. 高并发吞吐量 (High Batch Throughput) —— 算力是天花板

当并发量极高（例如 Batch Size = 128）时：

*   **动作：** 每一轮生成，GPU 要同时计算 128 个用户的矩阵。
*   **特征：** 虽然权重只读一次（省了带宽），但计算量翻了 128 倍。
*   **影响：** 此时带宽已经不是瓶颈了，计算单元（Tensor Cores）甚至可能冒烟。如果算力不够，TPOT（生成速度）就会下降。

***

### 2. 数学硬核：如何计算算力需求？

要计算你需要多少算力，必须记住一个**黄金估算常数**：

> **每处理/生成 1 个 Token，大约需要 $2 \times P$ 次浮点运算。** *($P$ 是模型参数量)*

*   **原理：** 神经网络的核心是矩阵乘法（$y = wx + b$）。一次乘法（Multiply）加一次加法（Add）算 2 次运算（FLOPs）。

*   **Qwen-72B 案例：**

    *   参数 $P \approx 72 \text{ Billion}$。
    *   单 Token 计算量 $\approx 72 \times 2 = 144 \text{ GFLOPs}$ (0.144 TFLOPs)。

#### 公式一：计算“首字延迟” (Prefill 速度)

假设用户输入了 **2000 token** 的上下文（RAG 场景），使用 **Qwen-72B (INT8)**，单卡 **RTX 4090**。

1.  **所需总计算量：**$\text{Total FLOPs} = \text{输入长度} \times 2 \times P$$2000 \times 144 \text{ GFLOPs} = 288,000 \text{ GFLOPs} = \mathbf{288 \text{ TFLOPs}}$

2.  **显卡能力 (RTX 4090)：**

    *   4090 的 INT8 Tensor Core 理论算力约为 **660 TFLOPS** (不含稀疏化)。
    *   **有效算力 (MFU, Model FLOPs Utilization)：** 实际上跑不满，通常按 **40%-60%** 估算。我们按 50% 算。
    *   $\text{实战算力} = 660 \times 50\% = 330 \text{ TFLOPS}$

3.  **计算时间 (TTFT)：**$\text{时间} = \frac{\text{所需计算量}}{\text{实战算力}} = \frac{288}{330} \approx \mathbf{0.87 \text{ 秒}}$

*   **结论：** 4090 处理 2k 长度的 Prompt，首字延迟大约 0.87秒。如果你用的是算力更弱的卡（如 A10），这个时间可能变成 3-4 秒，用户体验就差了。

#### 公式二：计算“最大并发上限” (Throughput 瓶颈)

假设你的老板要求系统每秒必须处理 **2000 个 Token**（比如 100 人并发，每秒每人生成 20 个字）。你需要多少 TFLOPS？

1.  **业务需求算力：**$\text{需求 TFLOPS} = \text{目标 Tokens/s} \times \text{单 Token 计算量}$$2000 \times 0.144 \text{ TFLOPs} = \mathbf{288 \text{ TFLOPs}}$

2.  **验证硬件：**

    *   **单张 4090 (INT8)** 有效算力约 330 TFLOPS。

    *   $330 > 288$。**结论：单张 4090 的算力足够支撑 2000 tokens/s 的吞吐。**

    *   **单张 A100 (FP16)**：

        *   A100 FP16 Tensor Core 理论 312 TFLOPS。
        *   有效算力 $\approx 150 \text{ TFLOPS}$。
        *   $150 < 288$。**结论：单张 A100 即使显存够大、带宽够高，但在此并发下，算力成为了瓶颈，跑不到 2000 tokens/s。**

***

### 3. 不同精度的算力陷阱

你在看显卡参数表时，一定要看**对应精度**的 TFLOPS。这区别极大！

以 **NVIDIA H100 (SXM)** 为例：

*   **FP64 (双精度):** 67 TFLOPS —— *科学计算用，AI 推理完全不用。*
*   **FP16 (半精度):** 1,979 TFLOPS (Tensor Core) —— *常用。*
*   **FP8 (8位精度):** 3,958 TFLOPS —— *H100 的杀手锏，比 FP16 快一倍。*

**架构师提示：** 这就是为什么我在之前的方案中推荐你用 **INT4/INT8 量化**。 不仅是因为省显存，更是因为 **INT8 的计算速度通常是 FP16 的 2 倍**。 对于算力受限的场景（高并发、长文档处理），量化是提升吞吐量的最廉价手段。

### 4. 总结：什么时候该担心算力？

| 场景                            | 瓶颈通常是...             | 解决办法                            |
| :---------------------------- | :------------------- | :------------------------------ |
| **单人聊天 (Batch=1)**            | **显存带宽 (Bandwidth)** | 买带宽更高的卡 (H100/4090)，算力再高也没用。    |
| **长文档阅读 (RAG Context > 10k)** | **算力 (TFLOPS)**      | 只有高算力才能缩短那个 "Thinking..." 的进度条。 |
| **高并发服务 (Batch > 64)**        | **算力 (TFLOPS)**      | 算力决定了你这台机器每秒最多能吐出多少字。           |
| **显存不够装模型**                   | **显存容量 (Capacity)**  | 加卡，或者量化。                        |


**简单的一句话公式：**

> **算力需求 (TFLOPS) = 业务总吞吐 (Tokens/s) $\times$ 模型参数量 (B) $\times$ 2 $\div$ 1000**

用这个公式，你就可以在买卡之前，算出它到底能不能抗住你们公司的流量了。

## 为什么张量并行能提升带宽

这是一个非常好的追问！这句话触及了大模型分布式推理中最核心的技术原理：**张量并行（Tensor Parallelism, TP）**。

简单来说，这句话的逻辑是：**“人多力量大（带宽叠加），但沟通需要时间（通信损耗）”。**

为了让你彻底理解，我将通过一个\*\*“搬砖”**的比喻，配合**技术原理解析\*\*来说明。

***

### 1. 通俗比喻：搬砖与砌墙

假设任务是：**每秒钟要砌好 20 块砖（生成 20 个 Token）**。 每次砌一块砖，都需要从仓库里搬运 **140 公斤的原材料（读取 140GB 权重）** 到工位。

*   **需求：** 每秒搬运 $140 \times 20 = 2800$ 公斤。

*   **单人能力（单张 A100）：** 他拼了命每秒只能搬运 **1935 公斤**。

    *   **结果：** 单人肯定干不完，速度达不到每秒 20 块。

#### 双人合作（双卡 A100 + 张量并行）

现在我们请了两个人（GPU A 和 GPU B）。关键在于他们是怎么合作的：

1.  **任务拆分（切分权重）：** 我们把 140 公斤的原材料切成两半。GPU A 负责搬运左边的 70 公斤，GPU B 负责搬运右边的 70 公斤。

2.  **同时干活（带宽叠加）：**

    *   当需要砌第一块砖时，**两个人是同时出发的**。
    *   GPU A 搬运 70 公斤（消耗带宽 1935 的一半能力）。
    *   GPU B 搬运 70 公斤（消耗带宽 1935 的一半能力）。
    *   **总效果：** 两个人加起来，每秒理论最大能搬运 $1935 + 1935 = \mathbf{3870}$ 公斤。

3.  **绰绰有余：** 理论能力 3870 > 需求 2800。这就是“能达标”的基础。

#### “受限于卡间通信”是什么意思？

虽然两个人搬得快，但在把原材料变成砖头（计算结果）之前，他们需要**碰头对一下账**。

*   **过程：**

    1.  GPU A 算出了一半的结果。
    2.  GPU B 算出了另一半的结果。
    3.  **通信（Synchronization）：** 他们必须停下来，互相交换数据，把两半结果拼成一个完整的结果，才能进行下一轮。

*   **损耗：** 这个“互相交换数据”的过程需要走 **NVLink**（GPU 之间的桥梁）。虽然 NVLink 很快（600GB/s），但终究不是瞬时的。

*   **结论：**

    *   理论带宽：3870 GB/s。
    *   扣除“碰头对账”浪费的时间，**有效带宽**可能降到 3200 GB/s 左右。
    *   **但依然大于** 我们需要的 2800 GB/s。所以结论是：**没问题，可以达标。**

***

### 2. 技术视角深入：张量并行 (Tensor Parallelism)

在技术实现上，这背后的机制是 **Tensor Parallelism (TP)**。

#### 为什么带宽会“翻倍”？

在大模型推理中，主要的算子是 **矩阵乘法 (MatMul)**：$Y = W \times X$。 假设权重矩阵 $W$ 很大（140GB）。在 TP 模式下，我们将 $W$ **按列切分**（Column Parallel）：

*   **GPU 1** 只有 $W$ 的左半部分 ($W_1$)。
*   **GPU 2** 只有 $W$ 的右半部分 ($W_2$)。

当需要计算时，**两张卡同时读取各自显存中的 $W_1$ 和 $W_2$**。 因为是**物理上独立的两块显存、两个内存控制器**，所以读取速度是物理叠加的。

$\text{总系统带宽} = \text{GPU1 带宽} + \text{GPU2 带宽}$

对于双卡 A100 (80G PCIe/SXM)： $1935 \text{ GB/s} + 1935 \text{ GB/s} = 3870 \text{ GB/s}$

#### 为什么说“受限于通信”？

在两张卡分别算出 $Y_1 = W_1 \times X$ 和 $Y_2 = W_2 \times X$ 之后，最终的结果 $Y$ 应该是 $Y_1 + Y_2$（或者拼接，取决于切分方式）。

这就涉及到一个必须的操作：**All-Reduce (全归约)**。

1.  GPU 1 必须把自己的 $Y_1$ 发给 GPU 2。
2.  GPU 2 必须把自己的 $Y_2$ 发给 GPU 1。
3.  两张卡相加，得到完整的 $Y$。

这个**跨卡传输**的时间，就是额外的**延迟开销 (Overhead)**。

*   如果使用 **NVLink** (600GB/s)，这个开销很小（微秒级）。
*   如果使用 **PCIe** (32-64GB/s，例如 RTX 4090 双卡)，这个开销会非常大，大到可能抵消掉双卡带来的带宽优势。

#### 总结那个计算

*   **目标：** 需要 2800 GB/s 的等效速度。
*   **现实：** 双卡 A100 提供 3870 GB/s 的物理管道。
*   **折扣：** 假设通信（All-Reduce）消耗了 15% 的时间。
*   **最终效果：** $3870 \times (1 - 0.15) \approx 3289 \text{ GB/s}$。
*   **判定：** $3289 > 2800$。**Success！**

这就是为什么我们在架构设计时，**对于 70B 这样的大模型，且要求高 Token 生成速度时，不仅显存要够，还必须用多卡做 TP 来叠加带宽。**

## TTFT 和 TPOT

你好！我是你的 AI 基础设施架构师。

这一节非常关键。如果说前面的硬件和框架是“内功”，那**性能指标与业务场景的匹配**就是“招式”。招式用对了，才能在用户体验和成本之间找到那个黄金平衡点。

很多时候，老板觉得“模型慢”，其实是因为工程师没搞清楚\*\*在线（Online）**和**离线（Offline）\*\*业务对指标要求的本质区别。

让我们来拆解这一切。

***

### 1. 关键指标详解：TTFT vs. TPOT

我们要把大模型的一次完整响应拆解为两个阶段：**预填充（Prefill）** 和 **解码（Decoding）**。

#### **TTFT (Time To First Token) - 首字延迟**

*   **定义：** 从用户点击“发送”那一刻起，到屏幕上蹦出**第一个字**所消耗的时间。

*   **包含过程：** 网络传输 + 请求排队 + **Prefill 阶段**（GPU 一次性处理用户输入的所有 Prompt tokens）。

*   **物理本质：** **Compute-bound（计算受限）**。因为 GPU 要并行计算输入的所有 token，算力越强，TTFT 越短。

*   **用户心理：** 决定了\*\*“响应是否及时”\*\*。如果 TTFT > 1秒，用户会觉得“卡了”；如果 > 3秒，用户可能会刷新页面。

#### **TPOT (Time Per Output Token) - 生成速度**

*   **定义：** 第一个字出来后，后续每个字蹦出来的**时间间隔**（单位通常是毫秒/token）。也可以倒过来表达为 **Generation Speed (tokens/s)**。

*   **包含过程：** **Decoding 阶段**（自回归生成，一个字一个字出）。

*   **物理本质：** **Memory-bound（带宽受限）**。我们在上一节讲过，这取决于显存带宽。

*   **用户心理：** 决定了\*\*“生成是否流畅”\*\*。

    *   人眼的阅读速度大约是 5-10 tokens/s (TPOT 100ms - 200ms)。
    *   如果系统生成速度（例如 50 tokens/s）远快于阅读速度，用户体验就是极佳的。

***

### 2. 场景关联：怎么调参？

作为架构师，不管是聊天还是做文档总结，我们调节的核心杠杆是 **Batch Size（并发批处理大小）**。

#### **场景 A：在线聊天机器人 (Chatbot)**

*   **核心痛点：** 用户极其没耐心。

*   **敏感指标：** **TTFT 第一，TPOT 第二。**

    *   TTFT 必须 < 0.5s \~ 1s。
    *   TPOT 只要快过人眼阅读速度（< 50ms-100ms）即可，再快用户也读不过来，意义递减。

*   **配置策略：**

    *   **Batch Size 不要太大：** 比如限制在 16 或 32。虽然 Batch Size 大能提升吞吐，但会导致排队时间变长（增加 TTFT），且单次推理变慢（增加 TPOT）。

    *   **优先 Preemption（抢占）：** 必须保证正在聊天的用户流畅，必要时拒绝新请求。

#### **场景 B：离线文档总结/数据分析 (Offline Batch Processing)**

*   **核心痛点：** 有 10 万篇文档要处理，要在明早 8 点前跑完。

*   **敏感指标：** **Throughput (吞吐量) 是唯一的神。**

    *   TTFT 是 1秒 还是 10秒 根本无所谓（没人盯着屏幕看）。
    *   TPOT 慢一点也无所谓。

*   **配置策略：**

    *   **拉满 Batch Size：** 比如设为 128 或 256，直到显存几乎被 KV Cache 撑爆。

    *   **原理：** Batch Size 越大，GPU 每次加载权重能服务的请求就越多，**单位算力成本越低**。

    *   **牺牲延迟换吞吐：** 此时 TPOT 可能高达 200ms（每秒才生成 5 个字），但因为你是 256 个请求并行，总吞吐量是  $5 \times 256 = 1280 \text{ tokens/s}$ 。这比在线场景高得多。

***

### 3. 架构影响：多卡 TP vs. 单卡量化

这是一个非常经典的高级架构面试题：**“我有一台双卡服务器，是把大模型切分到两张卡上跑（TP），还是每张卡各跑一个小一点/量化版的模型？”**

#### 模型并行 (Tensor Parallelism, TP) 的“通信税”

当你使用 TP 将模型切分到两张卡（比如 GPU 0 和 GPU 1）时：

*   **延迟影响：**

    *   **利：** 带宽翻倍，计算分摊。理论上 TPOT 应该减半。

    *   **弊：** 每生成一个 Token，两张卡都要进行 **All-Reduce** 通信。

*   **关键变量 —— 互联带宽：**

    *   **如果用 NVLink (600GB/s)：** 通信极快，利 > 弊。TPOT 显著降低。

    *   **如果用 PCIe (32GB/s)：** 通信极慢。**通信时间可能比推理计算时间还长**。导致 TPOT 反而变慢，得不偿失。

#### 什么时候“单卡量化”比“多卡并行”更好？

**答案：当通信瓶颈 > 带宽红利，或者并发量不大时。**

我们来看一个具体案例：**部署 Llama-3-70B**。

*   **方案 A：双卡 RTX 4090 (24G) 使用 TP=2，跑 FP16 (权重\~140G...等等，4090双卡才48G，必须量化才能跑 TP)**

    *   修正：双卡 4090 跑 70B 必须用 INT4 量化 (35GB 权重)。

    *   **架构：** 模型切分跨两张卡。

    *   **瓶颈：** 4090 **没有 NVLink**。卡间通信走 PCIe 4.0。

    *   **结果：** 每次生成 token 都要走 PCIe 同步。TPOT 可能高达 50-80ms，甚至不如单卡跑小模型快。且两张卡被绑死服务于一个请求，利用率低。

*   **方案 B：单卡 A100 (80G) 跑 FP16**

    *   **结果：** 显存够，带宽够。速度极快。

*   **方案 C (最强对比)：单卡 H100 vs 双卡 A100**

    *   如果跑 70B 模型，**单张 H100 (INT8/FP8)** 通常比 **双卡 A100 (FP16, TP=2)** 延迟更低。

    *   **原因：** H100 单卡带宽高达 3.3TB/s，接近双卡 A100 的总和，且完全没有通信开销（Communication Overhead = 0）。

#### 架构师总结表：如何选择？

| 场景                      | 推荐架构                                | 理由                                                                 |
| :---------------------- | :---------------------------------- | :----------------------------------------------------------------- |
| **追求极致低延迟 (TTFT/TPOT)** | **高性能单卡 (如 H100) + 量化**             | 避免跨卡通信延迟，利用单卡极致带宽。                                                 |
| **显存实在不够 (模型太大)**       | **多卡 TP (必须有 NVLink)**              | 没办法，装不下。必须用带宽叠加来换取运行能力。                                            |
| **消费级显卡 (4090) 组队**     | **Pipeline Parallel (PP) 或 单卡跑小模型** | 4090 没 NVLink，跑 TP 是灾难。不如用 PP（流水线并行，吞吐高但延迟高）或者干脆每张卡跑一个 Llama-3-8B。 |
| **离线高吞吐任务**             | **多卡独立运行 (Data Parallel)**          | 别搞 TP。让每张卡独立跑一个模型实例 (Replicas)，互不干扰，吞吐量翻倍。                         |
