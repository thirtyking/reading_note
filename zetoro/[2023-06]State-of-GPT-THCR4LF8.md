---
tags: []
parent: 'State of GPT'
collections:
    - '0. 综述'
$version: 2205
$libraryID: 1
$itemKey: THCR4LF8

---
\[2023-06]State of GPT

# State of GPT

你好！我是你的 AI 导师。今天我们要一起深入研读的材料非常重磅，它是 AI 领域的“大神”级人物 **Andrej Karpathy**（OpenAI 的联合创始人，前 Tesla AI 高级总监）在 Microsoft Build 2023 大会上所做的演讲，题目是 **《State of GPT》（GPT 的现状）**。

这不仅仅是一次演讲，它实际上是目前**构建类似 ChatGPT 这样的大语言模型（LLM）最权威的“烹饪食谱”和“使用指南”**。

Karpathy 将这次分享分为了两大部分：

1.  **如何训练：** 从 0 到 1 构建一个 ChatGPT 助手。
2.  **如何应用：** 如何有效地使用这些模型（Prompt Engineering）。

下面我为你拆解其中的核心逻辑。

***

### 1. 这个演讲主要解决什么问题？为什么重要？

**核心问题：如何将一个只会“文字接龙”的预训练模型，变成一个有用、听话、安全的 AI 助手？**

**重要性：** 很多初学者以为，把大量数据丢给神经网络训练，它自然就变成了 ChatGPT。其实不是。 基础模型（Base Model）只会不停地续写文本。比如你问它“想吃什么？”，它可能不会回答“汉堡”，而是续写成“想吃什么？去哪里吃？和谁吃？”（因为它见过很多类似的问题列表）。 Karpathy 揭示了 OpenAI 是如何通过一系列精细的步骤，驯服这个“野兽”，让它能够遵循指令、回答问题并保持礼貌。

***

### 2. 训练 GPT 助手的“四步走”配方（核心解决思路）

Karpathy 详细展示了一个 **Pipeline（流水线）**，这是理解现代大模型的关键。

#### **第一阶段：预训练 (Pre-training)**

*   **做什么：** 让模型阅读互联网上几乎所有的文本（CommonCrawl, Wikipedia, GitHub 代码等）。
*   **目的：** 学习语言结构和世界知识。
*   **代价：** 这是最贵的一步。消耗 99% 的算力（几千张 GPU 跑几个月）。
*   **结果：** 得到一个 **Base Model（基座模型）**。它是一个“文档补全机”，而不是助手。
*   **关键点：** Karpathy 提到，这其实是一个**无损压缩**的过程。如果你能完美预测下一个字，说明你理解了世界。

#### **第二阶段：有监督微调 (Supervised Fine-Tuning, SFT)**

*   **难点：** 基座模型不听指令。
*   **做什么：** 雇佣人类写很多高质量的对话数据（比如“用户问：写个代码”，“助手答：好的，代码如下...”）。
*   **结果：** **SFT Model**。现在模型知道该怎么通过对话来回应你了。
*   **局限：** 雇人写完美答案很贵，而且很难写出完美的答案让模型模仿。

#### **第三阶段：奖励建模 (Reward Modeling, RM)**

*   **创新点：** 人类写答案很难，但**人类给答案打分（比较好坏）很容易**。
*   **做什么：** 让模型对同一个问题生成好几个回答，让人类来排名（A 比 B 好）。然后训练一个“打分模型”（Reward Model）来模仿人类的打分标准。

#### **第四阶段：强化学习 (Reinforcement Learning, RLHF)**

*   **核心机制：** 使用 PPO（Proximal Policy Optimization）算法。
*   **做什么：** 让模型自己生成答案，用“打分模型”给它打分。如果分数高，就奖励模型；分数低，就惩罚。
*   **结果：** **RLHF Model**（这才是我们用到的 ChatGPT）。
*   **为什么这么做：** 这种方法能让模型的输出更符合人类的偏好（更有用、更安全）。

***

### 3. 关键架构图解读：GPT 助手训练流水线

视频中有一张贯穿始终的 **"GPT Assistant Training Pipeline"** 图。

**图的内容描述：** 这是一张横向的流程图，分为四个列（阶段）：

1.  **Pretraining（预训练）：** 巨大的数据桶 -> 巨大的 GPU 集群 -> Base Model。
2.  **Supervised Finetuning（有监督微调）：** 人类演示数据 -> SFT Model。
3.  **Reward Modeling（奖励建模）：** 比较数据（Ranking）-> RM Model。
4.  **Reinforcement Learning（强化学习）：** SFT 模型 + RM 模型 -> RLHF Model。

**它如何辅助理解：** 这张图直观地展示了**数据量级和算力投入的递减**：

*   **预训练**是“海量数据 + 暴力算力”（大炼钢）。
*   后面三个阶段是“少量高质量数据 + 精细调整”（做钟表）。
*   它解释了为什么普通人很难从头训练一个模型（因为第一步太贵），但可以在后面几个阶段进行微调。

***

### 4. 如何应用：System 1 vs. System 2 思维

*   **System 1（快思考）：** 这是目前 LLM 的默认状态。它只是在做“下一个词预测”，是直觉式的、线性的、不可回头的。就像你脱口而出一个答案，没经过深思熟虑。
*   **System 2（慢思考）：** 这是人类解决复杂问题的方式。我们会规划、反思、检查错误、修改草稿。

**核心创新应用思路：** Karpathy 指出，**如果你直接问 GPT 一个复杂问题（比如写一首不押韵的诗），它可能会失败，因为它在用 System 1 强行回答。**

我们需要通过 **Prompt Engineering（提示工程）** 强迫模型进入 System 2 模式：

1.  **思维链 (Chain of Thought):** 告诉模型“Let's think step by step”（让我们一步步思考）。这让模型在给出最终答案前，先生成一段“思考过程”，从而提高准确率。
2.  **自我反思 (Reflection):** 让模型自己检查刚才写的代码对不对，不对就重写。
3.  **工具使用 (Tool Use):** LLM 不擅长心算（比如 534 \* 982），不要逼它算。给它一个“计算器”工具，让它生成调用计算器的指令。
4.  **检索增强 (RAG):** 模型的记忆（参数）是有限且过时的。把最新的文档作为“上下文”喂给它（Context Window），就像让学生带着课本去考试。

***

### 5. 总结给学生的话

这篇演讲其实就讲透了两件事：

1.  **模型不是魔法，是工程产物：** ChatGPT 的诞生不是纯粹的黑科技，而是通过**预训练（学知识）** -> **微调（学说话）** -> **强化学习（学价值观）** 这套严密的工程流程制造出来的。
2.  **Prompt 是在以此补短：** 现在的模型本质上是“词语接龙机器”（Token Simulators）。它们没有真正的“脑子”去回溯和纠错。所有的 Prompt 技巧（比如思维链、多样本提示），本质上都是在**弥补模型没有“工作记忆”和“慢思考能力”的缺陷**，通过延长生成的长度，给模型争取“思考”的时间和空间。

Karpathy 最后的建议非常实用：**先用 GPT-4，把 Prompt 写好。只有在 Prompt 实在解决不了问题时，再考虑去微调模型。**
