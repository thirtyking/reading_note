---
tags: []
parent: '[LoRA] LoRA: Low-Rank Adaptation of Large Language Models'
collections:
    - 参数高效微调
$version: 2251
$libraryID: 1
$itemKey: XZH8M6F4

---
\[2021-06]\[LoRA] LoRA: Low-Rank Adaptation of Large Language Models

# \[LoRA] LoRA: Low-Rank Adaptation of Large Language Models

## 概述

你好！我是你的 AI 导师。今天我们要解读的这篇论文是 **LoRA: Low-Rank Adaptation of Large Language Models**（大语言模型的低秩适配）。

如果说 InstructGPT 是教 AI “懂规矩”，那么 LoRA 就是教我们\*\*“如何用最少的钱和显卡，把大模型变成你自己的专属模型”**。这篇论文是目前开源社区微调大模型（Fine-tuning）的**绝对主流技术\*\*，没有之一。

咱们开始吧。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**核心问题：如何极大幅度地降低微调大语言模型的成本（显存和存储），同时保证效果不打折？**

**背景：** 假设你手里有一个 GPT-3（1750 亿参数）。你想让它变成一个“法律专家”。 按照传统的方法（Full Fine-tuning，全量微调），你需要把这 1750 亿个参数全部重新训练一遍。这需要几十张顶级显卡（A100），花费几百万美元，而且每训练一个新任务（比如医学、代码），你都要存一份 350GB+ 的模型文件。

**重要性：** 如果只能全量微调，那么大模型注定只是大公司的玩具，普通开发者和中小企业根本玩不起。 LoRA 的出现，**把微调大模型的门槛从“核电站级”降低到了“家用电器级”**。现在你在自己的游戏本显卡上就能微调一个相当不错的模型，这全靠 LoRA。

***

### 2. 解决这个问题的难点在哪里？之前的方案为何不够好？

**难点：** 既要省钱（参数少），又要效果好（性能不降），还要跑得快（推理无延迟）。这在工程上是一个“既要又要还要”的难题。

**之前的解决方案（及其缺点）：**

1.  **Adapter Layers（适配器层）：**

    *   *做法：* 在模型原有的层之间，插入一些新的小层（神经网络层），只训练这些小层。

    *   *缺点：* **增加了推理延迟（Inference Latency）**。这就好比在高速公路上设了好几个关卡，虽然关卡很小，但车流量大时，整体速度就被拖慢了。

2.  **Prefix Tuning（前缀微调）：**

    *   *做法：* 在输入的 Prompt 前面加一些特殊的、可训练的“虚拟 Token”。

    *   *缺点：* **占用上下文长度（Context Window）**。相当于你为了让模型听话，每次都要在开头念一段很长的咒语，留给真正处理任务的字数就变少了。而且这种方法效果往往不如全量微调。

***

### 3. 本论文提出了何种解决方案？为什么能行？

**解决方案：LoRA（Low-Rank Adaptation，低秩适配）。**

LoRA 的思路非常巧妙，它基于一个数学假设：**大模型虽然参数很多，但针对某个具体任务时，真正起作用的参数变化是“低秩”的（Low-Rank）。**

**通俗解释（Why it works）：** 想象大模型是一个学识渊博的老教授。现在你要让他去考“驾照”。

*   **全量微调：** 是把老教授的脑细胞全部重塑一遍。

*   **LoRA 的假设：** 老教授底子很好，不需要重塑大脑。他只需要在原有知识库旁边，**外挂一个小笔记本**（LoRA 模块），上面记着几条关键的驾驶规则即可。这个笔记本的信息量比起他的大脑来说非常小（低秩）。

**具体做法：**

1.  **冻结原模型：** 也就是老教授的大脑完全不动（不更新参数）。

2.  **旁路更新：** 在原有的矩阵旁边，加两个非常小的矩阵（矩阵 A 和矩阵 B）相乘。

3.  **结果相加：** 最终的输出 = 原模型输出 + 旁路小矩阵的输出。

这样，我们只需要训练那两个极小的矩阵 A 和 B，参数量可能只有原模型的万分之一。

***

### 4. 方案的核心创新点在哪里？

1.  **极致的参数压缩：** LoRA 可以把可训练参数量减少 10,000 倍，显存需求减少 3 倍。这意味着单张消费级显卡（如 RTX 3090/4090）就能跑起来。

2.  **无推理延迟（No Inference Latency）：** 这是 LoRA 最厉害的地方。因为 LoRA 也是矩阵运算，**训练好之后，我们可以把那个“小笔记本”（矩阵 A x B）里的数字，直接加到“老教授的大脑”（原参数 W）里去。**

    *   $W_{new} = W_{old} + (B \times A)$。
    *   在这个加法做完后，模型结构没有任何变化，推理速度和原模型一模一样！

3.  **高效的任务切换：** 原模型只有一份（几百 GB），针对法律、医疗、代码任务，你只需要存对应的 LoRA 小文件（几十 MB）。切换任务时，只需要替换这几十 MB 的小文件即可，非常灵活。

***

### 5. 关键架构图解读

论文中图 1（Figure 1）是理解 LoRA 的灵魂。请在脑海中构建这样一幅图：

#### **图的内容描述：**

*   **左边是一个蓝色的矩形：** 标注为 **"Pretrained Weights" (W)**，且有一个小锁的图标（❄️ Frozen）。

    *   这代表原来的大模型参数，它是冻结的，训练时不许动。
    *   输入数据 $x$ 进去了，输出 $h = Wx$。

*   **右边是一个旁路（Bypass）：** 分为上下两个橙色的矩形。

    *   **下边的矩形 A：** 这是一个很宽但很矮的矩阵（降维）。输入 $x$ 先进这里。
    *   **上边的矩形 B：** 这是一个很窄但很高的矩阵（升维）。A 的输出进这里。
    *   这两个矩阵相乘 ($B \times A$)，代表了参数的变化量 $\Delta W$。
    *   因为中间有一个极其狭窄的通道（Rank $r$，秩很小），所以参数量极少。

*   **顶部有一个加号 ⊕：**

    *   左边的输出（原知识）和右边的输出（新适配的知识）在这里相加。
    *   最终输出 $h = Wx + BAx$。

#### **如何辅助理解方法论：**

这张图完美诠释了 LoRA 的\*\*“旁路”\*\*思想：

1.  **主路不动：** 保证了原模型能力不丢失。

2.  **旁路修修补补：** 专门学习新任务的特定知识。

3.  **先瘦身再还原（低秩分解）：** 矩阵 A 先把数据压缩（比如从 1024 维压到 4 维），矩阵 B 再把数据还原（从 4 维还原回 1024 维）。中间那个“4 维”就是我们说的**秩（Rank）**。正是因为中间这个腰很细，所以计算量和参数量才特别小。

***

### 总结一下给学生的话：

LoRA 这篇论文是 AI 工程化的里程碑。它告诉我们：**不要试图去改变整个世界（全量微调），只需要在关键的地方做一点点微小的调整（低秩适配），就能达到目的。**

正是因为有了 LoRA，现在 GitHub 和 HuggingFace 上才有那么多好玩的社区模型。它让“人人都能微调大模型”成为了现实。

## 思考一

如何在训练过程中做到既不动原有模型的参考，又能按要求微调旁路的参数？ 是不是说在正向传播时原模型和旁路模型都参与预测，反向传播时根据残差只对旁路模型进行梯度下降的微调？不知道我的理解对不对

***

你的理解**非常准确**！完全抓住了 LoRA 的核心逻辑。

用一句大白话来说就是：**“看书（正向传播）的时候两本都看，改错（反向传播）的时候只改那本薄的笔记。”**

为了让你理解得更透彻，我从\*\*“代码/工程实现”\*\*的角度，把你刚才描述的过程拆解一下，你会发现这在现代深度学习框架（如 PyTorch）中实现起来其实非常优雅。

***

### 1. 正向传播（Forward Pass）：两路并行，殊途同归

正如你所说，在预测阶段，数据 $x$ 进来后会分流：

*   **主路（原模型）：** 数据经过冻结的大矩阵$W$（比如 1750 亿参数）。虽然我们不改它，但它**必须参与计算**，因为它提供了基础的特征和逻辑。

    *   $Output_{main} = W \cdot x$

*   **旁路（LoRA）：** 数据同时经过两个可训练的小矩阵$A$和$B$。

    *   $Output_{lora} = B \cdot (A \cdot x)$

*   **合流：** 最终的输出是两者的直接相加。

    *   $Final\_Output = Output_{main} + Output_{lora}$

**注意：** 到这里为止，你是完全正确的，两个模型都参与了预测。

***

### 2. 反向传播（Backward Pass）：设置路障，只改旁路

这是实现“不动原模型、只动旁路”的关键。在深度学习框架（比如 PyTorch）中，这是通过一个简单的“开关”实现的：

1.  **设置路障（Freeze）：** 在训练开始前，我们在代码里把原模型$W$的参数标记为 `requires_grad = False`（不需要梯度）。 这意味着：框架在计算梯度时，会把$W$当作一个**常数**（Constant），而不是变量。

2.  **计算 Loss（误差）：** 模型预测结果和真实标签对比，得出一个 Loss（比如预测是“狗”，实际是“猫”，Loss 就很大）。

3.  **梯度回传（Gradient Flow）：** Loss 开始向后传播，告诉前面的参数“你们该怎么变才能让 Loss 变小”。

    *   当梯度传到\*\*“合流点”\*\*（加号）时，它会向两条路分发。
    *   **通向主路时：** 碰到 `requires_grad = False` 的路障，**梯度直接截断/消失**。计算机不会计算 $W$ 需要怎么变，也不会更新 $W$ 的数值。
    *   **通向旁路时：** 一路畅通。梯度流向矩阵 $B$，再流向矩阵 $A$。优化器（Optimizer）根据这些梯度，**只修改$A$和$B$里的数字**。

***

### 3. 为什么这样能省显存？（核心知识点补充）

你可能会问：*“既然主模型也要参与计算，那为什么 LoRA 能省那么多显存？”*

这是因为在训练大模型时，最吃显存的通常不是**模型参数本身**，而是**优化器状态（Optimizer States）**。

*   **全量微调时：** 如果你要更新 1750 亿个参数，优化器（比如 Adam）需要为每一个参数维护“动量”和“方差”等中间变量。这些中间变量占用的显存通常是模型参数的 **2 倍到 3 倍**！这是巨大的开销。

*   **LoRA 微调时：**

    *   原模型 $W$ 不更新 $\rightarrow$ **不需要**维护 $W$ 的优化器状态（省下巨量显存）。
    *   只有小矩阵 $A$ 和 $B$ 需要更新 $\rightarrow$ 只需要维护极少量的优化器状态。

**总结：** 你的理解完全正确。

*   **正向：** $W$ 和 LoRA 共同出力。
*   **反向：** 误差信号只用来修正 LoRA，$W$ 岿然不动。

这就是 LoRA 能够“四两拨千斤”的本质原因。
