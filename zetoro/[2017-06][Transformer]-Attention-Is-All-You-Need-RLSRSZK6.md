---
tags: []
parent: '[Transformer] Attention Is All You Need'
collections:
    - 核心组件
$version: 2251
$libraryID: 1
$itemKey: RLSRSZK6

---
\[2017-06]\[Transformer] Attention Is All You Need

# Attention Is All You Need

<https://baoyu.io/translations/llm/illustrated-transformer>

## 概括

你好！很高兴能为你解读这篇可以说是 AI 领域「里程碑」式的论文——**《Attention Is All You Need》**。

现在大火的 ChatGPT、Claude，以及各种大语言模型，它们的“老祖宗”就是这篇论文提出的 **Transformer** 模型。如果不理解这篇论文，就无法理解现在的 AI 革命。

咱们不用复杂的数学公式，就用大白话把它的核心逻辑拆解一下。

***

### 1. 本论文要解决什么问题？为什么它很重要？

**核心问题：序列转序列的建模（Sequence-to-Sequence Modeling）。**

简单来说，就是把“一串东西”变成“另一串东西”。最典型的应用就是**机器翻译**（比如把“我爱学习”这串中文，变成“I love studying”这串英文）。

**为什么重要？** 因为人类的语言本质上就是一种“序列”。如果我们能完美解决这个问题，就意味着机器能听懂话、能翻译、能写文章、能写代码。这是实现通用人工智能（AGI）的必经之路。

***

### 2. 解决这个问题的难点在哪里？之前是怎么做的？为什么不够好？

在 Transformer 出现之前，这个领域的霸主是 **RNN**（循环神经网络）和 **LSTM**（长短期记忆网络）。

#### 之前的做法（RNN/LSTM）：

想象一下如果你是以前的 AI，你要翻译一句话，你必须像人类读书一样，**一个词一个词地读**。

*   读了第一个词，记在脑子里（隐藏状态 Hidden State）。
*   带着第一个词的记忆，去读第二个词，更新记忆。
*   以此类推，直到读完。

#### 存在两大硬伤：

1.  \*\*无法并行计算（慢）：\*\*这是最致命的。因为必须读完第 1 个词才能读第 2 个，这导致计算过程必须按顺序排队。哪怕你买了 1000 块显卡（GPU），它们也得等着，没法大家一起干活。这就导致训练时间极长，很难训练超大规模的数据。
2.  \*\*长距离依赖问题（忘）：\*\*当句子很长时，读到最后几个词，AI 往往已经忘了开头讲的是什么了。虽然 LSTM 缓解了这个问题，但对于超长文本，依然力不从心。就像你读一本厚书，读到大结局时，可能已经想不起第一章那个配角叫什么了。

在此之前，人们尝试在 RNN 上加一种叫\*\*Attention（注意力机制）\*\*的补丁，试图让 AI 回头看之前的词，但只要底层架构还是 RNN，那个“必须要排队”的根本问题就没解决。

***

### 3. 本论文提出了何种解决方案？为什么能解决？

论文题目叫《Attention Is All You Need》（只需要注意力机制），这就很霸气。意思是：**把 RNN 那个“排队”的结构扔掉，只保留注意力机制！**

作者提出了一个全新的架构：**Transformer**。

#### 它是怎么解决问题的？

Transformer 放弃了“一个词一个词读”的模式，它开启了\*\*“上帝视角”\*\*。

*   **并行化（解决慢）：** 当 Transformer 看到一句话时，它不是按顺序读，而是**一眼就把整句话所有的词同时读进去**。这样，所有的计算可以同时在显卡上跑，效率提升了成千上万倍。

*   **自注意力机制（解决忘）：** 因为它一眼看到了所有的词，所以不管句子多长，句子开头的词和结尾的词之间的“距离”都是 1。它不需要费劲记忆，因为它随时都能看见所有的词。

**形象的比喻：**

*   **RNN** 像是**传声筒游戏**：每个人只把听到的传给下一个人，传到最后，信息可能就丢了，而且必须一个人传完下一个人才能听。

*   **Transformer** 像是**圆桌会议**：所有人（词）都坐在圆桌边，每个人都能直接听到其他人说话，没有中间商赚差价，而且大家可以同时交流。

***

### 4. 方案的核心创新点在哪里？

为了实现这个“上帝视角”，Transformer 设计了几个精妙的机关：

#### A. 自注意力机制（Self-Attention）

这是灵魂所在。 当模型处理“Bank”这个词时，它怎么知道是“银行”还是“河岸”？ Self-Attention 让“Bank”这个词去和句子里**其他所有的词**都“握一下手”，计算一下关联度。

*   如果句子里有“Money”，那“Bank”就更关注“Money”，从而确认自己是“银行”。

*   如果句子里有“River”，那“Bank”就确认自己是“河岸”。**创新点：** 让单词自己去寻找它应该关注的上下文，而不是死板地看前后文。

#### B. 多头注意力（Multi-Head Attention）

如果只用一次注意力，可能只关注到了“语法”关系。 作者搞了“多头”，就好比让 AI 拥有**多个分身**，每个分身关注不同的侧面：

*   1 号头关注语法结构。
*   2 号头关注指代关系（比如“它”指的是谁）。
*   3 号头关注情感色彩。最后把大家看到的信息拼起来，理解就非常立体、深刻。

#### C. 位置编码（Positional Encoding）

因为 Transformer 是“一眼全看”，它本身是不在乎词的顺序的（“我爱你”和“你爱我”对它来说，单纯看词是一样的）。 为了解决这个问题，作者给每个词强行加了一个\*\*“位置身份证”\*\*。

*   给“我”打上标签：位置 1。
*   给“爱”打上标签：位置 2。这样模型既享受了并行计算的快感，又没有丢失语序的信息。

***

### 总结

作为学生，你只需要记住 **《Attention Is All You Need》** 做到了三件事：

1.  **抛弃了 RNN：** 打破了序列模型必须“按顺序算”的魔咒。

2.  **极致的并行：** 释放了 GPU 的算力，让训练海量数据（如后来的 GPT-4）成为可能。

3.  **全连接的视野：** 通过 Self-Attention，让长距离的词之间可以无损地建立联系。

这篇论文结束了 LSTM 的统治时代，开启了 Transformer 的大模型时代。

***

## 思考一

1.  我知道单层Q，K，V是如何计算的，请问多层间是如何连接的？层间Q，K，V是什么关系？
2.  论文中的Q，K，V三个矩阵如何迭代更新？
3.  为什么需要encoder和decoder两个过程？encoder是不是只能看到输入，而decoder不仅能看到输入还能看到已经生成的输出？这和decoder only的架构有什么区别？各自的优缺点是什么？

***

很高兴看到你已经理解了单层 Attention 的核心（Q、K、V 的计算），这说明你已经跨过了最难的一道坎！

你的这三个问题非常关键，触及了 Transformer 架构的**骨架（层级结构）**、**灵魂（训练机制）和变体（架构选型）**。咱们继续用通俗易懂的方式来拆解。

***

### 1. 多层间是如何连接的？层间 Q、K、V 是什么关系？

这是一个非常容易混淆的点。很多初学者以为第一层的 Q 会传给第二层做 Q，这是**错误**的。

#### 层与层的连接方式：接力棒

Transformer 是由 N 个结构相同的“层（Layer/Block）”堆叠起来的（原论文是 6 层，现在的大模型通常几十层）。

*   **输入流：** 第一层的输入是单词的 Embedding（向量）。

*   **传递：** 第一层计算完，输出一个新的向量矩阵，这个矩阵直接作为**第二层的“输入”**。

*   **残差连接（Residual Connection）：** 这里有个细节，为了防止每层处理后把原始信息搞丢了，每一层的输出其实是 `Input + Layer(Input)`。这叫残差连接，就像咱们传话，不仅传处理后的意思，把原话也带着，保证信息不失真。

#### 层间 Q、K、V 的关系：各自为政

每一层都有**自己独立的一套参数矩阵**（$W_Q, W_K, W_V$）。

*   **第 1 层：** 拿到原始输入 $X$，用第 1 层的权重算出 $Q_1, K_1, V_1$，做完 Attention 得到输出 $X'$。
*   **第 2 层：** 拿到第 1 层的输出 $X'$，把它当作自己的输入，用第 2 层的权重算出 $Q_2, K_2, V_2$。

**总结：** 层与层之间**没有**直接传递 Q、K、V。它们传递的是“加工后的向量”。每一层都重新根据输入生成自己的 Q、K、V。

*   **比喻：** 就像写一篇公文，要经过 6 个部门审批。

    *   第 1 层（初审部）读了文章，划重点（QKV），修改了一版（输出）。

    *   第 2 层（复审部）拿到的是初审部改过的文章，它**重新**读一遍，重新划它眼里的重点（QKV），再修改一版。

***

### 2. 论文中的 Q、K、V 三个矩阵如何迭代更新？

这里需要纠正一个概念：**Q、K、V 本身是不迭代更新的，迭代更新的是生成它们的“权重矩阵”。**

#### 这里的逻辑链条：

1.  **权重矩阵（Weights）：** 模型里存着三个大矩阵 $W_Q, W_K, W_V$。这是模型真正的“大脑”，刚开始是随机初始化的（全是乱码）。
2.  **生成 QKV：** 数据 $X$ 进来，乘以权重矩阵，才算出了 $Q, K, V$。（公式：$Q = X \times W_Q$）。
3.  **计算 Loss：** 模型算完得到结果（比如翻译结果），跟标准答案对比，发现错了。
4.  **反向传播（Backpropagation）：** 这种错误会生成“梯度”，告诉模型 $W_Q, W_K, W_V$ 应该怎么调才能对。
5.  **更新权重：** 优化器（Optimizer）修改 $W_Q, W_K, W_V$ 的数值。

**总结：** 在训练过程中，随着成千上万次迭代，**$W_Q, W_K, W_V$ 这三个参数矩阵会越来越精准**。 下一次同样的数据 $X$ 进来，乘以更新后的 $W$，生成的 $Q, K, V$ 就会更准确，Attention 找得就越准。

***

### 3. Encoder 和 Decoder 的爱恨情仇

#### Q: 为什么需要 Encoder 和 Decoder 两个过程？

这源于 Transformer 最早是为了解决**机器翻译**（Sequence-to-Sequence）任务。

*   **Encoder（编码器）：负责“读懂”原文。它需要看全貌。比如翻译“I ate an apple”，它不仅要看“ate”，还要看前后的“I”和“apple”才能理解时态和语境。所以 Encoder 是双向可见**的（上帝视角）。

*   \*\*Decoder（解码器）：负责“写出”译文。\*\*它负责生成。比如已经生成了“我吃了”，下一步要生成“一个”。生成时，它既要看已经写出来的“我吃了”（自回归），又要看 Encoder 递给它的“原文理解”（Cross Attention）。

#### Q: 它们能看到什么？

*   **Encoder：** 看到完整的输入句子（比如整句英文）。

*   **Decoder：**

    1.  **看自己（Self-Attention）：** 只能看到**已经生成的词**。为了防止它偷看后面还没生成的词（作弊），我们在训练时会加一个 **Mask（掩码）**，把后面的词挡住。

    2.  **看 Encoder（Cross-Attention）：** 这是一个特殊环节。Decoder 的每一层在做完自己的 Self-Attention 后，会有一个 Cross-Attention 层。**这里的 Q 来自 Decoder（我想要什么），而 K 和 V 来自 Encoder（原文提供了什么）。** 这是两者交互的桥梁。

#### Q: Decoder-only (如 GPT) vs Encoder-Decoder (如 BERT/T5/Transformer原版)

现在的趋势是 Decoder-only（GPT 系列）称霸，为什么？

| 架构类型                | 代表模型                       | 结构特点       | 优点                                                                | 缺点                                                               |
| ------------------- | -------------------------- | ---------- | ----------------------------------------------------------------- | ---------------------------------------------------------------- |
| **Encoder-Only**    | BERT                       | 只有编码器，双向理解 | **理解能力最强**。非常擅长做完形填空、情感分析、分类任务。                                   | **不擅长生成**。因为它像个阅读理解大师，但不会说话。                                     |
| **Encoder-Decoder** | Transformer (原版), T5, BART | 完整的读+写结构   | **翻译、摘要任务的神**。因为输入和输出是两种不同的语言或形式，需要先把输入彻底读懂再转换。                   | 结构复杂，训练效率稍低。在纯粹的“自由创作”上不如 Decoder-only 顺畅。                       |
| **Decoder-Only**    | **GPT 系列**, LLaMA          | 只有解码器，单向生成 | **生成能力最强**。就像人说话一样，想在这个词后面接什么词。这种模式最符合“通用生成”的逻辑，且模型结构简单，容易堆叠层数做大。 | 在需要极度精确理解上下文（比如从长文中抽取这一句的各种成分）时，理论上比 BERT 稍弱（虽然现在大力出奇迹，规模弥补了缺陷）。 |


**核心总结：**

*   **Encoder-Decoder**：适合“输入 -> 转换 -> 输出”的任务（如中译英）。

*   **Decoder-only**：适合“输入 -> 续写”的任务（如对话、写小说、写代码）。

因为现在的 AI 发展趋势是\*\*“一切任务皆可看作续写（Next Token Prediction）”\*\*，所以 GPT 这种 Decoder-only 架构成了主流。

## 思考二

1.  在训练过程中，训练数据是什么样的？模型输出结果与什么内容相比较来决定误差？每一层都把输出加下输入，有没有可能导致最后输出的结果和第一层的输入是一样的？

***

这个问题非常敏锐！你已经开始思考模型内部的数据流向和训练本质了。咱们把这三个问题拆开来讲，你会发现其中的逻辑非常精妙。

### 1. 训练数据是什么样的？模型输出跟什么比？

为了方便理解，我们还是以**机器翻译**（中文“我爱学习” -> 英文“I love studying”）为例。Transformer 的训练采用了一种叫 **Teacher Forcing（强制教学）** 的模式。

#### **A. 喂给模型的数据（Input）**

训练时，我们是一次性把数据丢进去的，但分为两部分：

1.  **给 Encoder 的数据（源语言）：**

    *   输入：`["我", "爱", "学", "习"]`

    *   这部分模型是可以随便看的，用来理解语义。

2.  **给 Decoder 的数据（目标语言 - 带掩码）：**

    *   **关键点：** 我们在训练时，其实**已经知道**标准答案是 "I love studying"。但我们不能直接全给它看，否则它就“抄答案”了。

    *   我们会把标准答案**错位**输入给 Decoder。

    *   比如，当模型要预测第 1 个词时，我们要给 Decoder 输入一个起始符 `<SOS>` (Start Of Sentence)。

    *   当模型预测第 2 个词时，给它看 `<SOS> I`。

    *   **在 Transformer 内部的实际操作中**：我们是一次性把 `["<SOS>", "I", "love", "studying"]` 作为一个矩阵扔给 Decoder，但是！我们会利用 **Mask（掩码矩阵）** 把未来的词挡住。

        *   模型在算 "I" 的位置时，只能看到 `<SOS>`。

        *   模型在算 "love" 的位置时，只能看到 `<SOS>` 和 `I`。

#### **B. 模型用来比较的对象（Target / Label）**

模型输出的是一个**概率分布**（比如字典里有 10000 个词，它会输出 10000 个概率值）。

*   **第一步目标：** 输入 `<SOS>`，我们希望模型输出 `I` 的概率最大。

*   **第二步目标：** 输入 `<SOS> I`，我们希望模型输出 `love` 的概率最大。

*   **第三步目标：** 输入 `... studying`，我们希望模型输出 `<EOS>` (End Of Sentence，结束符)。

**比较过程（计算 Loss）：** 模型计算出的概率（比如它预测 "love" 的概率是 0.1，预测 "like" 是 0.8） **VS** 真实标签（"love" 是 100%）。 算法（通常是**交叉熵损失 Cross Entropy Loss**）会计算这两个分布的差距。差距越大，Loss 越大，模型就要挨打（更新参数）。

***

### 2. 每一层都“加”输入，最后结果会不会和第一层一样？

这是一个非常棒的直觉！既然 $Output = Input + Change$，那加了 6 层甚至 100 层之后，原始的 $Input$ 不就一直占主导地位吗？最后输出的不就是“原始输入 + 一点点杂音”吗？

答案是：**不会。** 这里有两个核心机制在起作用。

#### **机制一：Layer Normalization（层归一化）—— 数据的“整形师”**

你注意到了公式是 $x + \text{Layer}(x)$，但完整的公式其实是： $\text{LayerNorm}(x + \text{Layer}(x))$

这步 **Normalization** 非常关键！ 无论 $x$ 和 $\text{Layer}(x)$ 加完之后数值变得多大（比如加到了 1000、10000），Normalization 都会把它们强行拉回到一个标准的分布（比如均值为 0，方差为 1 的范围内）。

*   **形象比喻：就好比做千层饼。虽然每一层都往上叠面皮（加法），导致饼越来越厚（数值变大），但每叠一层，都有一个“压面机”（Normalization）把它压实、压扁到标准的厚度。所以，数值绝对大小不会无限膨胀，但内部的信息结构**已经发生了翻天覆地的变化。

#### **机制二：向量空间的旋转与变换**

虽然数学上是“相加”，但在高维空间里，这意味着**信息的演变**。

*   **第一层的输入：** 是\*\*“词本身的意思”\*\*。

    *   比如“苹果”这个向量，只代表一种水果。

*   **每一层的 Update（$Layer(x)$）：** 是在\*\*“注入上下文信息”\*\*。

    *   第 1 层发现旁边有“乔布斯”，于是 $Layer(x)$ 产生了一个指向“科技公司”方向的向量。
    *   $x + Layer(x)$ 之后，这个向量的方向就从“水果区”偏转到了“科技区”。

*   **最后一层的输出：虽然它保留了第一层的影子（残差连接），但经过几十次的“微调”和“偏转”，这个向量在空间中的位置，已经从单纯的词汇空间**（Word Embedding）跑到了**语义空间**（Contextual Embedding）。

**总结：** 残差连接保证了\*\*“如果你什么都不懂，至少别把原来的忘了”\*\*，但每一层的 Attention 都在努力把向量推向更准确的语义方向。加上 Normalization 的控制，最后的输出早就“脱胎换骨”了，而不是简单的原地踏步。

## 思考三

1.  是否可以理解，没有残差连接时，训练的是如何把输入变成预期的输出；有残差连接时，训练的是如何在输入上不断修改最后产出预期的输出？

***

**太精准了！** 你这句话简直是对残差连接（Residual Connection）最本质、最完美的概括。

这就是 2015 年 ResNet（残差网络）论文的核心思想，也是 Transformer 能成功的基石。你的理解完全正确，我来帮你把这个理解进一步升华一下。

### 1. 两种模式的深层对比

#### 模式 A：没有残差连接（传统的“重写模式”）

*   **目标：** 训练网络如何把输入 $x$ 变成预期的 $y$。
*   **公式：** $y = F(x)$
*   **潜台词：** “我不管你上一层给我的 $x$ 是什么，我要通过我的计算，直接生成一个新的结果。”
*   **难度：** 就像每一层都要**在一张白纸上画出完整的画**。如果层数很深，中间只要有一层画歪了或者没画好，后面的层就很难补救，因为原始信息已经丢了。

#### 模式 B：有残差连接（Transformer 的“修订模式”）

*   **目标：** 训练网络如何在输入$x$上进行**微调（Modify）**，使其接近预期的$y$。

*   **公式：**$y = x + F(x)$

    *   这里的  $F(x)$  不再是“结果”，而是\*\*“残差”（即差值/变化量）\*\*。

*   **潜台词：** “上一层给我的$x$我先留着，我只负责计算**需要补充或修正的那一点点信息**$F(x)$，然后叠加上去。”

*   **难度：** 就像**老师改作文**。学生已经写了个初稿（$x$），老师不需要重写一篇，只需要用红笔在上面改改词、加加句（$F(x)$）。这显然比重写容易得多！

***

### <span style="background-color: rgba(255, 212, 0, 0.5)">2. 为什么“学修改”比“学重写”更容易？</span>

这里有一个数学上的直觉，叫\*\*“恒等映射”（Identity Mapping）\*\*。

假设某一层网络的输入 $x$ 已经非常完美了（比如在前几层已经处理得很好），不需要再做任何处理。

*   \*\*在“重写模式”下：\*\*网络必须拼命去学习一组复杂的参数，让 $F(x) \approx x$（把输入复制一遍）。这对神经网络来说，其实是一件挺费劲的事（它得把权重矩阵变成单位矩阵）。
*   **在“修订模式”下：网络只需要让$F(x) \rightarrow 0$（把权重都设为 0）。这对神经网络来说太简单了**，它只要“什么都不做/躺平”，就能完美保留上一层的信息。

**结论：** 残差连接让模型的**起跑线变高了**。模型默认就是“及格”的（至少保留了原样），它只需要努力去学习如何从“及格”变成“优秀”。

### 3. 在 Transformer 中的实际意义

回到 Transformer 的语境，这种“不断修改”的过程体现得淋漓尽致：

1.  **初始状态：** 输入向量只是单纯的字面意思（比如“苹果”的 Embedding）。

2.  **第 1 层修订：** Attention 发现上下文有“手机”，于是计算出一个“科技属性”的差值，加到原来的向量上。

    *   *当前状态 = 字面意思 + 科技属性*

3.  **第 2 层修订：** 发现它是主语，于是计算出一个“主语语法”的差值，加了上去。

    *   *当前状态 = 字面意思 + 科技属性 + 主语语法*

4.  **第 N 层修订：** ...

**最后输出的结果，就是把每一层的“修改意见”全部累加在原始输入上的集大成者。**

总结：你的理解非常深刻，这就是**深度学习从“炼金术”走向“工程化”的一个关键转折点**——与其让模型每次都重新发明轮子，不如让它站在巨人的肩膀上做微小的改进。

## 思考四

Transformer架构在训练时的数据流和工作流是怎么样的？在推理时的数据流和工作流通是什么样的？两者有什么不同？

***

这是一个非常核心的工程问题！很多初学者最容易晕的地方就在这里：**为什么训练的时候说是“并行”的，而到了用的时候又变成了“一个词一个词蹦”？**

这两者的**数据流（Data Flow）和工作流（Workflow）有着本质的区别。我们用“刷题模式”**（训练）和\*\*“考试模式”\*\*（推理）来类比。

***

### 1. 训练时（Training）：刷题模式（Teacher Forcing）

**核心特征：并行计算、上帝视角、强行矫正。**

假设我们要训练模型把 `"我爱学习"` 翻译成 `"I love study"`。

#### **数据流与工作流：**

1.  **准备数据（一次性打包）：**

    *   **Encoder 输入：** `["我", "爱", "学", "习"]`

    *   **Decoder 输入：** `["<SOS>", "I", "love", "study"]` （注意：这是**标准答案**，且最前面加了开始符）。

    *   **期望输出（Label）：** `["I", "love", "study", "<EOS>"]` （注意：这是标准答案，往左移了一位）。

2.  **Encoder 工作（看全貌）：**

    *   Encoder 一眼看完了 `"我爱学习"` 整句话，生成了一组语义向量（  $K, V$  ）。这部分和推理时是一样的。

3.  **Decoder 工作（作弊式并行）：**

    *   **关键点：** 我们**不需要**等模型预测出 "I" 之后，再把 "I" 喂给它去预测 "love"。

    *   **Teacher Forcing（强制教学）：** 我们直接把正确的 `"I"` 喂给它，问它：“在前面是 `<SOS>` 的情况下，下一个词应该是啥？”（期望它回答 "I"）。

    *   同时（并行地），把 `"I love"` 喂给它，问：“下一个词是啥？”（期望回答 "study"）。

    *   **掩码（Mask）的作用：** 为了防止模型偷看，虽然我们把整句 `"I love study"` 都喂进去了，但在计算 `"I"` 的位置时，Attention 机制里的 Mask 会把后面的 `"love study"` 盖住。它物理上在矩阵里，但逻辑上看不见。

4.  **计算 Loss：**

    *   模型一次性输出了 4 个预测结果。

    *   拿着这 4 个结果跟标准答案 `["I", "love", "study", "<EOS>"]` 逐一对比，算出总误差，然后反向传播更新参数。

**总结：** 训练时，Decoder 是**并行**的。虽然逻辑上是序列，但计算上是**矩阵运算**，一次搞定。

***

### 2. 推理时（Inference）：考试模式（Autoregressive）

**核心特征：串行计算、自回归、步步惊心。**

现在模型训练好了，我要让它翻译 `"我爱学习"`。此时我们**没有**标准答案。

#### **数据流与工作流：**

1.  **Encoder 工作（只做一次）：**

    *   把 `"我爱学习"` 读进去，生成语义向量（  $K, V$  ）。这就完事了，Encoder 可以在旁边歇着了。

2.  **Decoder 工作（死循环）：**

    *   **第 1 步：**

        *   输入：`["<SOS>"]`

        *   结合 Encoder 的信息，模型算出概率最大的词：`"I"`。

        *   *输出：* `"I"`

    *   **第 2 步：**

        *   输入：拿着刚才生成的 `"I"`，拼接到输入后面，变成 `["<SOS>", "I"]`。

        *   再跑一遍 Decoder，算出下一个词：`"love"`。

        *   *输出：* `"love"`

    *   **第 3 步：**

        *   输入：把 `"love"` 拼进去，变成 `["<SOS>", "I", "love"]`。

        *   再跑一遍，算出：`"study"`。

    *   **第 4 步：**

        *   输入：`["<SOS>", "I", "love", "study"]`。

        *   算出：`"<EOS>"`（结束符）。

    *   **终止：** 看到 `<EOS>`，循环结束。

**总结：** 推理时，Decoder 是**串行**的。必须算出上一个词，才能算下一个词。这就叫**自回归（Auto-Regressive）**。

***

### 3. 两者有什么不同？（核心对比表）

| 特性             | **训练过程 (Training)**                                 | **推理过程 (Inference)**                                                                     |
| -------------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **输入数据**       | **一次性给全**。既给源句子，也给目标句子（标准答案）。                       | **挤牙膏**。只给源句子。目标句子靠自己一个词一个词生出来。                                                          |
| **Decoder 状态** | **并行 (Parallel)**。利用 GPU 矩阵运算，一次预测所有位置的词。           | **串行 (Serial)**。For 循环，上一步的输出是下一步的输入。                                                    |
| **能否看到“未来”**   | 物理上能看到（都在矩阵里），但**逻辑上用 Mask 挡住了**。                   | **完全看不到**。因为未来的词还没生成出来呢。                                                                 |
| **依赖对象**       | **依赖标准答案 (Ground Truth)**。不管上一词预测得对不对，下一词都强行输入正确的词。 | **依赖自己 (Self-Generated)**。如果上一个词预测错了（比如把 I 预测成了 You），那后面所有的预测都是基于这个错误的“You”进行的（一步错，步步错）。 |
| **速度**         | **极快**。GPU 满载运行。                                    | **较慢**。因为没法并行，必须排队。                                                                      |


### 4. 深入理解：为什么会有“暴露偏差”（Exposure Bias）？

通过对比流程，你会发现一个潜在的问题：

*   **训练时：** 模型过得很舒服。哪怕它第 1 个词猜错了，第 2 个词输入时，老师（Teacher Forcing）还是会给它**正确的**第 1 个词作为提示。它永远在“正确的前缀”下做预测。

*   **推理时：** 没人帮它了。一旦第 1 个词猜错，第 2 个词就是基于错误的输入在猜，错误会累积。

这就是学术界常说的**Exposure Bias（暴露偏差）**——训练和推理的环境不一致导致的问题。虽然现在的大模型靠海量数据和强力结构缓解了这个问题，但它依然是 Transformer 架构的一个固有特性。

**一句话总结：** **训练是“拿着答案对答案”，可以一眼看全；推理是“闭卷考试写作文”，必须写完这句才能想下一句。**

## 思考五

Decoder推理时是如何预测下一个词的？预测出的向量又是如何映射回文字的？从工作流和数据流的角度解释一下。

***

这个问题直击了 Transformer 怎么“说话”的核心。

把向量变成文字，并不是像魔法一样直接变出来的，而是一个严谨的数学计算过程。我们可以把它拆解为 **“向量传输带”**（数据流）和 **“打分选择器”**（工作流）。

假设现在的场景是：模型已经输出了 `"I love"`，正在预测下一个词（虽然我们知道是 `"study"`，但模型还不知道）。

***

### 第一阶段：数据流（从向量到“隐状态”）

**1. 输入准备：**

*   **输入内容：** `["<SOS>", "I", "love"]`
*   **数字化：** 查表（Embedding Table），把这三个词变成三个向量。加上位置编码。
*   **输入维度：** 假设模型维度（$d_{model}$）是 512，那么输入就是一个 `3 x 512` 的矩阵。

**2. Decoder 内部流转：**

*   这个 `3 x 512` 的矩阵经过 Decoder 的 N 层处理（Self-Attention + Cross-Attention + FFN）。

*   **输出结果：** 依然是一个 `3 x 512` 的矩阵。

*   **关键点：** 我们**只关心最后一个向量**！

    *   第一个向量对应 `<SOS>` 的理解。

    *   第二个向量对应 `"I"` 的理解。

    *   **第三个向量**（对应 `"love"` 的位置）：这才是集大成者。它包含了“前面是 I，当前是 love，Encoder 原文是我爱学习”的所有信息。**这个向量就是用来预测下一个词的种子选手。**

    *   我们把它取出来，记作 **$h_{last}$**（维度 1 x 512）。

***

### 第二阶段：从向量到分数（Linear Head）

这是“向量”变成“文字”的**物理接口**。

**1. 词表映射（Linear Layer / Unembedding）：**

*   模型里存着一张巨大的**词汇表**，比如有 50,000 个词（Token）。
*   最后这一层是一个全连接层（Linear Layer），它的形状是 `[512, 50000]`。
*   **计算：** $h_{last} \times W_{output}$。
*   **结果：** 得到一个 `1 x 50000` 的超长向量（Logits）。

**2. 形象理解：打分板**

*   这 50,000 个数字，分别代表了字典里每一个词“想成为下一个词”的**欲望强度（得分）**。

    *   `"study"` 对应的索引位置，得分可能是 15.8（很高）。

    *   `"apple"` 对应的索引位置，得分可能是 -3.2（很低，上下文不通）。

    *   `"you"` 对应的索引位置，得分可能是 5.1（有点可能，但不如 study）。

***

### 第三阶段：从分数到概率（Softmax）

上面的得分（Logits）范围很大，有正有负，没法直接用。我们需要把它变成**概率**。

**操作：Softmax 函数** $\text{Probability} = \text{Softmax}(\text{Logits})$

*   它把 50,000 个得分归一化，让它们加起来等于 1（100%）。

*   结果示例：

    *   `P("study")` = 0.85

    *   `P("you")` = 0.10

    *   `P("apple")` = 0.00001

    *   ...

***

### 第四阶段：从概率到选择（Decoding Strategy）

拿到概率后，模型怎么决定最终输出哪个词？这里有几种**策略**（这也是你用 ChatGPT 时调整 Temperature 的原理）：

**策略 A：贪婪搜索 (Greedy Search)**

*   **做法：** 谁概率最大选谁。

*   **结果：** 直接选 0.85 对应的词。

*   **优点：** 稳定。**缺点：** 容易死板，如果不小心选错一个，后面就全错了。

**策略 B：采样 (Sampling)**

*   **做法：** 按照概率掷骰子。

*   `"study"` 概率大，被选中的机会就大，但偶尔也可能选中概率稍低的 `"you"`。

*   **结果：** 得到了一个**整数索引（Token ID）**，比如 `index = 888`。

*   **优点：** 有创造性，生成的文本更多样化。

***

### 第五阶段：从索引回归文字（Tokenizer Decode）

**查字典（Detokenization）：**

*   拿着刚才选出来的 ID `888`。

*   去词表（Vocabulary）里反向查找。

*   发现 `888` 对应的字符串是 `"study"`。

*   **输出：** 把 `"study"` 打印到屏幕上。

***

### 总结整个工作流

1.  **输入：** `["I", "love"]` (IDs)

2.  **Transformer 计算：** 生成包含上下文信息的**最后一个隐向量** (Vector 512维)。

3.  **投影 (Linear)：** 把 512维向量 炸开成 50,000维的**打分表** (Logits)。

4.  **归一化 (Softmax)：** 把打分变成**概率** (Probabilities)。

5.  **抽卡 (Decoding)：** 根据概率选出一个 **ID** (Integer)。

6.  **查表：** 把 ID 变回**文字** (String)。

这一套流程走完，生成了 `"study"`。 然后，模型会把 `"study"` 拼接到 `"I love"` 后面，变成 `"I love study"`，作为新的输入，开始下一轮循环，直到生成 `<EOS>` 为止。
