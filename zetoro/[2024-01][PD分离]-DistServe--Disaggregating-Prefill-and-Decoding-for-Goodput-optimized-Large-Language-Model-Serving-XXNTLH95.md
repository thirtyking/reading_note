---
tags: []
parent: '[PD分离] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving'
collections:
    - 服务架构
$version: 2241
$libraryID: 1
$itemKey: XXNTLH95

---
\[2024-01]\[PD分离] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving

# \[PD分离] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving

## 概述

你好！很高兴能为你解读这篇非常有影响力的论文 **DistServe**。作为你的导师，我希望帮你跳过复杂的数学公式，直接抓住它最核心的逻辑。

你可以把这篇论文想象成是在优化一个\*\*“厨房出餐流程”\*\*。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**一句话总结：** 现有的 LLM（大语言模型）服务系统在处理“阅读提问”和“生成回答”这两个阶段时，因为资源争抢导致效率打折，本论文旨在解决由此引发的卡顿和延迟问题。

**详细解释：** 在大模型推理中，处理一个请求其实分为两个完全不同的阶段：

1.  **Prefill（预填充阶段）：** 模型“阅读”你的输入（Prompt），计算并理解上下文。这是一个**一次性、高算力**的过程。

2.  **Decoding（解码阶段）：** 模型“逐字生成”回复。这是一个**串行、长耗时**的过程，每生成一个字都要访问一次内存。

**为什么重要？** 现有的系统（比如 vLLM 等）通常把这两个过程放在同一个 GPU 上混合处理。这就导致了一个严重的矛盾：

*   **用户体验的指标有两个：**

    1.  **首字延迟（TTFT）：** 你点发送后，多久蹦出第一个字？（越快越好）

    2.  **生成速度（TPOT）：** 字蹦出来的流畅度，是不是像机关枪一样快？（越快越好）

*   **矛盾点：** 为了让“首字”快，GPU 需要全力运算；但如果此时 GPU 正在全力“生成”别人的回复，就会产生冲突。**这导致很难同时满足这两个指标**。这篇论文就是要解决这种“顾此失彼”的困境。

***

### 2. 解决这个问题的难点在哪里？之前的方案为什么不行？

**难点：计算特性的冲突（Compute-bound vs. Memory-bound）。**

*   **Prefill 阶段**是“计算密集型”（Compute-bound），它像是一个大力士，需要瞬间爆发力，吃满算力。

*   **Decoding 阶段**是“内存带宽密集型”（Memory-bound），它像是一个搬运工，算力用得少，但需要频繁搬运数据。

**之前的解决方案：Continuous Batching（连续批处理，如 Orca, vLLM）**

*   **思路：** 只要 GPU 还有空位，就立刻塞新的请求进去，不管旧的请求是不是还在生成中。

*   **为什么不能完美解决？** 产生了\*\*“干扰”（Interference）\*\*。

    *   想象你在写作文（Decoding，生成阶段），突然老师扔给你一本厚书让你立刻读完（Prefill，新请求进来了）。

    *   为了读这本书，你必须停下手中的笔。这就导致写作文的过程卡顿了。

    *   在 GPU 上，新请求的 Prefill 阶段会抢占大量计算资源，导致正在 Decoding 的任务被迫暂停或变慢。这使得**生成速度忽快忽慢，无法保证服务质量（SLO）**。

***

### 3. 本论文提出了何种解决方案？为什么能解决？

**核心思路：PD 分离（Prefill-Decoding Disaggregation）。** DistServe 提出：既然两者的性格如此不合，干脆**分家**！

**解决方案：** DistServe 不再让一个 GPU 既干 Prefill 又干 Decoding，而是把 GPU 集群划分成两个专属的小组：

1.  **Prefill 节点（阅读组）：** 专门负责处理新进来的请求，读完提示词，生成中间状态（KV Cache）。

2.  **Decoding 节点（写作组）：** 专门负责接收中间状态，然后逐字生成回复。

**为什么能解决？**

*   **消除干扰：** “阅读组”不管怎么忙，都不会影响“写作组”的流畅度。

*   **各司其职：**

    *   Prefill 节点可以针对“高算力”进行优化（比如使用张量并行，TP）。
    *   Decoding 节点可以针对“高内存带宽”进行优化（比如使用流水线并行，PP）。这就像厨房里，切菜工（Prefill）只负责切菜，炒菜师傅（Decoding）只负责炒菜，互不打扰，流水线作业。

***

### 4. 方案的核心创新点在哪里？

1.  **PD 分离架构（Disaggregation）：** 明确提出将 Prefill 和 Decoding 拆分到不同的物理 GPU 或实例上。

2.  **高效的 KV Cache 传输：**

    *   *背景：* 既然分家了，阅读组读完的信息（KV Cache，即键值缓存，这是模型理解上下文的记忆）必须传给写作组。如果传输太慢，分家就没意义了。

    *   *创新：* DistServe 优化了网络传输层，使得通过网络传输 KV Cache 的速度，比重新计算一遍要快得多，保证了流水线的顺畅。

3.  **Goodput 优先的资源分配策略：**

    *   *术语解释：* Goodput（有效吞吐量）不同于 Throughput（总吞吐量）。Throughput 是“一小时做多少菜”，Goodput 是“一小时能端出多少**不超时、不退单**的菜”。

    *   *创新：* 系统会自动计算：为了满足不卡顿的目标，到底应该分多少个 GPU 给阅读组，分多少给写作组？它能动态调整这个比例。

***

### 5. 本方案有什么局限性？与之前方案相比的劣势？

虽然 DistServe 效果很好，但也不是没有代价的：

1.  **网络带宽依赖严重：**

    *   这是最大的软肋。因为要频繁在 GPU 之间传输巨大的 KV Cache 数据，它需要极高速度的内部网络（如 NVLink 或 高速 InfiniBand）。如果是普通的低速网络，传输时间过长，效果反而不如不拆分。

2.  **资源门槛变高（起步价高）：**

    *   之前的方案（vLLM），你只有 1 张 GPU 也能跑。

    *   DistServe 既然要分家，**至少需要 2 张 GPU**（一张做 Prefill，一张做 Decoding）。对于资源受限的小用户不太友好。

3.  **低负载下的资源浪费：**

    *   如果没什么人来访问（低负载），专门空着一组 GPU 等着做 Prefill 是很浪费的。而传统的合并模式在低负载下反而能灵活利用闲置资源。

### 总结给学生

这篇论文的核心在于\*\*“解耦”\*\*。它发现了 LLM 推理中两个阶段的资源需求冲突，通过物理拆分和网络传输解决了“互相打架”的问题。

*   **以前（vLLM）：** 一个厨师既切菜又炒菜，切菜时炒锅就得停火，导致上菜慢。

*   **现在（DistServe）：** 切菜工和炒菜师傅分开，切好的菜通过超快传送带（高速网络）送过去，大家都全速工作，保证了上菜既快又稳。

## 思考一

1.  prefill具体做了什么工作？只是把用户的输入的KV全部计算出来吗？
2.  为什么Prefill是高算力，而decoding是高内存带宽？
3.  KV cache的传输优化，具体是怎么做的？
4.  Goodput的自动调整具体是怎么做的？

***

很高兴看到你提出了这么核心的四个问题！这说明你已经不满足于表面的理解，开始思考系统底层的逻辑了。这些问题正是大模型推理（LLM Inference）系统设计中最硬核的部分。

让我们逐一拆解：

***

### 1. Prefill 具体做了什么工作？只是把用户的输入的 KV 全部计算出来吗？

**不全是。Prefill 实际上是“根据输入，预测第一个输出字”的完整过程。**

你可以把它理解为模型的一次\*\*“全力冲刺”\*\*。具体步骤如下：

1.  **输入处理：** 系统收到你的 Prompt（比如 1000 个字），把它并行输入进模型。

2.  **全量计算（主要工作）：** 模型会对这 1000 个字同时进行计算。每一层的神经元都要激活，计算它们之间的注意力关系（Attention）。

    *   *这一步非常重：* 它的计算量是  $O(N^2)$  的（N 是输入长度），输入越长，计算量爆炸式增长。

3.  **产出 KV Cache（副产品）：** 在计算过程中，为了以后不再重复算这 1000 个字，我们把中间计算出的“钥匙和值”（Key-Value）存下来，这就是 KV Cache。

4.  **产出首字（最终目的）：** Prefill 的终点是计算出**概率最大的第一个字**是什么。

**结论：** Prefill 既是在做全量的前向传播（Forward Pass）来生成第一个字，同时顺手把“记忆”（KV Cache）存了下来。

***

### 2. 为什么 Prefill 是高算力（Compute-bound），而 Decoding 是高内存带宽（Memory-bound）？

这是一个非常经典的面试题。核心在于\*\*“数据搬运”和“数学计算”的比例\*\*不同。

我们假设模型的权重（Weight）是厨房里的**食材**，GPU 核心是**厨师**，显存带宽是**冰箱门**。

#### **Prefill 阶段（高算力）：**

*   **场景：** 一次性来了 1000 个客人（输入 token）。

*   **操作：** 厨师从冰箱把食材（模型权重）拿出来一次，然后用这一份食材，给这 1000 个客人做菜（矩阵乘法是 Batch 操作）。

*   **比例：** 搬一次数据，做 1000 次计算。

*   **瓶颈：** 冰箱门（带宽）够用，但厨师（算力）切菜切不过来了。

*   **术语：** 算术强度高（High Arithmetic Intensity）。

#### **Decoding 阶段（高内存带宽）：**

*   **场景：** 每次只生成 1 个新字（1 个 token）。

*   **操作：** 厨师为了处理这 1 个新字，依然需要把整个冰箱的食材（几十 GB 的模型权重）全部搬出来过一遍。

*   **比例：** 搬几十 GB 的数据，只为了做极少量的计算（算 1 个向量）。

*   **瓶颈：** 厨师闲得发慌，大部分时间都在等冰箱门把食材运过来。

*   **术语：** 算术强度低（Low Arithmetic Intensity）。

**总结：** Prefill 是“一次搬运，疯狂计算”；Decoding 是“疯狂搬运，只算一点”。

***

### 3. KV Cache 的传输优化，具体是怎么做的？

因为 Prefill 和 Decoding 分家了，Prefill 算出的几百 MB 甚至 GB 级的 KV Cache 必须传给 Decoding 节点。如果不优化，网速会成为瓶颈。

DistServe 主要采用了以下策略：

1.  **直接内存访问（RDMA / GPU Direct）：**

    *   *传统做法：* GPU A -> CPU A -> 网卡 -> CPU B -> GPU B。这中间经过了 CPU 的内存拷贝，非常慢。

    *   *优化做法：* **GPU A -> 网卡 -> GPU B**。利用 RDMA 技术，不经过 CPU，直接让两个 GPU 隔空“投送”数据。这极大地降低了延迟。

2.  **张量并行下的切分传输：**

    *   如果使用了张量并行（Tensor Parallelism），KV Cache 本身就是切分在多张卡上的。DistServe 能够保持这种切分结构直接对位传输，避免了“先合并再传输再切分”的复杂过程。

3.  **流水线掩盖（Overlapping）：**

    *   虽然论文重点在于架构，但在工程实现上，传输通常与计算是并行的。即在 Decoding 节点准备接收请求时，数据可能已经开始预取或分层传输，尽量减少模型空转等待数据的时间。

**核心结论：** 只要网络带宽足够大（比如数据中心常用的 InfiniBand 或高速以太网），传输 KV Cache 的时间是**远远小于**重新计算一遍 KV Cache 的时间的（大约快 10 倍以上）。

***

### 4. Goodput 的自动调整具体是怎么做的？

这是 DistServe 最“智能”的地方。它不是瞎猜怎么分机器，而是通过一套算法来决定：**我有 100 张卡，到底 30 张做 Prefill，70 张做 Decoding，还是 50:50？**

步骤如下：

1.  **性能画像（Profiling）：**

    *   系统先在单张卡或单组卡上跑测试：输入 100 字、500 字、2000 字时，Prefill 要多久？生成 100 字、500 字时，Decoding 每秒能蹦几个字？
    *   建立一个性能数据库（Cost Model）。

2.  **定义目标（SLO）：**

    *   设定红线：比如“首字延迟必须 < 200ms”且“生成速度必须 > 50 token/s”。只有满足这两个条件的请求才叫 Goodput（有效吞吐）。

3.  **搜索最优配置（Placement Optimization）：**

    *   系统模拟各种切分方案。

    *   *方案 A：* 1 个 Prefill 实例（用 2 卡 TP），4 个 Decoding 实例（各用 2 卡 TP）。

    *   *方案 B：* 2 个 Prefill 实例，3 个 Decoding 实例。

    *   系统利用排队论模型（Queueing Theory）或模拟器，计算在给定的用户流量下，哪种方案能让**满足 SLO 的请求数量（Goodput）最大化**。

4.  **动态调整：**

    *   虽然论文主要强调静态规划，但在实际部署中，如果流量特征变了（比如大家突然都问长文本了，导致 Prefill 压力暴增），这套逻辑可以重新计算，指导运维动态调整集群的角色分配。

**总结：** 就像一个餐厅经理，拿着秒表掐时间，算出切菜工和炒菜工的最佳比例，确保高峰期不积压订单。

## 思考二

为什么decoding节点可以使用流水线并行进行优化？

***

这是一个非常敏锐的问题！

通常我们认为张量并行（TP, Tensor Parallelism）是减少延迟的利器，而流水线并行（PP, Pipeline Parallelism）通常用于训练。但在 DistServe 的 Decoding 节点中引入 PP，主要是为了解决**显存容量（Memory Capacity）**和**跨节点通信**的瓶颈。

我们可以用\*\*“汽车组装流水线”\*\*来理解这个逻辑。

***

### 1. 核心原因：解决 KV Cache 的“显存爆炸”问题

这是最关键的原因。

*   **问题背景：** Decoding 阶段需要一直存储“过去所有对话的记忆”（KV Cache）。随着对话越来越长，或者是并发的用户越来越多（Batch Size 变大），这个 KV Cache 会变得极大，大到单张 GPU 的显存根本装不下。

*   **TP 的局限：** 在张量并行（TP）中，虽然模型参数切分了，但每一层都要处理完整的 Batch，产生的 KV Cache 压力依然很大。

*   **PP 的优势（分摊记忆压力）：**

    *   流水线并行是按\*\*层（Layer）\*\*切分的。

    *   比如 32 层的模型，GPU 1 负责前 16 层，GPU 2 负责后 16 层。

    *   **重点来了：** GPU 1 只需要存储前 16 层的 KV Cache，GPU 2 只需要存储后 16 层的。

    *   这意味着：**显存被完美分摊了。** 你可以使用更大的 Batch Size（服务更多用户）或者支持更长的上下文，而不会爆显存（OOM）。

### 2. 次要原因：减少“跨节点”的通信开销

如果你的模型太大，一台机器（比如 8 卡）装不下，必须用两台机器（16 卡）怎么办？

*   **TP 的劣势（话痨模式）：**

    *   TP 需要在每一层计算完后，所有 GPU 进行一次通信（All-Reduce）。
    *   如果跨机器跑 TP，数据要走以太网或 InfiniBand，相比机器内部的 NVLink，速度慢很多。这会导致严重的卡顿。

*   **PP 的优势（传话模式）：**

    *   PP 只需要在切割点通信。比如 GPU 1 算完前 16 层，把结果发给 机器 B 的 GPU 2，GPU 2 接着算。

    *   这种通信量非常小（通常只是几个向量），而且频率低。

    *   **结论：** 当算力需求超出一个节点时，PP 是比 TP 更经济、效率更高的扩展方式。

### 3. 为什么 Decoding 阶段能容忍 PP 的“延迟”？

你可能会问：*“流水线不是像接力赛吗？GPU 2 没拿到棒之前只能干等，这不会增加延迟吗？”*

*   **是的，PP 会轻微增加单个请求的延迟（Bubble 问题）。**

*   **但是，DistServe 关注的是 Goodput（有效吞吐量）：**

    *   在 Decoding 阶段，系统通常是**高并发**状态。

    *   虽然对于“单个用户”来说，信号在流水线上跑一圈慢了一点点。

    *   但对于“整个系统”来说，流水线可以让 GPU 1 处理用户 A 的第二层时，GPU 2 正在处理用户 B 的第一层（通过 Micro-batching）。

    *   只要显存够大（由 PP 保证），我就能塞进足够多的并发请求，把流水线填满，让 GPU 不空转。

### 总结

Decoding 节点使用流水线并行（PP），不是为了让单个字蹦得更快，而是为了：

1.  **大肚能容：** 把 KV Cache 分散到多张卡上，防止显存撑爆。

2.  **支持高并发：** 显存有了空间，就能同时服务更多用户（大 Batch），提升整体吞吐量。

3.  **跨机扩展：** 在多机环境下，比 TP 通信效率更高。

就像把原本挤在一个工位上的\*\*“全能工匠”**（TP），拆成了通过传送带连接的**“流水线工人”\*\*（PP），虽然单个产品的流转路径变长了，但工厂整体同时能处理的订单量大大增加了。
