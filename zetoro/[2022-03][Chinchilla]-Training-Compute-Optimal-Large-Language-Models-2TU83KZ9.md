---
tags: []
parent: '[Chinchilla] Training Compute-Optimal Large Language Models'
collections:
    - 扩展定律
$version: 2251
$libraryID: 1
$itemKey: 2TU83KZ9

---
\[2022-03]\[Chinchilla] Training Compute-Optimal Large Language Models

# \[Chinchilla] Training Compute-Optimal Large Language Models

## 概述

你好！很高兴你对这篇论文感兴趣。这篇论文通常被称为 **"Chinchilla 论文"**（Chinchilla 是论文中提出的模型名字，是一种南美栗鼠），来自 DeepMind。

它在 LLM（大语言模型）的发展史上是一个**里程碑**，它彻底改变了人们“烧钱”训练模型的方式。

来，搬把椅子坐下，我们把复杂的公式放到一边，用最直白的话把它的核心逻辑理清楚。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**一句话总结：它解决的是“预算分配”问题。**

**通俗解释：** 想象你是一家 AI 公司的老板，手里有一笔固定的巨款（比如 1000 万美元）用来租显卡训练一个大模型。这就代表你的**计算预算**（Compute Budget）是固定的。

要把模型训练好，你面临两个选择，这笔钱该怎么花？

*   **选项 A：** 造一个超级大的模型（参数量 huge），但因为钱不够了，只能让它读少量的书（训练数据少）。

*   **选项 B：** 造一个中等大小的模型，用剩下的钱让它读非常非常多的书（训练数据多）。

**这篇论文要解决的问题就是：** 给定固定的计算预算，模型大小（Model Size）和训练数据量（Training Tokens）之间的**最佳比例**是多少？

**为什么重要？**

*   **省钱省时间：** 训练一次 GPT-3 级别的模型要花几百万美元。如果比例没搞对，就相当于花了大价钱买了一辆法拉利，却只加了一升油，车还没跑起来就停了，极其浪费。

*   **模型性能上限：** 只有找到最佳比例，才能在同样的预算下，训练出最聪明的模型。

***

### 2. 解决这个问题的难点在哪里？之前是怎么做的？

**难点：** 训练大模型太贵了！你不可能把 100 亿参数、1000 亿参数的模型都训练一遍来测试哪个好。只能靠“猜”或者基于小规模实验去“推算”。

**之前的解决方案（Kaplan Scaling Laws）：** 在 Chinchilla 之前，OpenAI 的 Kaplan 等人在 2020 年发表过一篇著名的论文。他们的结论是：**“模型大小最重要”**。

*   **之前的误区：** Kaplan 的定律告诉大家，当算力增加时，你应该要把大部分算力用来增加模型的参数量，而数据量不需要增加那么多。

*   **导致的后果：** 大家都开始疯狂“堆参数”。比如 DeepMind 自己的前代模型 **Gopher**（2800 亿参数）和英伟达/微软的 **MT-NLG**（5300 亿参数）。这些模型变得无比巨大，但训练数据量相对较少（大概只看了 3000 亿个单词）。

**为什么之前的方案不好？** 这些巨型模型虽然参数多，但\*\*“没吃饱”\*\*（Undertrained）。这就好比你雇了一个绝世天才（脑容量极大），但只让他读了一本小学课本，他的能力根本发挥不出来。而且，模型太大导致它在实际使用（推理）时非常慢、非常贵。

***

### 3. 本论文提出了何种解决方案？为什么能解决？

**解决方案：** DeepMind 的研究人员没有盲目信任之前的定律，而是重新做了一次严谨的科学实验。

1.  他们训练了 **400 多个** 不同大小的小型和中型模型。

2.  他们调整每个模型的参数量和数据量，观察在不同算力预算下，谁的 Loss（误差率，越低越好）最低。

3.  通过拟合这些数据，他们推导出了新的数学公式（Scaling Laws）。

**核心结论（Chinchilla Scaling Laws）：** 结论非常反直觉（相对于 Kaplan 的结论）：**模型大小和训练数据量应该“等比例”增加。**

*   也就是说，如果你把算力预算翻倍，你不能光把模型变大，你也得让它多读一倍的书。

*   具体的黄金比例大约是：**每增加 10 倍的计算量，模型参数量扩大 3.2 倍，数据量也扩大 3.2 倍。**（接近 1:1 的增长关系）。

**为什么能解决问题？（验证）：** 为了证明他们是对的，DeepMind 训练了一个新模型叫 **Chinchilla**。

*   **Gopher（旧模型）：** 2800 亿参数，数据量少。

*   **Chinchilla（新模型）：** **700 亿参数**（只有 Gopher 的 1/4 大），但训练数据是 Gopher 的 **4 倍**（1.4 万亿 tokens）。

*   **结果：** 尽管两者的训练总成本（算力）是一样的，但 Chinchilla 在几乎所有任务上都**吊打**了 Gopher，甚至打败了比它大得多的 GPT-3（1750 亿参数）。

***

### 4. 方案的核心创新点在哪里？

1.  **修正了“大即是好”的偏见：** 它纠正了 AI 社区对“参数至上”的盲目追求。它告诉大家，与其追求万亿参数的虚名，不如把数据质量搞好，把数据量加上去。

2.  **给出了具体的“配方”：** 它给出了一个明确的指导公式。以后谁想训练模型，只要输入“我有多少钱（算力）”，公式就会告诉你：“你应该设计一个 X 亿参数的模型，并准备 Y 亿条数据”。这让炼丹（训练模型）变成了科学。

3.  **推理效率的胜利（Hidden Gem）：** 虽然论文主要讲训练，但 Chinchilla 模式训练出来的模型更小（70B vs 175B+）。这意味着，当我们要把模型部署给用户使用时，**运行成本大幅降低，速度大幅提升**。这对商业落地意义非凡。

***

**总结给学生：** 你可以把这篇论文看作是 AI 界的“营养学指南”。以前大家以为想要长得壮（性能好）就得拼命把胃撑大（堆参数），这篇论文告诉大家：**“胃口适中（参数适量）但多吃有营养的食物（海量数据），才能长得最壮且最健康。”**

这就是为什么现在的 Llama 3、Mistral 等模型，参数量都不大（7B, 70B），但都使用了数万亿 tokens 进行训练的原因。它们都是 Chinchilla 定律的受益者。

## 思考一

本论文和 Scaling Laws for Neural Language Models 谁先发表的，结论有什么异同点？

***

这真是个非常敏锐的问题！弄清楚这两篇论文的关系，你对大模型发展史的理解就通透了一大半。

这里涉及两篇论文：

1.  **OpenAI 的论文**（简称 **Kaplan 论文**）：题目是 *Scaling Laws for Neural Language Models*。

2.  **DeepMind 的论文**（简称 **Chinchilla 论文**）：题目是 *Training Compute-Optimal Large Language Models*（就是我们在讲的这一篇）。

我来帮你梳理它们的时间线、关系以及核心冲突。

***

### 1. 谁先发表的？

**OpenAI 的 Kaplan 论文先发表。**

*   **OpenAI (Kaplan et al.)**：发表于 **2020 年 1 月**。

    *   它是“开山鼻祖”。它第一次用严谨的数学告诉大家：模型性能和算力之间存在精确的幂律（Power Law）关系。它直接指导了后来 GPT-3（1750 亿参数）的诞生。

*   **DeepMind (Chinchilla)**：发表于 **2022 年 3 月**。

    *   它是“修正者”和“挑战者”。它在 Kaplan 发表两年后，发现大家的模型越做越大但效率不高，于是重新做实验，推翻了 Kaplan 的核心参数结论。

***

### 2. 结论有什么异同点？（核心干货）

你可以把它们想象成装修房子的两种理论。

*   **共同点**：都认为“钱（算力）越多，装修效果（模型性能）越好”，而且这种变好是可以预测的。

*   **不同点**：对于“买多大的房子（模型参数）”和“买多少家具（训练数据）”的比例，两者的建议截然不同。

#### A. 相同点：方向一致

1.  **Scaling 有效：** 两篇论文都确认，随着算力（Compute）、数据量（Data）和参数量（Parameters）的增加，模型的 Loss（误差）会呈指数级下降。
2.  **算力是瓶颈：** 两者都试图在给定的算力预算下寻找最优解。
3.  **甚至公式形式都一样：** 都是幂律公式 $L(N, D) = ...$（只不过公式里的系数 $a$ 和 $b$ 变了）。

#### B. 不同点：比例大翻转（这是重点！）

| 特性         | **OpenAI (Kaplan 2020)**                | **DeepMind (Chinchilla 2022)**          |
| ---------- | --------------------------------------- | --------------------------------------- |
| **核心信条**   | **“大模型至上”**(Model Size is King)         | **“数据模型均衡”**(Balance is Key)            |
| **算力增长分配** | 如果算力增加 10 倍：大部分投入到**把模型做大**。小部分投入到增加数据。 | 如果算力增加 10 倍：**一半**投入把模型做大。**一半**投入增加数据。 |
| **具体的比例**  | **参数量增加 5.5 倍**数据量增加 1.8 倍              | **参数量增加 3.2 倍**数据量增加 3.2 倍              |
| **导致的后果**  | 催生了参数量巨大但数据不足的“虚胖”模型（如 MT-NLG, Gopher）。 | 催生了参数量适中但极其聪明的“精壮”模型（如 Llama, Mistral）。 |
| **潜台词**    | “数据没那么重要，只要脑子够大，读少量的书也能变聪明。”            | “脑子大没用，必须读够书。之前的模型都饿坏了（Undertrained）。”  |


***

### 3. 为什么 OpenAI (Kaplan) 当时“搞错”了？

学生可能会问：“OpenAI 那么厉害，为什么 2020 年会得出‘错误’的结论？”

这不是因为他们数学不好，而是因为**实验方法的局限性**。给你解释两个主要原因：

1.  **学习率（Learning Rate）的设置问题：** OpenAI 当时在做实验时，没有针对每一组不同大小的数据完美地调整“学习率衰减计划”（Learning Rate Schedule）。简单说，他们没有给小模型+大数据量的组合足够好的“复习计划”，导致小模型看起来表现不好。DeepMind 修正了这一点，发现只要优化得当，小模型多训练一会儿，效果惊人。

2.  **早期数据的误导：** 2020 年的时候，高质量的数据集还没那么多，训练超大规模数据（比如 10 万亿 token）的边际效应还没有被完全观测到。

### 总结

*   **Kaplan (2020)** 是**1.0 版本**的指南：它告诉我们“大力出奇迹”，让我们敢于去造 GPT-3 这样的大模型。

*   **Chinchilla (2022)** 是**2.0 版本**的修正：它告诉我们“光大不行，得均衡”，它开启了现在这个“高性能小模型（High performance smaller models）”的时代。

**现在的业界标准完全倒向了 Chinchilla。** 比如 Meta 的 Llama 3，8B 的参数量用了 15T（15万亿）的数据，这比 Chinchilla 建议的数据量还要大得多，完全就是 Chinchilla 理念的极端践行者。
