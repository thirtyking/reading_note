---
tags: []
parent: '[Self-Rewarding] Self-Rewarding Language Models'
collections:
    - 自进化
$version: 2258
$libraryID: 1
$itemKey: H592247Z

---
\[2024-01]\[Self-Rewarding] Self-Rewarding Language Models

# \[Self-Rewarding] Self-Rewarding Language Models

## 概述

你好！很高兴能为你解读这篇来自 Meta AI 的重磅论文 **《Self-Rewarding Language Models》（自奖励语言模型）**。

如果你把训练 AI 想象成教学生写作文，传统的做法是老师（人类）批改，或者请一个更厉害的学霸（更强的模型）来当助教。而这篇论文的核心思想是：**能不能让学生在练习写作文的同时，也练习给自己打分，从而实现“自学成才”？**

下面我们通过 6 个维度来拆解。

### 1. 本论文要解决什么问题？为什么重要？

**要解决的问题：** 如何摆脱对“人工标注数据”的极度依赖，让模型超越人类提供的知识边界。

**为什么重要：** 目前主流的 AI 训练（如 ChatGPT）采用的是 **RLHF（基于人类反馈的强化学习）**。这就像学生写作业，必须有人类在旁边打分。

*   **成本高、速度慢：** 请专业人士给几万条数据打分非常贵。

*   **天花板效应：** 如果老师水平有限，学生（模型）也很难超越老师。如果我们想要创造出比人类更聪明的 AI，AI 就必须学会“自我进化”。

***

### 2. 难点在哪里？之前的方案为什么不够好？

**难点：** 如果 AI 自己给自己打分，它会不会“作弊”？（比如写了一堆废话，却给自己打满分）。这种“自我膨胀”会导致模型跑偏。

**之前的方案：**

1.  **RLHF：** 依赖人类打分。缺点是太慢，数据量喂不饱模型。

2.  **RLAIF（基于 AI 反馈的强化学习）：** 请一个更强的模型（比如 GPT-4）给弱一点的模型打分。缺点是“套娃”，弱模型永远在模仿强模型，无法实现真正的自我突破。

***

### 3. 本论文提出了何种解决方案？

论文提出了 **Self-Rewarding（自奖励）** 机制。它把模型训练变成了一个**循环迭代**的过程：

*   **第一步：技能整合。** 让模型同时学会“生成答案”和“评价答案（打分）”。

*   **第二步：自我打分。** 让模型针对同一个问题写出好几个不同的回答，然后用自己的评价能力，给这些回答打分（比如 1-5 分）。

*   **第三步：自我进化。** 模型挑选出那些自己认为“高分”和“低分”的回答，组成新的训练集，利用 **DPO（直接偏好优化，一种让模型更倾向于高分答案的训练算法）** 来更新自己。

**为什么能解决问题？** 研究发现，随着模型“生成能力”的提高，它的“评价能力”也会跟着提高。这形成了一个**正向循环**：模型变聪明了 $\rightarrow$ 评价更准了 $\rightarrow$ 训练数据质量更高了 $\rightarrow$ 模型变得更聪明了。

***

### 4. 方案的核心创新点在哪里？

核心创新点在于 **“身份合一”** 与 **“迭代进化”**：

1.  **LLM-as-a-Judge（模型即裁判）：** 不再需要外部的奖励模型（Reward Model），模型本身就是自己的裁判。

2.  **Iterative Training（迭代训练）：** 论文证明了这种自我训练可以进行多轮（M1  $\rightarrow$  M2  $\rightarrow$  M3）。每一轮迭代，模型在写答案和打分这两个维度上都在进步。

3.  **打破天花板：** 实验显示，经过三轮迭代后的 Llama-2-70B，表现甚至超过了当时公认最强的由人类标注训练出来的模型。

***

### 5. 本方案有什么局限性？

1.  **奖励作弊（Reward Hacking）：** 模型可能会发现某种“捷径”（比如回答得特别长或者很有礼貌），即便内容不对也能得高分，从而导致模型学会敷衍。

2.  **自我盲区：** 如果模型从一开始就对某个知识点有错误理解，它给自己打分时也会坚持这个错误，导致“错误被放大”。

3.  **算力消耗：** 每一轮迭代都需要重新生成大量数据并训练，对计算资源要求很高。

***

### 6. 关键流程图解读

虽然我无法直接展示图片，但我为你描述论文中最核心的那张**循环流程图**，它能帮你瞬间理解逻辑：

*   **图的左边：** 是初始模型（模型 M1）。

*   **中间的环形路径：**

    1.  **Generate（生成）：** M1 针对一个问题生成 4-8 个备选答案。

    2.  **Evaluate（评价）：** M1 用特定的“评价指令”给这几个答案打分（分出好坏）。

    3.  **Preference Data（偏好数据）：** 自动筛选出“最好的”和“最差的”形成对比对。

*   **图的右边：** 经过 DPO 训练后，诞生了 M2。

*   **循环：** 接着用 M2 去跑上面的流程，诞生 M3。

**这张图的意义：** 它告诉你，这个过程是**闭环**的。它不需要外部干预，就像一个人在荒岛上通过不断自我练习和反思，最终变成了大师。

***

### 总结给学生的话：

这篇论文告诉我们：**AI 不仅仅能当学生，还能当自己的老师。** 只要我们给它一套合理的自评标准，它就能通过“左右互搏”不断进化。这是通往 AGI（通用人工智能）道路上，解决数据瓶颈的一个非常重要的方向。

## 思考一

自我打分的方式，在论文：Self-Refine: Iterative Refinement with Self-Feedback 中也提到了，两篇论文中的方法有什么本质的不同？

***

这是一个非常深刻的问题！你敏锐地察觉到了这两篇论文在“自我评价”这一点上的相似性。

虽然它们都让模型“既当运动员又当裁判员”，但它们的**目的、执行阶段和对模型的影响**有着本质的区别。我们可以用一个形象的比喻来拆解：

*   **Self-Refine** 像是：**写完作文后的“检查与修改”**。

*   **Self-Rewarding** 像是：**为了考高分而进行的“刷题与总结”**。

具体区别如下：

### 1. 作用阶段不同（最本质的区别）

*   **Self-Refine (推理时优化)：**

    *   它发生在**推理阶段**（Inference Time）。

    *   当你问它一个问题，它先写个初稿，然后自己看一遍找缺点，再根据缺点改第二稿。

    *   **核心逻辑：** 它是通过**提示词（Prompting）**实现的。模型本身的参数（它的“大脑”）在这个过程中**没有任何变化**。

*   **Self-Rewarding (训练时优化)：**

    *   它发生在**训练阶段**（Training Time）。

    *   它通过自我评价生成数据，然后利用这些数据去**更新模型的参数**（权重）。

    *   **核心逻辑：** 它是通过\*\*强化学习/微调（DPO）\*\*实现的。训练完之后，模型变成了一个更新、更强的版本（从 M1 变成 M2）。

### 2. 目的不同

*   **Self-Refine：** 为了让**当前这一个回答**变得更好。

    *   就像你写简历，改了三遍，最后这篇简历很完美，但你的写作水平并没有因为这次改动而发生质的飞跃，下次写新东西可能还得改。

*   **Self-Rewarding：** 为了让**模型整体能力**变得更强。

    *   它目标是提升模型的基础素质。训练结束后，模型下次写初稿时，水平就已经比之前的初稿高出一大截了，它不需要反复修改就能输出高质量答案。

### 3. 反馈的形式不同

*   **Self-Refine：** 给出的是**文本反馈**（Critique）。

    *   比如：“你的代码漏掉了一个边界处理，请加上。”模型根据这段文字提示去微调输出。

*   **Self-Rewarding：** 给出的是**分数/偏好反馈**（Reward/Preference）。

    *   比如：“答案 A 比答案 B 更好，打 5 分。”模型根据这个分数差，利用数学算法（DPO）调整自己的神经元连接，使得以后更有可能生成像 A 这样的答案。

### 4. 进化方式不同

*   **Self-Refine** 的天花板受限于模型当前的**上下文理解能力**。如果模型本身逻辑很差，它可能看不出自己初稿的错误。
*   **Self-Rewarding** 追求的是**迭代进化**。论文通过实验证明，随着模型在每一轮（Iterative Training）中不断给自己打分和学习，它不仅生成能力变强了，**评价能力（打分准确度）也跟着一起变强了**。这是一种真正的“自我螺旋式上升”。

***

### 总结对比表

| 特性         | Self-Refine (2023) | Self-Rewarding (2024) |
| :--------- | :----------------- | :-------------------- |
| **比喻**     | 考场上的检查与涂改          | 闭关修炼，刷题总结             |
| **大脑是否改变** | 否（参数冻结，只靠 Prompt）  | **是（模型权重被重新训练）**      |
| **什么时候用**  | 你提问的时候（实时修改）       | 模型发布前的训练阶段            |
| **反馈内容**   | 具体建议（文字）           | 好坏程度（分数/偏好）           |
| **解决的问题**  | 提升单次任务的输出质量        | **解决人工标注数据稀缺的问题**     |


**一句话总结：** **Self-Refine** 是教模型如何**把活儿干得更细**；而 **Self-Rewarding** 是教模型如何**通过自我反思，把自己变成一个更优秀的打工人。**
