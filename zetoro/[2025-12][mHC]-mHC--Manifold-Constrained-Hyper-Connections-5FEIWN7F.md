---
tags: []
parent: '[mHC] mHC: Manifold-Constrained Hyper-Connections'
collections:
    - 核心组件
$version: 2273
$libraryID: 1
$itemKey: 5FEIWN7F

---
\[2025-12]\[mHC] mHC: Manifold-Constrained Hyper-Connections

# \[mHC] mHC: Manifold-Constrained Hyper-Connections

## 概述

你好！很高兴能为你解读 DeepSeek 最新发布的这篇关于架构创新的论文：**《mHC: Manifold-Constrained Hyper-Connections》**。

作为研究员，我会尝试绕开复杂的数学公式，用更直观的逻辑带你理解这项研究。

***

### 1. 本论文要解决什么问题？为什么重要？

**核心问题：如何让超大规模神经网络（LLM）在变得更“宽”、更“复杂”的同时，还能保持训练的稳定性？**

*   **背景：** 现在的深度学习（如 Transformer）之所以能叠几百层，全靠**残差连接（Residual Connection）**。简单来说，它就像一条高速公路，让信息可以直接从输入跳到输出，避免了层数太深导致信号“断掉”（梯度消失）或“炸掉”（梯度爆炸）。
*   **新尝试（HC）：** 最近有人提出了 **Hyper-Connections (HC)**，它不满足于只有一条路，而是把残差路径变“宽”了（多条平行流），并允许这些流之间进行复杂的相互作用。
*   **痛点：** 这种 HC 虽然性能潜力大，但因为它是“野蛮生长”的，没有约束。在训练大模型时，信号会随着层数增加而无限放大或缩小，导致训练崩溃（不稳定），且占用的内存和计算资源极高。

**为什么重要？** 随着模型规模向数千亿甚至万亿参数演进，传统的残差设计可能已经到达极限。HC 这种“拓扑结构”的创新是下一代基础模型的关键方向，但必须先解决它“容易崩”的问题。

***

### 2. 解决这个问题的难点在哪里？之前的方案为什么不行？

<a href="./[2024-09][HC]-Hyper-Connections-BIRLCCH3.md" rel="noopener noreferrer nofollow" zhref="zotero://note/u/BIRLCCH3/" ztype="znotelink" class="internal-link">[2024-09][HC] Hyper-Connections</a>

*   **难点 1：身份映射（Identity Mapping）的丧失。** 残差连接之所以稳，是因为它默认做了“加法”（ $x + f(x)$ ），这保证了信号的均值和方差相对稳定。而 HC 引入了可学习的矩阵。如果矩阵没学好，信号每经过一层就被放大一点，几十层下来就是天文数字。

*   **难点 2：硬件效率的“内存墙”。** HC 增加了多条残差流，这意味着数据搬运量剧增。在现代 GPU 上，计算不累，搬运数据（内存读写）才是最慢的。

*   **之前方案的不足：**

    *   **标准残差：** 稳定但结构太死板，限制了模型的表达能力。
    *   **原始 HC：** 虽然灵活，但在 27B（270 亿参数）这种规模上训练时，信号增益会飙升到 3000 倍以上（见论文图 3b），直接导致训练失败。

***

### 3. 本论文提出了何种解决方案？方案为什么有效？

论文提出了 **mHC（Manifold-Constrained Hyper-Connections）**，即“流形约束超连接”。

**核心思路：给这些自由的连接矩阵戴上“镣铐”，让它们在一个特定的数学范围内（流形）跳舞。**

*   **具体手段：** 强制要求连接矩阵必须是**双随机矩阵（Doubly Stochastic Matrix）**。

    *   什么是双随机矩阵？就是每一行加起来等于 1，每一列加起来也等于 1。

*   **为什么有效？**

    1.  **信号守恒：** 因为行列和都为 1，这种矩阵本质上是在做“加权平均”。它既允许不同路径的信息相互交换（混合），又保证了总的信号强度既不会爆炸也不会消失。
    2.  **数学优美：** 这种矩阵的相乘结果依然是双随机矩阵。这意味着无论模型叠多少层，信号的稳定性都是由数学定理保证的。
    3.  \*\* infraestructura 优化：\*\* 针对“搬运慢”的问题，团队开发了专门的 CUDA 内核（算子融合、选择性重算），把原本可能大幅拖慢速度的设计，优化到了只增加 6.7% 的额外开销。

***

### 4. 方案的核心创新点在哪里？

1.  **引入 Sinkhorn-Knopp 算法：** 这是一个经典算法，论文巧妙地用它在模型训练过程中实时地将一个普通的矩阵“投影”成双随机矩阵。这是实现稳定性的核心“黑科技”。
2.  **将拓扑结构与数学约束结合：** 之前的研究要么只关注怎么连线（拓扑），要么只关注怎么归一化（约束）。mHC 证明了：想要复杂的连接，必须有严苛的数学约束。
3.  **极致的工程落地：** 不仅仅是写个公式，DeepSeek 团队还配合 DualPipe（双向流水线并行）等技术，解决了 HC 带来的高通信延迟问题，让它在大规模集群上真正跑得起来。

***

### 5. 本方案有什么局限性？

*   **计算开销：** 尽管优化到了 6.7%，但对于追求极致效率的大模型训练来说，这依然是一笔额外的开支。
*   **实现复杂：** 相比于原本简单的 `x + f(x)`，mHC 需要复杂的算子支持和内存管理逻辑。
*   **约束可能太强：** 强制要求双随机矩阵虽然保证了稳定，但也许在某种程度上限制了模型学习某些极端特征的能力（这属于稳定性和灵活性之间的博弈）。

***

### 6. 关键架构图解读（图 1）

请看论文第一页的 **Figure 1**，它清晰地对比了三种进化阶段：

*   **图 (a) Residual Connection：** 只有一条直路。最简单、最稳，但也最窄。
*   **图 (b) Hyper-Connections (HC)：** 变成了多条并行的路，路与路之间有三个可学习的矩阵（Res, Pre, Post Mapping）。虽然变宽了，但这些矩阵是“乱跑”的，容易导致信号失控。
*   **图 (c) Manifold-Constrained HC (mHC)：** 结构和 HC 一样，但矩阵名字前面多了一个 **$\mathcal{M}$**。这代表它们被投影到了“双随机流形”上。你可以把它理解为在所有的路口装上了自适应的红绿灯和分流器，确保车流（信息流）无论怎么交换，总量始终受控。

**总结给学生：** 这篇论文告诉我们，想让 AI 模型结构更复杂（更宽、连接更多）来提升智力，不能直接硬加，必须通过严谨的数学工具（双随机约束）和高效的工程手段（算子融合）来保驾护航。mHC 为未来构建更深、更强大的基础模型提供了一套更稳固的“脚手架”。

## 思考一

1.  为什么HC的多条残差流会导致数据搬运量剧增？mHC又是如何优化的？
2.  解释一下Sinkhorn-Knopp 算法，以及如何将普通的矩阵“投影”成双随机矩阵？

***

这两个问题问得非常深入，触及了这篇论文在**工程实现**和**数学机制**上的核心。作为研究员，我为你详细拆解：

***

### 1. 为什么 HC 会导致数据搬运量剧增？mHC 如何优化？

#### **原因：从“单车道”变“多车道”，且增加了大量“关卡”**

在标准的残差连接（ResNet）中，数据流就像一条单车道公路。每一层只需要做一次加法：$x_{new} = x_{old} + f(x_{old})$。 但在 **HC（Hyper-Connections）** 中，情况发生了剧变：

1.  **宽度膨胀（$n$倍流量）：** HC 把残差流的宽度扩大了  $n$  倍（论文中  $n=4$ ）。原本只需要搬运  $C$  个特征数据，现在要搬运  $n \times C$  个。

2.  **复杂的“路口”交互：** 论文中的 **Table 2** 揭示了真相。为了管理这 $n$ 条流，HC 引入了三个映射矩阵：

    *   **Pre Mapping ($H^{pre}$):** 把  $n$  条流的数据聚合给计算层（读一遍全量数据）。

    *   **Post Mapping ($H^{post}$):** 把计算层的输出写回  $n$  条流（又写一遍全量数据）。

    *   **Res Mapping ($H^{res}$):** 让这  $n$  条流自己之间进行混合（又读又写一遍全量数据）。

3.  **内存墙（Memory Wall）：** 在 GPU 里，读写显存（I/O）的速度远慢于核心计算速度。HC 的总读写量大约是标准残差的 **$n$倍**（对于  $n=4$ ，就是 4-5 倍的吞吐压力）。如果没有优化，GPU 大部分时间都在“搬东西”而不是“算东西”。

#### **mHC 的优化手段：**

*   **算子融合（Kernel Fusion）：** 这是最关键的。mHC 开发了定制的 CUDA 内核，把原本需要分好几步走的读写操作“合并”了。例如，它在一个内核里同时完成数据的读取、归一化、矩阵乘法和最后的写回。**这样数据只需要从显存搬到 GPU 缓存里一次，就能干完好几件活**，极大地减少了往返搬运次数。
*   **选择性重算（Selective Recomputing）：** HC 产生了很多中间变量，如果都存起来备用，显存会炸。mHC 的策略是：**不存，用的时候现算**。因为 mHC 的映射算子计算量其实不大，与其花时间去显存里“搬”旧数据，不如让 GPU 核心直接现场重新算一遍，反而更快。
*   **通信重叠（DualPipe Overlapping）：** 在多卡训练时，mHC 调整了流水线执行顺序（见图 4），让昂贵的“多流数据通信”和“模型计算”同步进行，把搬运时间藏在计算时间里。

***

### 2. 解释 Sinkhorn-Knopp 算法：如何将矩阵“投影”？

这是论文最精彩的数学部分。

#### **什么是“双随机矩阵”？**

想象一个方阵，如果每一行加起来等于 1，每一列加起来也等于 1，且所有数都大于 0，它就是**双随机矩阵**。它在数学上的好处是：信号经过它处理后，总量（均值）保持不变，既不发散也不消失。

#### **Sinkhorn-Knopp 算法的逻辑：**

这个算法就像是一个\*\*“循环平衡大师”\*\*。假设你现在有一个普通的正数矩阵 $A$（mHC 通过 `exp(H)` 得到），想把它变形成双随机矩阵：

1.  **第一步（行归一化）：** 把矩阵每一行求和。然后用每一行里的数除以该行的和。

    *   *结果：* 现在每一行加起来都等于 1 了。但列的和乱了。

2.  **第二步（列归一化）：** 把上一步得到的矩阵每一列求和。然后用每一列里的数除以该列的和。

    *   *结果：* 现在每一列加起来都等于 1 了。但刚才调好的行之和又变了（虽然变动变小了）。

3.  **不断循环：** 重复“行归一化  $\to$  列归一化  $\to$  行归一化...”。

**神奇之处：** 数学证明，只要循环次数足够，这个矩阵会迅速收敛到一个完美的双随机矩阵。论文中选择了 **20 次迭代**（$t_{max}=20$），这在工程上既快又足够精确。

#### **为什么要叫“投影”到“流形”？**

*   **流形（Manifold）：** 你可以把它想象成高维空间里的一个特定“曲面”。在这个曲面上的点（矩阵），全部都符合“行列和为 1”的规则。

*   **投影（Projection）：** 模型的学习过程是自由的，它最初算出来的矩阵  $H^{res}$  可能是一个乱七八糟的点（不在曲面上）。

*   **Sinkhorn-Knopp 的作用**就是伸出一只手，把这个乱跑的点“强行拽回”到那个稳定的曲面上（即双随机矩阵的集合，数学上叫 Birkhoff 多胞形）。

**总结：** 通过 Sinkhorn-Knopp 算法，mHC 就像给模型安装了一个\*\*“自动稳压器”\*\*。无论神经网络想怎么折腾连接矩阵，最终输出的权重永远是平稳的加权平均。这正是 mHC 能在 27B 大规模训练中不崩盘的终极秘密。

## 思考二

残差流扩大N倍，只是输入数据扩大了，模型参数和优化器参数并没有变？我的理解对吗？如果我的理解正确，好像是说搬运的瓶颈主要是在模型参数上，那输入数据增加了为什么会影响搬运？

***

你的直觉很敏锐，但这里存在一个常见的理解误区：**在大模型训练中，真正的“搬运”瓶颈并不只有模型参数，还有“中间结果”（激活值，Activations）。**

针对你的疑问，我们分三个层面来深度拆解：

### 1. 你的理解对吗？（参数量 vs. 数据量）

**你的理解对了一半，但漏掉了一个关键增量。**

*   **参数量（Parameters）：** 你是对的。主要的层函数  $\mathcal{F}$ （比如 Attention 和 MLP）的参数  $W_l$  **完全没有变**。新增加的三个映射矩阵（ $\mathcal{H}^{pre}, \mathcal{H}^{post}, \mathcal{H}^{res}$ ）虽然是新增参数，但它们非常小（维度是  $n \times n$  或  $1 \times n$ ），相比于动辄几千维的特征向量，这些新增参数量微乎其微。

*   **优化器状态（Optimizer States）：** 既然参数量几乎没变，优化器状态占用的显存确实也没有明显增加。

*   **关键变量——激活值（Activations）：** **这就是你漏掉的部分。** 残差流扩大  $n$  倍，意味着每一层产生的中间结果  $x_l$  变成了原来的  $n$  倍。在训练时，为了反向传播计算梯度，你必须把每一层的  $x_l$  都存进显存里。

### 2. 为什么“输入数据（激活值）”增加了会严重影响搬运？

你可能会想：“模型参数几十 GB 都要反复读写，这点输入数据算什么？” 但事实是，在大模型运算中，**激活值的搬运压力往往比参数还要大**，原因如下：

#### **A. 乘数效应：序列长度 \times Batch Size**

模型参数在一次前向传播中只读一次，但激活值的大小是：

> **$n$（流数） $\times$ 序列长度 $\times$ Batch Size $\times$ 隐藏层维度**

在长文本训练时，激活值的数据量会极其巨大，远超模型参数的大小。

#### **B. 计算强度（Arithmetic Intensity）的问题**

这是理解 GPU 瓶颈的核心点。GPU 里的操作分为两类：

1.  **计算密集型（Compute-Bound）：** 比如大矩阵乘法（像  $\mathcal{F}$  里的运算）。GPU 算得满，搬运时间被计算掩盖了。

2.  **访存密集型（Memory-Bound）：** 比如**残差相加、层归一化（LayerNorm）**。这些操作计算非常简单，但必须先把大量数据从显存搬进核心，算完再搬回去。

**HC 增加的操作（Mapping 和 Merge）全都是“访存密集型”。** 原本残差只需要搬运 $1 \times C$ 的数据做个加法；现在 HC 需要搬运 $n \times C$ 的数据，还要经过复杂的映射再写回去。这就好比原本一辆车就能拉完的货，现在要四辆车，而路（显存带宽）就那么宽，所以卡住了。

### 3. mHC 是如何优化的？（针对你的理解偏差）

既然你认为瓶颈主要在模型参数，那我们看看 mHC 的工程优化思路：

1.  **针对激活值搬运（解决 Memory-Bound）：**

    *   **算子融合（Kernel Fusion）：** 正因为输入数据  $n \times C$  变大了，读写次数变多了，mHC 强制让 GPU “一次读取，干完所有的活”。

    *   如果不优化，流程是：读  $nC$  数据  $\to$  算 Pre Mapping  $\to$  写回  $\to$  读数据  $\to$  算 Res Mapping  $\to$  写回…… 这样反复搬运。

    *   **优化后：** 读一次  $nC$  数据，在 GPU 缓存里把这三个 Mapping 全算完，直接输出结果。**把读写次数从多次降为一次。**

2.  **针对反向传播的显存压力：**

    *   由于激活值扩大了  $n$  倍，如果你要把每一层 4 倍的数据都存着等反向传播，显存直接就爆了（OOM）。

    *   **mHC 的方案是“重算”：** 前向传播时不存这  $n$  倍的数据。反向传播需要用到时，再现场临时算一遍。

    *   **为什么划算？** 因为 mHC 的映射计算量很小（ $n \times n$  的小矩阵），计算时间远小于从显存里搬运 4 倍数据的等待时间。

### 总结

*   **模型参数**确实没怎么变，它是**静态瓶颈**。
*   **激活值（输入数据）扩大 $n$ 倍后，成为了动态搬运瓶颈**。
*   mHC 的优化核心不在于减少参数，而在于**通过极致的算子合并和“以计算换访存”的策略，把这$n$倍膨胀的数据搬运压力给化解掉。**

所以，你的理解中“参数是瓶颈”在分布式通信时是对的，但在**单层内部运算**时，HC 引入的激活值访存压力才是真正的性能杀手。
