---
tags: []
parent: '[PagedAttention/vLLM] Efficient Memory Management for Large Language Model Serving with PagedAttention'
collections:
    - 服务架构
    - 显存/缓存管理
$version: 2235
$libraryID: 1
$itemKey: T4FAGSCN

---
\[2023-09]\[PagedAttention/vLLM] Efficient Memory Management for Large Language Model Serving with PagedAttention

# \[PagedAttention/vLLM] Efficient Memory Management for Large Language Model Serving with PagedAttention

## 概述

你好！很高兴你能来问这篇论文。vLLM（及核心技术 PagedAttention）是目前大模型部署（Serving）领域非常“红”的一项技术。

对于很多搞算法的同学来说，平时主要关注模型怎么训练得更聪明，但到了“怎么让模型跑得更快、更省钱”这个工程问题上，关注度往往不够。而这篇论文，就是专门解决**大模型推理时的显存管理**问题的。

来，我们把它拆解开，像剥洋葱一样看看它的核心逻辑。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**一句话总结：** 解决大模型在推理过程中，因为显存（GPU Memory）浪费严重，导致无法同时服务更多用户的问题。

**详细解释：** 大模型（LLM）推理时，最大的瓶颈往往不是计算速度，而是**显存**。 LLM 推理是“一个字一个字”往外蹦的。为了不让模型每生成一个新字都把前面所有字重新算一遍，我们需要把前面计算过的中间状态存下来，这个东西叫 **KV Cache**（键值缓存）。

*   **问题在于：** KV Cache 随着生成的字数增加，体积会越来越大，而且它是动态增长的（你不知道模型会回答 10 个字还是 1000 个字）。

*   **重要性：** 显存是昂贵的资源（比如 A100 显卡）。如果 KV Cache 管理不好，显存很快就满了，你就只能同时服务很少的用户（低 Batch Size）。**显存利用率越高，能同时处理的请求就越多（吞吐量 Throughput 越高），部署成本就越低。**

### 2. 解决这个问题的难点在哪里？之前的方案有什么问题？

**难点：** **“不可预知性”导致的“碎片化”。**

在 vLLM 出现之前，大部分推理框架（比如 HuggingFace Transformers）管理 KV Cache 的方式比较“原始”，类似于你去餐厅吃饭：

*   **之前的做法（预分配）：** 系统不知道你会回答多长，为了保险，它会假设你回答达到最大长度（比如 2048 个 token），并在显存里**预先划出一大块连续的**空间给你。

*   **导致的问题（显存碎片）：**

    1.  **内部碎片（Internal Fragmentation）：** 你预定了 2048 个座位的长桌子，结果只坐了 100 个人（只生成了短回复），剩下的 1900 个座位就浪费了，别人也不能坐。

    2.  **外部碎片（External Fragmentation）：** 即使按需分配，因为要求**显存必须连续**，显存里可能这儿空一点、那儿空一点，虽然总空闲很大，但找不到一块足够大的**连续区域**来塞进一个新的长请求。

**结果：** 论文指出，在 vLLM 之前，现有的系统显存浪费极其严重，**甚至有 60% - 80% 的显存是“占着茅坑不拉屎”的碎片**，根本没利用起来。

### 3. 本论文提出了何种解决方案？为什么能解决？

**核心思路：** **不要让 KV Cache 住在“连续的大豪宅”里，而是把它切碎，住在“分散的小公寓”里。**

作者借鉴了**操作系统（OS）管理内存**的经典思想：**虚拟内存和分页（Paging）。**

**具体方案（PagedAttention）：**

1.  **切块（Blocks）：** 把 KV Cache 切成很多固定大小的小块（Block），比如每块存 16 个 token 的数据。

2.  **非连续存储：** 这些小块在物理显存上可以是**打乱存放**的，不需要连续。哪里有空位就塞在哪里。

3.  **映射表（Block Table）：** 既然存乱了，怎么找回来呢？系统维护一张“表格”，记录逻辑上的顺序对应的物理地址在哪里（类似于 OS 的页表）。

**为什么能解决？**

*   **按需分配：** 模型每生成一小段（比如 16 个字），系统才分配一块新的显存。用多少拿多少，不再需要预先占一大坑。

*   **消灭碎片：** 因为不需要连续空间，任何零散的显存空位都可以被利用起来。这基本消灭了外部碎片，内部碎片也只存在于最后一个 Block 的末尾（浪费极小）。

### 4. 方案的核心创新点在哪里？

这篇论文最妙的地方在于**跨界借鉴**。

1.  **PagedAttention 算子：** 这是最硬核的创新。标准的 Attention 算法要求 K 和 V 矩阵是连续的。作者重写了 CUDA 内核（Kernel），让 Attention 计算时能够接受**不连续**的内存块。它一边查表（找数据在哪），一边计算。这就打通了从“物理不连续”到“逻辑连续计算”的桥梁。

2.  **高效的内存共享（Memory Sharing）：** 这也是操作系统思想的延伸——**Copy-on-Write（写时复制）**。

    *   *场景：* 比如你让模型生成 3 种不同的翻译结果（Parallel Sampling），或者使用 Beam Search。

    *   *传统做法：* 把输入的 Prompt 复制 3 份，占用 3 倍显存。

    *   *vLLM 做法：* 3 个请求共用同一份 Prompt 的 Block。只有当它们生成的后续内容不同时，才分配新的 Block 存不一样的数据。这极大地节省了显存。

### 5. 本方案有什么局限性？与之前的方案相比哪些地方存在劣势？

虽然 vLLM 几乎成了现在的行业标准，但作为研究员，我们要客观看到它的代价：

1.  **实现复杂度极高：** 写一个标准的 Attention 只需要几行 PyTorch 代码。但实现 PagedAttention 需要手写复杂的 CUDA 代码来管理内存映射和计算。这增加了开发和维护的难度（虽然现在 vLLM 封装好了，但在当时是个很重的工程）。

2.  **微小的计算开销（Kernel Overhead）：**

    *   **劣势：** 每次计算 Attention 都要去查表（Block Table）找物理地址，这比直接读连续内存多了一道工序。

    *   *实际影响：* 在 Batch Size 很小（比如只服务 1 个用户）时，vLLM 可能会比那种极致优化的连续内存方案稍微慢一点点（Latency 略增）。但 vLLM 的目标是吞吐量（Throughput），所以在多用户并发场景下，这个微小的单次延迟相比于带来的巨大并发优势，是可以忽略不计的。

3.  **对特定算子的依赖：** 如果你发明了一种全新的 Attention 结构（比如不再是标准的 QK 乘积），你就得重新去写一套 PagedAttention 的 CUDA 内核，灵活性不如原生 PyTorch 操作好。

***

### 总结给学生听：

同学，你就记住这个比喻：

以前的 LLM 推理像是在**排队买票进场**，而且每个人都必须霸占一整排座位，不管你是胖是瘦，导致剧院空荡荡但进不去人。

**vLLM (PagedAttention)** 就像是引入了**灵活的“散座”管理系统**。它允许观众随便坐，哪里有空位坐哪里，甚至允许双胞胎（相同的 Prompt）先坐大腿（内存共享），直到他们要分开行动。

**结果就是：** 同样的显卡，以前只能同时服务 10 个人，现在能服务 20 甚至 30 个人。这就是为什么它在工业界如此重要的原因。
