---
tags: []
parent: '[GraphRAG] From Local to Global: A Graph RAG Approach to Query-Focused Summarization'
collections:
    - RAG
$version: 2248
$libraryID: 1
$itemKey: MIPE8G3T

---
\[2024-04]\[GraphRAG] From Local to Global: A Graph RAG Approach to Query-Focused Summarization

# \[GraphRAG] From Local to Global: A Graph RAG Approach to Query-Focused Summarization

## 概述

你好！很高兴看到你在关注 RAG（Retrieval-Augmented Generation，检索增强生成）领域的前沿进展。这篇由微软研究院发表的《From Local to Global: A Graph RAG Approach to Query-Focused Summarization》非常有影响力，它正好切中了当前 RAG 技术的一个痛点。

来，搬把椅子坐下，我们把那些复杂的公式先放一边，我用最直白的话给你拆解一下这篇论文。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**一句话总结：它要解决“如何在海量文本中回答宏观、全局性的问题”。**

**场景举例：** 想象你是一个公司的 CEO，你的硬盘里有过去 10 年所有的项目文档、邮件和会议记录（假设有 100 万字）。

*   如果你问：“2021 年 5 月 3 日发布的产品的价格是多少？” —— 这是**局部细节问题**。

*   如果你问：“过去 10 年，我们公司在产品策略上的主要变化趋势是什么？” —— 这就是**全局性问题**（Global Query）。

**为什么重要？** 现在的 AI（尤其是 RAG 系统）非常擅长回答第一个问题（找细节），但非常不擅长回答第二个问题（找规律）。但在实际应用中，像是情报分析、市场调研、法律案件梳理，用户往往更关心这种宏观的“全局理解”。如果 AI 只能像复读机一样背诵细节，而不能像分析师一样总结全貌，它的价值就大打折扣。

***

### 2. 解决这个问题的难点在哪里？之前的方案为什么不行？

**核心难点：** **“点”与“面”的矛盾。**

**之前的解决方案（Baseline RAG）：** 目前最主流的 RAG 是基于**向量检索**（Vector Search）。 它的做法是：把那 100 万字切成几千个小片段（Chunks），存进数据库。当你提问时，系统会计算你的问题和哪个小片段“长得像”（语义相似度高），然后把最像的几个片段扔给 AI 去回答。

**为什么不行？** 对于全局问题，这种“切块+检索”的方法有致命缺陷：

1.  **盲人摸象**：全局问题的答案往往分散在整个数据集中，而不是集中在某一段话里。比如“策略变化趋势”，可能前 100 个文档里有一点线索，后 500 个文档里有一点线索。向量检索只能找到最相似的几个点，拼凑不出全貌。

2.  **关联性丢失**：传统的 RAG 并不理解文档 A 和文档 B 之间隐秘的联系（比如 A 文档提到了“张三”，B 文档说“那个经理”，其实是同一个人）。

**结论**：之前的 RAG 擅长在干草堆里找一根针，但如果你问“这堆干草大致是什么形状？”，它就傻眼了。

***

### 3. 本论文提出了何种解决方案？为什么能解决？

**解决方案：GraphRAG（基于图谱的 RAG）。**

作者并没有直接把文本切块就完事了，而是分了两个阶段：

**第一阶段：构建“知识地图”（索引阶段）**

1.  **提取实体与关系**：让大模型（LLM）通读所有文档，把里面的“实体”（如人名、地名、概念）和“关系”（如 A 属于 B，C 导致 D）提取出来。

2.  **构建知识图谱**：用这些提取出来的信息构建一个巨大的网（Knowledge Graph），把原本毫无关联的文本片段通过实体连接起来。

3.  **社区发现（Community Detection）**：这是最关键的一步。算法会自动识别图谱中联系紧密的“小圈子”（Communities）。比如，所有关于“2021 年财务危机”的实体和关系会被聚类成一个社区。

4.  **生成社区摘要**：让 LLM 为每一个“小圈子”写一份总结报告。

**第二阶段：回答问题（查询阶段）** 当你问“主要变化趋势是什么？”时： 系统不再去原始文本里乱翻，而是直接去读那些**预先写好的“社区摘要”**。 系统会把这些摘要汇总，生成一个局部的回答，最后再把这些局部回答整合成一个最终的全局答案（类似于 Map-Reduce 的过程）。

**为什么能解决？** 因为它不是从“字面相似度”出发，而是从**结构**出发。它预先通过“社区摘要”把分散的信息聚拢了。回答问题时，它看的是已经高度浓缩的“情报简报”，而不是原始的“录音笔录”。

***

### 4. 方案的核心创新点在哪里？

这篇论文最漂亮的地方在于它重新定义了 RAG 的检索单元：

1.  **从“文本块”到“图社区”**：传统的 RAG 检索的是具体的文本片段（Chunk），而 GraphRAG 检索的是基于图算法聚类出的“社区摘要”。这大大提升了信息的密度和宏观度。

2.  **分层摘要（Hierarchical Summarization）**：它构建的图谱是有层级的（从底层的具体实体，到高层的宏观概念）。这让 AI 既能回答细节，也能通过高层摘要回答宏观问题。

3.  **全量信息的结构化压缩**：它本质上是用 LLM 把海量的非结构化文本，压缩成了一个结构化的、带有摘要的知识库。

***

### 5. 本方案有什么局限性？与之前的方案相比有哪些劣势？

虽然 GraphRAG 在全局问题上完胜，但它绝对不是完美的，甚至代价很高：

1.  **“贵”且“慢”（构建成本高）**：

    *   构建知识图谱需要让 LLM 读遍每一句话来提取实体，这会消耗大量的 Token（钱）和计算时间。相比之下，传统的切片+向量化非常便宜且快速。

2.  **不支持实时更新**：

    *   一旦有新文档进来，你可能需要重新计算社区聚类和摘要，维护成本高。而传统 RAG 只需要把新文档插入数据库即可。

3.  **细节丢失风险**：

    *   在生成“社区摘要”的过程中，虽然保留了宏观信息，但极度细微的数字或非核心事实可能会被丢弃。如果你问一个特别偏门的细节，传统 RAG 可能反而找得更准。

4.  **幻觉传递**：

    *   如果在构建图谱时，LLM 提取了错误的关系（比如把两个人搞混了），这个错误会被固化在图谱和摘要里，后面怎么问都是错的。

### 总结给学生的话：

这篇论文告诉你，**RAG 不仅仅是“向量检索”**。当面对需要综合理解、跨文档推理的复杂任务时，引入**结构化知识（知识图谱）**并进行**预处理（生成摘要）**，是让 AI 从“搜索工”进化为“分析师”的关键。

你可以把 GraphRAG 理解为：先让 AI 把书读薄（建立索引和摘要），考试的时候自然就能回答出全书的大意了。

## 思考一

1.  社区发现是如何做的？
2.  本方案不支持实时更新的局限性，是否可以通过已有的图的增量更新技术来解决？

***

这是一个非常敏锐的思考！你问到了 GraphRAG 的核心机制和它在工程落地时最头疼的问题。

作为资深研究员，我很欣赏你这种“不仅想知道它是怎么做的，还想知道能不能改进它”的思维方式。我们逐一拆解。

***

### 1. 社区发现（Community Detection）是如何做的？

在 GraphRAG 中，社区发现不仅是为了把节点聚在一起，更是为了**建立一种层级化的索引结构**。作者使用的是 **Leiden 算法**（这是经典的 Louvain 算法的改进版，生成的社区结构更稳定、连贯）。

具体流程可以想象成是在整理一个杂乱无章的“人际关系网”：

**第一步：构建基础图谱**

*   系统通过 LLM 从文档中提取出实体（点）和关系（边）。
*   此时，整个图是非常巨大的，可能成千上万个点，密密麻麻连在一起。

**第二步：运行 Leiden 算法（分层聚类）**

*   Leiden 算法会遍历这个大图，通过最大化“模块度”（Modularity）来寻找联系紧密的群体。

*   **关键点：它是分层的（Hierarchical）。**

    *   **Level 0（最底层）**：可能会发现“张三、李四、王五”经常一起出现，把他们聚成一个微型社区（比如“销售A组”）。

    *   **Level 1（中间层）**：算法再把几个微型社区看作节点，继续聚类，发现“销售A组”和“销售B组”联系紧密，聚成“销售部”。

    *   **Level 2（高层）**：继续向上，把“销售部”和“市场部”聚成“营销中心”。

**第三步：生成社区摘要**

*   有了这些划分好的圈子（Level 0, 1, 2...），系统会把每一个圈子里的所有实体和关系描述丢给 LLM，让 LLM 写一段总结：“这个圈子主要是关于销售部门在2023年的业绩表现……”

**总结给学生：** 你可以把它想象成**绘制地图**的过程。

1.  先看到每家每户（原始实体）。
2.  算法自动划出“街道”（底层社区）。
3.  再把街道划归为“区”（中层社区）。
4.  最后划归为“城市”（高层社区）。GraphRAG 的强大之处在于，它不仅划好了区，还给每个区、每条街都写了一份\*\*“介绍信”（摘要）\*\*，查询的时候是先看介绍信的。

***

### 2. 本方案不支持实时更新的局限性，是否可以通过已有的图的增量更新技术来解决？

这是一个非常好的工程问题。**答案是：理论上可以，但代价比你想象的要大得多。**

之所以说它难，不是难在“图算法”上，而是难在“LLM 生成摘要”上。

我们来分析一下，如果我们把传统的**图增量更新技术**（Incremental Graph Update）直接套用进来，会遇到什么坑：

#### 难点 A：牵一发而动全身（拓扑结构的不稳定性）

*   **图算法层面**：Leiden 这类社区发现算法，虽然有增量版本，但社区划分往往对数据很敏感。

*   **场景**：假设原本图里有两个独立的社区：“苹果公司”和“造车新势力”。现在进来一篇新文档，提到“苹果造车”。这条新线索（边）可能会导致两个原本独立的巨大社区突然**合并**成一个新社区。

*   **后果**：一旦社区结构变了（哪怕只是合并或分裂），之前为旧社区生成的**所有摘要就作废了**。你需要重新让 LLM 把这个新合并的大社区读一遍，重写摘要。

#### 难点 B：摘要的过时（语义层面）

即使社区结构没变（新文档只是加入了一个现有社区），你也不能简单地把新节点插进去就完事。

*   **原因**：GraphRAG 的检索依赖的是**预先生成的摘要**。

*   **场景**：原本的社区摘要写着：“主要讨论了A产品的优点。”现在来了一篇新文档说：“A产品昨天爆炸了。”

*   **后果**：虽然图结构只需要加一个点，但你必须**更新社区摘要**，否则检索时，系统读到的还是旧摘要（只提优点），由于摘要里没提爆炸，系统就会回答错误。

*   **成本**：这意味着每插入少量新信息，你都要调用 LLM 对受影响的社区进行“重读”和“重写”。LLM 的调用成本是很高的。

#### 难点 C：层级传导（Ripple Effect）

别忘了 GraphRAG 是**分层**的。

*   如果你更新了底层的 Level 0 社区摘要，那么包含它的 Level 1 社区摘要可能也得变，Level 2 可能也得变……这就像蝴蝶效应，底层的一个小改动，可能需要重算整条路径上的所有摘要。

#### 结论：

**“图的增量更新”技术能解决“点和边”的插入问题，但解决不了“语义摘要”的低成本维护问题。**

**目前的解决思路（研究方向）：** 如果要解决这个问题，未来的研究方向可能不是全量重算，而是：

1.  **增量摘要（Incremental Summarization）**：让 LLM 只阅读“旧摘要”和“新文档”，生成“新摘要”，而不是每次都重读所有原始文档。

2.  **动态社区阈值**：降低社区变化的敏感度，尽量让新数据只影响局部，不引发大规模的社区重组。

所以，虽然 GraphRAG 在处理静态数据集（如历史档案、法律卷宗）时效果极佳，但如果用在每秒都有新闻产生的实时系统中，目前的架构确实还比较“笨重”。
