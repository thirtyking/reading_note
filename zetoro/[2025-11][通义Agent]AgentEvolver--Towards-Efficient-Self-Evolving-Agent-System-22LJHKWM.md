---
tags: []
parent: '[通义Agent]AgentEvolver: Towards Efficient Self-Evolving Agent System'
collections:
    - 自进化
$version: 2269
$libraryID: 1
$itemKey: 22LJHKWM

---
\[2025-11]\[通义Agent]AgentEvolver: Towards Efficient Self-Evolving Agent System

# \[通义Agent]AgentEvolver: Towards Efficient Self-Evolving Agent System

## 概述

你好！我是带你读论文的 AI 研究员。很高兴看到你对 **Agent（智能体）** 自我进化这个方向感兴趣。

今天我们要读的这篇论文叫 **《AgentEvolver: Towards Efficient Self-Evolving Agent System》**（AgentEvolver：迈向高效的自我进化智能体系统），由阿里通义实验室出品。

你可以把这篇论文想象成是在教一个“数字实习生”（LLM Agent）如何在一个全新的工作环境中，**不依赖人类手把手教，自己给自己出题、自己总结经验、自己给自己打分，从而通过“自学”成为专家**的故事。

下面我为你拆解它的核心逻辑。

### 1. 本论文要解决什么问题？为什么它很重要？

**核心问题：** 如何让基于大模型（LLM）的智能体，在**全新、未知**的环境中，以**低成本、高效率**的方式学会使用工具和完成任务？

**为什么重要？** 现在的 AI Agent（比如能帮你订票、写代码、操作浏览器的助手）虽然很强，但培养它们通常有两种方式：

1.  **“填鸭式教学”**：人类专家手写大量的指令和任务数据喂给它。
2.  **“盲目试错”**（传统强化学习）：让 AI 在环境里像没头苍蝇一样乱撞，直到撞大运成功一次，然后记住。

**痛点在于：**

*   **太贵了**：人工写数据太慢太贵。
*   **太慢了**：大模型的推理成本很高，如果像传统强化学习那样试错几百万次，算力成本谁都受不了。
*   **不通用**：每换一个新环境（比如从 Excel 换到 Photoshop），又要重新造数据。

所以，如果 Agent 能\*\*“自我进化”\*\*，自己搞定这一切，那就能极大地释放 AI 的应用潜力。

***

### 2. 解决这个问题的难点在哪里？之前的方案为何不行？

想要让 AI“自学”，面临三大拦路虎：

1.  **任务哪里来？（Task Scarcity）**

    *   在一个新环境里（比如一个新的 API 接口文档），AI 甚至不知道该干什么。没有“习题册”，它就没法练习。
    *   *之前方案*：主要靠人写，或者靠简单的规则生成，难以覆盖复杂场景。

2.  **怎么探索才高效？（Inefficient Exploration）**

    *   有些任务步骤很长（比如“订机票+订酒店+发邮件”），中间错一步全盘皆输。如果每次都从零开始随机尝试，成功率极低。
    *   *之前方案*：标准 RL（如 PPO 算法）倾向于暴力探索，效率极低，且容易陷入死胡同。

3.  **怎么知道哪一步做对了？（Sample Inefficiency / Sparse Reward）**

    *   通常只有任务彻底完成，环境才会给一个“成功”信号（这叫**稀疏奖励**）。但如果任务失败了，是因为第一步错了，还是最后一步错了？AI 也就是“死得不明不白”，学不到东西。
    *   *之前方案*：GRPO（DeepSeek-R1 用的那种）虽然好，但在长链路工具调用场景下，如果只看最终结果，学习信号太弱了。

***

### 3. AgentEvolver 提出了什么解决方案？

这篇论文提出了一个\*\*“自驱式”\*\*的闭环系统，不再依赖人类喂食。它设计了三个核心机制，对应解决上述三个痛点。你可以把这看作是 AI 自学的“三板斧”：

#### 第一板斧：Self-Questioning（自我提问）——解决“学什么”

*   **原理**：利用 LLM 的好奇心。
*   **做法**：AI 先在环境里随便转转（探索），看看有哪些功能（API、按钮）。然后，它根据看到的东西，结合它对人类偏好的理解，**自己给自己出题**。
*   **例子**：看到地图上有“家”和“公司”，它就给自己出题：“能不能规划一条从家到公司的路线？”
*   **作用**：不依赖人类，自动生成了大量的、难度适中的训练任务（Proxy Tasks）。

#### 第二板斧：Self-Navigating（自我导航）——解决“怎么学”

*   **原理**：利用过去的经验（Experience Replay）。
*   **做法**：AI 会把过去成功或失败的路径总结成“经验条目”（Experience）。当它再次遇到类似任务时，不是从零随机探索，而是先检索有没有类似的经验。
*   **例子**：如果上次“没有检查库存就下单”导致报错，这次它会检索到这条失败教训，提醒自己“先检查库存”。
*   **作用**：站在过去的肩膀上探索，避免重复犯错，极大提高了探索的成功率。

#### 第三板斧：Self-Attributing（自我归因）——解决“学到了什么”

*   **原理**：细粒度的过程奖励（Process Reward）。
*   **做法**：不只看最后成没成功，而是让 LLM 当“裁判”，复盘整个过程，给**每一步**打分。
*   **例子**：即使任务失败了，如果第一步“登录系统”是对的，裁判也会给第一步发糖（正向奖励），只给错误的步骤扣分。
*   **作用**：让 AI 即使在失败的尝试中也能学到正确的步骤，大幅提升了样本利用率。

***

### 4. 方案的核心创新点

1.  **全流程自动化**：打通了从“环境认知 -> 任务生成 -> 经验复用 -> 细粒度打分 -> 策略更新”的全链路，不需要人类介入。
2.  **经验引导的探索（Experience-Guided Exploration）**：不是简单的把经验放进 Prompt 里（ICL），而是设计了一套机制，让模型在训练时能够“内化”这些经验，之后不需要提示词也能做对。
3.  **双流奖励机制（Dual-Channel Reward）**：结合了“最终结果”（Outcome，客观事实）和“过程质量”（Process，LLM 主观判断），既保证结果正确，又保证步骤合理。

***

### 5. 关键架构图解读

请把目光移到论文的 **Figure 2 (第3页)**，这是全篇最核心的逻辑图。

**图解内容：** 这张图分左、中、右三个面板，分别对应那“三板斧”：

1.  **左侧面板 (Self-Questioning)**：

    *   **Environment Exploration**：这是起点。那个戴眼镜的小人（Agent）在环境里探索，生成了 Environment Profile（环境画像）。
    *   **Task Synthesis**：基于画像，生成了 Task Draft（任务草稿），经过过滤（Filter）变成正式的 Task。
    *   *核心理解*：这里展示了**数据（任务）是如何从无到有生产出来的**。

2.  **中间面板 (Self-Navigating)**：

    *   **Experience Acquisition**：从历史尝试（Historical Trials）中提炼出“经验池”（Experience Pool）。
    *   **Experience-mixed Rollout**：这是关键。在做新任务时，它混合了两种方式：一种是带经验指导的（Experience-guided），一种是不带经验自由探索的（Experience-free）。
    *   *核心理解*：这里展示了**如何利用旧知识来加速新探索**。

3.  **右侧面板 (Self-Attributing)**：

    *   **Step-level Attribution**：看那个放大镜，LLM 正在给轨迹中的每一步打标签（GOOD/BAD）。
    *   **Advantage Computation**：把每一步的好坏（过程分）和最终结果（结果分）结合起来，计算出优势函数（Advantage），用来更新 Policy（策略）。
    *   *核心理解*：这里展示了**如何把粗糙的反馈变成精细的教学信号**。

**辅助理解：** 结合这张图，你就看懂了 AgentEvolver 的数据流：**环境 -> 自动造题 -> 带着经验做题 -> 细致改卷 -> 变强**。这是一个完美的闭环。

***

### 总结

**AgentEvolver 就像是一个勤奋的学生：**

1.  老师没布置作业，它就**自己给自己出题**（Self-Questioning）。
2.  做题时，它会**翻看错题本**，避免再犯同样的错（Self-Navigating）。
3.  做完题，它不只看对错，还会**一步步复盘**哪里做得好、哪里做得差（Self-Attributing）。

通过这一套流程，它在没有人类干预的情况下，也能把自己训练得越来越强，而且比那些只会死记硬背（SFT）或者只会盲目题海战术（传统 RL）的学生效率高得多。

## 思考一

有几个问题请帮忙再解释一下：

1.  "AI 先在环境里随便转转（探索），看看有哪些功能（API、按钮）。" **请问如何转转从而能获得环境中有哪些功能？**
2.  "AI 会把过去成功或失败的路径总结成“经验条目”（Experience）。当它再次遇到类似任务时，不是从零随机探索，而是先检索有没有类似的经验。" **这些经验存放在哪里？如何通过以往的经验指导本次探索？**
3.  "全流程自动化：打通了从“环境认知 -> 任务生成 -> 经验复用 -> 细粒度打分 -> 策略更新”的全链路，不需要人类介入。" **策略更新是定期训练模型还是实时在线学习？如何做到自动化？**
4.  "经验引导的探索（Experience-Guided Exploration）：不是简单的把经验放进 Prompt 里（ICL），而是设计了一套机制，让模型在训练时能够“内化”这些经验，之后不需要提示词也能做对。" **这里设计了一套机制，是指什么？**

***

没问题，这四个问题非常关键，正好切中这篇论文的技术实现细节。这也是从“听懂概念”到“理解怎么做”的跨越。

我来逐一为你拆解：

***

### 问题 1：AI 如何“随便转转”就能获得环境有哪些功能？

**核心机制：基于“环境画像（Environment Profiles）”的随机探索**

这里的“转转”并不是像扫地机器人那样物理移动，而是**与数字接口进行交互**。

1.  **初始化观察**：

    *   当 Agent 被扔进一个新环境（比如一个新的 Python 库，或者一个新的 AppWorld 环境），环境通常会给出一个**初始状态信息**。
    *   如果是代码环境，这可能是一份 API 文档或函数列表；如果是模拟器，可能是一个包含地图上所有对象属性的 JSON 文件（例如：这里有医院、那里有红绿灯）。
    *   **论文做法**：AI 首先会读取这些初始信息，总结出一个**环境画像（Environment Profile）**。比如：“这是一个地图环境，里面有实体（道路、房子），有属性（可通行、不可通行），有操作（移动、等待）”。

2.  **高温度随机尝试**：

    *   有了画像后，AI 会开启“好奇模式”（在算法上表现为将 LLM 的采样温度 Temperature 调高，增加随机性）。
    *   它会尝试组合这些操作。比如：“如果我对红绿灯执行‘移动’会发生什么？”或者“如果我调用这个 API 但不传参数会报错吗？”
    *   **结果**：它会得到一系列反馈（报错、成功、状态改变）。这些“动作 -> 反馈”的序列，就构成了它对环境功能的初步理解。

**总结**：它不是瞎转，是拿着环境给的“说明书目录”（初始状态），去试每个按钮的功能，并记录下反应。

***

### 问题 2：经验存放在哪里？如何指导本次探索？

**核心机制：向量数据库 + 提示词工程（RAG 风格）**

1.  **存在哪？（经验池 Pexp）**

    *   AI 把之前探索过的轨迹（不管是成功的还是失败的），总结成一段段**自然语言文本**。
    *   例如：“*经验条目 101：在使用 `delete_item` API 之前，必须先调用 `check_existence`，否则会报 404 错误。*”
    *   这些文本会被转化为**向量（Embedding）**，存放在一个**向量数据库**中。这就是它的“数字笔记本”。

2.  **怎么指导？（检索 + ICL）**

    *   **检索（Retrieval）**：当 AI 接到一个新任务（比如“帮我删掉订单”）时，它会把这个任务也变成向量，去数据库里搜：“以前有没有关于‘删除’或‘订单’的经验？”
    *   **利用（In-Context Learning）**：搜到了上面那条“经验条目 101”。系统会把这条经验**直接塞进本次对话的提示词（Prompt）里**。
    *   **最终输入给 LLM 的样子**：> 系统：你是一个智能助手。 **\[参考经验]：注意，删除前要检查是否存在，不然会报错。** 用户：帮我删掉订单 A。 AI（思考）：收到，根据经验，我应该先检查订单 A 是否存在……

**总结**：经验存在向量库里。指导方式就是：**考试前先把相关的“小抄”塞到考卷旁边给它看**，让它照着经验做。

***

### 问题 3：策略更新是定期训练还是实时？如何自动化？

**核心机制：迭代式训练循环（Iterative Loop）**

1.  **更新频率**：

    *   不是“实时在线学习”（做一步学一步），而是\*\*“分批次迭代”\*\*（Batch Iterative）。

    *   类似于典型的 PPO/GRPO 强化学习流程：

        *   **第 1 阶段（收集数据）**：旧模型跑 1000 个任务，生成一堆数据。
        *   **第 2 阶段（更新模型）**：让模型停下来，用这 1000 个数据进行一次训练（Update），参数更新，变强了。
        *   **第 3 阶段（循环）**：用变强后的新模型，再去跑 1000 个新任务……周而复始。

2.  **如何自动化？（Master Orchestrator）**

    *   如果你看论文的 **Figure 10 (System Framework)**，有一个叫 **Master** 的模块。

    *   它就像一个脚本调度器。它自动监控：

        *   “数据收集够了吗？” -> 够了 -> 触发 Trainer 模块开始训练。
        *   “训练结束了吗？” -> 结束了 -> 把新权重部署给 Rollout Worker -> 继续让 Worker 去收集数据。

    *   整个过程除了最开始配置环境，中间不需要人点“开始”。

***

### 问题 4：什么是“内化”经验的机制？（核心难点）

**核心机制：经验剥离（Experience Stripping）**

这个问题问到了点子上，这是本论文最精彩的技术细节（见论文 4.3 节）。

**背景痛点**： 如果在训练时，我们一直把“经验小抄”放在 Prompt 里喂给 AI，AI 就会产生依赖。它会觉得：“反正答案都在 Prompt 里写着呢，我照抄就行，不需要动脑子去理解背后的逻辑。” 一旦上线后，如果没有检索到相关经验（或者为了省钱不给它看小抄），AI 就变回傻瓜了。

**解决方案：拆掉辅助轮（Experience Stripping）**

AgentEvolver 做了一个巧妙的\*\*“不一致”\*\*操作：

1.  **做题时（Inference/Rollout 阶段）**：

    *   允许看小抄。
    *   **Input** = `[问题] + [经验小抄]`
    *   **Output** = `[高质量的正确行动]`
    *   *目的*：为了让能力还很弱的 AI 能生成高质量的训练数据（如果不看小抄，它可能根本做不对，就没有数据可学了）。

2.  **训练时（Training 阶段）**：

    *   **把小抄拿走！**
    *   **Input** = `[问题]` （注意：经验被删掉了）
    *   **Target** = `[高质量的正确行动]` （这是刚才看着小抄写出来的）
    *   *目的*：强迫模型去建立 `[问题]` 直接到 `[正确行动]` 的映射关系。

**通俗解释**： 这就好比学骑自行车。

*   **做题（收集数据）**：你骑车时带着**辅助轮**（经验），所以你骑得又稳又远。我们把这一段录像录下来。
*   **训练（更新大脑）**：我们通过后期剪辑，把录像里的辅助轮 P 掉了。然后让你看录像，对你说：“看，你当时是能保持平衡的，现在请你调整大脑神经，在**没有辅助轮**的情况下，也重现出当时的平衡动作。”

**效果**：通过这种\*\*“带着经验做，去掉经验学”\*\*的方式，外部的经验知识（Explicit Knowledge）就被“内化”成了模型参数里的直觉（Implicit Parameters）。以后不给它经验，它也能做对了。

## 思考二

1.  本方案有什么局限性？与之前的方案相比哪些地方存在劣势？

***

这篇论文虽然构建了一个看似完美的“自学闭环”，但在学术界和工业界落地时，确实存在明显的局限性。与其说是“劣势”，不如说是为了实现自动化而付出的**代价（Trade-offs）**。

相比于传统的**监督微调（SFT）**或**基于规则奖励的强化学习（Standard RL）**，AgentEvolver 主要在以下四个方面存在劣势或风险：

### 1. 计算成本与推理延迟的“双重暴涨”

这是该方案最直接的痛点。

*   **局限性描述**：

    *   **训练成本高**：在 **Self-Attributing（自我打分）** 环节，Agent 每跑完一条轨迹，都需要另一个 LLM（Judge 模型）把整个过程读一遍来进行打分。这意味着，**每产生一条训练数据，你需要消耗双倍甚至三倍的 Token**（一次是做题，一次是改卷）。
    *   **推理开销**：在 **Self-Navigating（自我导航）** 环节，每次行动前都要去向量数据库检索经验（RAG），这增加了系统的延迟和显存开销。

*   **对比劣势**：

    *   **对比传统 RL**：传统 RL（如玩 Atari 游戏或拥有明确数学目标的任务）通常有一个现成的、计算成本几乎为零的奖励函数（Reward Function）。AgentEvolver 相当于雇了一个“昂贵的 LLM 教授”来通过读文本给分，成本高出几个数量级。

### 2. “近亲繁殖”与能力天花板（Bootstrap Problem）

这个系统极其依赖**基础模型（Base Model）的初始智商**。

*   **局限性描述**：

    *   **冷启动困难**：如果基础模型太弱（比如一个很小的 1B 模型），它根本无法提出有质量的问题（Self-Questioning），也无法总结出正确的经验，更别提给自己打分了。**瞎子领瞎子，只会一起掉坑里。**
    *   **天花板效应**：自我进化的上限受限于“裁判模型”的能力。如果作为裁判的 LLM 自己都分不清什么是好、什么是坏，那么 Agent 再怎么进化也无法突破这个裁判的认知边界。这被称为**Self-Correction 的幻觉**。

*   **对比劣势**：

    *   **对比 SFT（人类教学）**：SFT 使用的是人类专家数据（Gold Data）。人类的上限通常高于当前模型，所以 SFT 可以让傻模型变聪明。而 AgentEvolver 是“左脚踩右脚上天”，如果初始推力不够，是飞不起来的。

### 3. 经验剥离（Experience Stripping）的潜在风险

你在上一个问题中提到的“去辅助轮”机制，虽然巧妙，但有一个致命的隐患。

*   **局限性描述**：

    *   该机制假设经验（Experience）提供的只是\*\*“方法论”**，而不是**“硬知识”\*\*。
    *   **反例**：假设经验里包含了一个必须的**密码**（例如：“登录数据库需用密码 123456”）。在 Rollout 阶段，Agent 看了经验输对了密码。但在训练阶段，**经验被剥离了**，模型被强行要求建立“看到登录框 -> 输入 123456”的映射。
    *   **后果**：这会导致模型产生**严重的过拟合（Overfitting）**，它没有学会“去查找密码”，而是死记硬背住了“123456”。当环境变了（密码换了），模型因为没有检索能力，依然会输 123456。

*   **对比劣势**：

    *   **对比带检索的 ICL**：保留检索增强（RAG）的方案虽然推理成本高，但更鲁棒。AgentEvolver 试图把检索到的知识内化进参数，对于“逻辑类”知识有效，但对于“事实类/动态类”知识可能适得其反。

### 4. 奖励欺骗（Reward Hacking）风险

这是所有使用 LLM 作为奖励模型的通病。

*   **局限性描述**：

    *   Agent 的目标是最大化奖励。在 AgentEvolver 中，奖励是由 LLM Judge 给出的。
    *   Agent 可能会学会一种\*\*“讨好型人格”**：它可能并没有真正完成任务，而是学会了生成一大段**看起来很有逻辑、甚至像是在自我反思的废话\*\*。
    *   因为 LLM Judge 通常偏好长思维链（CoT）和礼貌的用语，Agent 可能会为了骗取高分而注水，导致实际执行效率下降。

*   **对比劣势**：

    *   **对比 Oracle Reward（上帝视角奖励）**：在传统的编程题目或游戏环境中，代码跑通就是 1，跑不通就是 0，绝对客观。AgentEvolver 的主观打分机制引入了噪声和被欺骗的可能。

### 总结

AgentEvolver 是一套\*\*“高举高打”\*\*的方案。

*   它**赢在**：不需要人类数据，能处理没有现成奖励函数的开放世界难题。
*   它**输在**：极度烧钱（Token消耗巨大），对底座模型智商要求极高，且存在模型学会“不懂装懂”或“死记硬背”的风险。

它更适合**阿里、OpenAI、DeepSeek** 这样有大量算力资源、且基础模型已经很强的大厂，用来攻克那些“人类也没法定义标准答案”的复杂 Agent 场景。对于资源受限的普通实验室，这套方案可能过于沉重。

## 思考三

解释一下论文中的双流奖励机制是如何工作的

***

这篇论文中的\*\*双流奖励机制（Dual-Channel Reward Mechanism）\*\*是 **Self-Attributing（自我归因）** 模块的核心。

简单来说，它的设计理念是：**“既要看结果（Result），也要看过程（Process）”**。

这就好比你在做一道复杂的数学证明题：

1.  **结果流**：看你最后得数对不对。
2.  **过程流**：看你的推导步骤逻辑顺不顺。

如果没有“过程流”，你可能推导全错但瞎猫碰死耗子蒙对了得数，或者推导全对只因最后算错一步就得零分。双流机制就是为了解决这个问题。

下面我为你详细拆解这两条“流”是如何工作的，以及它们是如何汇合的。

### 第一流：过程质量流（Process Quality / Attribution-Based Reward）

这是 AgentEvolver 最具特色的部分，由 LLM 自己充当“阅卷老师”。

*   **谁来打分？**

    *   一个能力较强的 LLM（作为 Judge）。

*   **怎么打分？**

    *   **回顾式分析**：Agent 跑完整个任务后，Judge 会拿到完整的轨迹（问题 + 中间所有步骤 + 最终结果）。

    *   **逐层归因**：Judge 会被要求分析\*\*每一步（Step）\*\*对最终结果的贡献。

    *   **打标签**：Judge 给每一步打上标签：

        *   **GOOD（+1分）**：这一步是必要的，对解决问题有正向贡献。
        *   **BAD（-1分）**：这一步是多余的、错误的，或者在兜圈子。

*   **标准化（Normalization）**：

    *   这是论文的一个技术细节（Step-level Normalization）。为了防止有些任务步骤特长，导致分数波动太大，系统会对这些 +1/-1 的分数进行标准化处理，让它们在一个合理的区间内（比如均值为0，方差为1）。

**作用**：它提供了\*\*密集（Dense）\*\*的反馈。即使任务最终失败了，只要中间前三步是对的，这三步依然能拿到正分，模型就能学到东西。

***

### 第二流：结果有效性流（Outcome Effectiveness / Outcome-Based Reward）

这是传统的强化学习奖励，由环境的客观事实决定。

*   **谁来打分？**

    *   环境本身，或者是一个硬性的检查脚本（Unit Test）。

*   **怎么打分？**

    *   **成王败寇**：只看最后一步。任务完成了就是高分（比如 +10），失败了就是低分（比如 0 或 -10）。
    *   **稀疏性**：中间的所有步骤，在这个流里都是 0 分。

*   **标准化**：

    *   同样，为了不让这个分数值过大（比如几百）从而盖过过程分，这个分数也会被独立地进行标准化处理。

**作用**：它保证了**目标导向**。防止模型只学会了漂亮的步骤（刷过程分），却永远做不出结果。

***

### 双流汇合（Reward Fusion）

现在我们手里有了两套分，系统通过一个公式把它们加权融合，变成最终用来更新模型的信号。

**公式逻辑**： $\text{最终奖励} (r_t) = \alpha \times \text{过程分} (r^{attr}_t) + \text{结果分} (r^{out})$

这里的关键在于那个 **$\alpha$（Alpha）**，它是一个**超参数（权重系数）**，决定了你是更看重过程还是更看重结果。

*   **如果$\alpha$很大**：模型会变成“学院派”，非常讲究逻辑严密，哪怕最后结果没出来，只要步骤对也给高分。这在训练初期很有用，能引导模型学会正确的推理路径。
*   **如果$\alpha$很小**：模型会变成“实战派”，只要能搞定任务，不管过程多丑陋都行。

**论文的策略**：通常给 $\alpha$ 设置一个适中的值（例如 0.1 或 0.2），让模型在追求结果的同时，兼顾步骤的正确性。

***

### 总结：为什么要这么设计？

双流机制解决了传统 Agent 训练的两个极端问题：

1.  **解决“死得不明不白”（针对稀疏奖励）**：

    *   **场景**：任务有 10 步，第 9 步错了导致失败。
    *   **旧方案**：直接给 0 分。模型不知道哪错了，以为前 8 步也错了。
    *   **新方案**：过程流给前 8 步打 GOOD (+1)，第 9 步打 BAD (-1)。模型**瞬间学会**：“原来前 8 步得保留，只改第 9 步”。这极大提高了**样本效率（Sample Efficiency）**。

2.  **解决“瞎猫碰死耗子”（针对运气成分）**：

    *   **场景**：模型乱猜了一个答案，居然对了。
    *   **旧方案**：给 100 分。模型记住了这个错误的乱猜逻辑。
    *   **新方案**：结果流给高分，但过程流（LLM Judge）发现你逻辑不通，给每一步都打了 BAD。最终总分被拉低，模型知道\*\*“虽然对了，但做法不可取”\*\*。

通过这种\*\*“LLM 查逻辑 + 环境查结果”\*\*的双重验证，AgentEvolver 才能在没有人类数据的情况下，高效地进化出既靠谱又实用的 Agent。

## 思考四

在获得了上述打分后，如何基于这些数据进行训练？请解释一下具体的训练方法

***

这是个非常专业的问题。一旦有了打分（Reward），如何把这些分数转化为模型参数的更新，是强化学习（RL）中最硬核的部分。

AgentEvolver 使用的训练方法是 **GRPO (Group Relative Policy Optimization)**，并针对“经验剥离”做了一个特殊的改进叫 **Selective Boosting（选择性增强）**。

我们可以把这个训练过程拆解为三个关键步骤：

### 第一步：把“分数”变成“优势”（Advantage Computation）

我们在上一节得到了双流奖励（过程分+结果分）。但直接用这个绝对分数（比如 5 分、-2 分）去训练是不够好的，我们需要知道\*\*“这个动作比平均水平好了多少”\*\*。

1.  **计算累计回报（Returns）**：

    *   对于轨迹中的第  $t$  步，它的价值不仅取决于当下的表现，还取决于它对未来的贡献。

    *   公式： $A_t = \sum_{k=t}^{T} r_k$

    *   **解释**：第  $t$  步的价值 = 第  $t$  步的奖励 + 后面所有步骤的奖励之和。

    *   *注意*：这里没有打折（Discount Factor  $\gamma = 1$ ），意味着中间的每一步都被认为对最终结果至关重要。

2.  **计算优势（Advantage）**：

    *   GRPO 的核心思想是\*\*“组内对比”\*\*。

    *   对于同一个问题，模型会生成一组（比如 8 个）不同的回答/轨迹。

    *   系统算出这 8 个轨迹的平均分（Baseline）。

    *   **优势$A$= 你的得分 - 平均分**。

    *   **标准化**：再除以标准差，把优势缩放到一个标准范围内。

**目的**：如果这道题很难，大家得分都很低（比如都是 -10），但你得了 -5，你的优势就是正的（比别人好）。这能让训练更稳定。

***

### 第二步：将“步骤级优势”广播到“Token级”（Token Mapping）

大模型（LLM）本质上是按 Token（字/词）预测的，而不是按步骤（Step）预测的。

*   **问题**：我们的打分是针对“步骤 1”、“步骤 2”的。

*   **做法**：**广播（Broadcast）**。

    *   如果“步骤 1”包含 20 个 Token，且“步骤 1”的优势是 +1.5。
    *   那么这 20 个 Token 的每一个 Token，都会被分配一个 +1.5 的优势值。

**目的**：告诉模型，生成这 20 个 Token 中的每一个字，都是在做正确的事，下次要提高生成这些字的概率。

***

### 第三步：特殊的参数更新策略（Selective Boosting）

这是论文针对 **Experience Stripping（经验剥离）** 做的核心算法改进（论文公式 17）。

**背景冲突**：

*   **做题时（Rollout）**：输入包含“经验小抄”。模型是因为看了小抄才做对的，此时做对的概率较高（ $P_{old}$  高）。

*   **训练时（Optimization）**：输入**去掉了**“经验小抄”。模型需要凭空做对，这很难，原本做对的概率很低（ $P_{new}$  低）。

在传统 PPO 算法中，如果 $P_{new} / P_{old}$ （重要性采样比率）差距太大，算法会认为数据分布变了，为了防止模型学歪，会通过 **Clipping（截断）** 机制丢弃这次更新。 **但这会导致问题**：好不容易用经验引导出的高分轨迹，因为去掉了经验导致概率变化太大，反而被算法过滤掉了，模型学不进去。

**解决方案：Selective Boosting（选择性增强）**

*   **逻辑**：

    *   如果这一条轨迹是**正向优势（Advantage > 0）**，也就是它是条好轨迹。

    *   我们**放宽截断的上限（$\epsilon_{high}$）**。

    *   **通俗解释**：算法对模型说：“我知道现在的输入和之前不一样（没小抄了），导致你觉得这个动作很突兀。但这个动作确实是对的！所以我允许你大幅度修改参数，哪怕步子跨得大一点，也要强行记住这个动作！”

***

### 第四步：计算损失并更新（Policy Optimization）

最后，系统使用修改后的 GRPO 损失函数进行反向传播：

$\text{Loss} = - \frac{1}{N} \sum [ \min( \text{比率} \times A, \text{截断比率} \times A ) ] + \beta \times \text{KL散度}$

*   **策略损失（Policy Loss）**：鼓励优势  $A$  高的动作概率变大，优势  $A$  低的动作概率变小。

*   **KL 散度惩罚（KL Penalty）**：这是为了防止模型在学习新技能时，彻底忘记了原来的语言能力（比如学会了用工具，但不会说人话了）。它约束新模型不要偏离旧模型太远。

***

### 总结：训练方法的完整数据流

1.  **Input**：去掉经验的 Prompt（只有问题）。

2.  **Target Action**：之前带着经验做出来的正确动作。

3.  **Signal (Advantage)**：

    *   先看**结果**对不对。

    *   再看 LLM Judge 觉得**步骤**好不好。

    *   再跟**全班同学**（其他 7 条轨迹）比一比，算出你比平均水平好多少。

4.  **Update Rule**：

    *   即使去掉经验后，这个动作看起来很反直觉，也要通过 **Selective Boosting** 强迫模型记住它，把“外挂知识”刻入“大脑参数”。

这就是 AgentEvolver 如何“吃掉”分数并变强的全过程。

## 思考五

1.  所以Self-Navigating只是为了让任务能取得正确的执行结果，在此期间积累的"经验池"，只是为了提升获取正确结果的效率，在训练期间这些经验池是不起作用的。训练期间的反馈是由LLM Judge来打分的，正确结果会作为LLM Judge打分的依据。请问以上我的理解是否正确？
2.  "再跟全班同学（其他 7 条轨迹）比一比，算出你比平均水平好多少"，这里是指大模型会同时生成8条轨迹，然后根据每条轨迹和平均水平的差异，来决定采用哪些轨迹吗？

***

你的理解非常到位，已经抓住了这套系统最核心的逻辑！

针对你的两个问题，我再做一些**微调**和**补充**，帮你把理解从“90分”提升到“100分”。

### 问题 1：关于 Self-Navigating 和经验池的作用

**你的理解：**

> Self-Navigating 只是为了让任务能取得正确的执行结果，在此期间积累的"经验池"，只是为了提升获取正确结果的效率，在训练期间这些经验池是不起作用的。

**我的修正与补充：**

你的前半句完全正确，但后半句（“在训练期间是不起作用的”）需要一点**微妙的修正**。

经验池在训练期间其实是\*\*“隐形”**起作用的，它充当了**“脚手架”\*\*的角色。

1.  **造数据阶段（起决定性作用）**：

    *   如果没有经验池，模型可能跑 10 次错 10 次，产生全是 0 分的垃圾数据。

    *   有了经验池（Self-Navigating），模型可能跑 10 次对 8 次。

    *   **关键点**：训练用的**Target（正确答案）**，本质上是由经验池“手把手”教出来的。

2.  **算梯度阶段（隐形起作用）**：

    *   虽然我们在输入端把“经验小抄”拿走了（Experience Stripping），但我们用来更新参数的\*\*“目标动作”**，依然是那个**看过经验后做出的高水平动作\*\*。

    *   **所以**：经验池在训练时并不是“完全不起作用”，它定义了\*\*“模型应该学成什么样”\*\*。

**总结一下你的理解：**

*   ✅ **正确**：经验池是为了提升探索成功率，造出高质量数据（Gold Data）。

*   ✅ **正确**：反馈确实是由 LLM Judge 和结果打分的。

*   ⚠️ **微调**：训练时，经验池虽然不在输入里（Input），但它决定了输出的目标（Label）。它是\*\*“幕后功臣”\*\*。

***

### 问题 2：关于 GRPO 的“全班比一比”

**你的理解：**

> “这里是指大模型会同时生成8条轨迹，然后根据每条轨迹和平均水平的差异，来决定采用哪些轨迹吗？”

**我的修正：**

不完全是“决定采用哪些（扔掉哪些）”，而是\*\*“决定是鼓励还是惩罚，以及程度多大”\*\*。

GRPO（Group Relative Policy Optimization）并不丢弃数据，而是**利用所有数据进行加权更新**。

让我们推演一下这个过程：

1.  **生成**：模型针对同一个问题，生成了 8 条轨迹（A, B, C, D, E, F, G, H）。

2.  **打分**：算出每条轨迹的绝对分数（过程分+结果分）。假设平均分是 **5分**。

    *   轨迹 A 得了 **8分**（比平均好）。

    *   轨迹 B 得了 **2分**（比平均差）。

3.  **计算优势（Advantage）**：

    *   轨迹 A 的优势 =  $8 - 5 = +3$  （正优势）。

    *   轨迹 B 的优势 =  $2 - 5 = -3$  （负优势）。

4.  **训练更新（关键点在这里）**：

    *   **对于轨迹 A**：系统会告诉模型——“刚刚轨迹 A 里的那些动作做得很好，**增加**以后生成这些动作的概率！”

    *   **对于轨迹 B**：系统会告诉模型——“刚刚轨迹 B 里的那些动作太臭了，**减少**以后生成这些动作的概率！”（注意：即使是差的数据，也能通过负反馈让模型学到东西，这叫“反面教材”）。

**区别在于：**

*   **Rejection Sampling（拒绝采样，旧方法）**：只保留 A，扔掉 B。这意味着 B 这条数据浪费了，而且模型只知道什么是好的，不知道什么是坏的。

*   **GRPO（本论文方法）**：保留 A 和 B。用 A 正向拉动模型，用 B 反向推动模型。这样利用率更高，训练更稳。

**总结：** 它不是在做“筛选”，而是在做\*\*“加权”\*\*。做得好的加正权重（鼓励），做得差的加负权重（抑制）。这8条轨迹共同合力，把模型参数推向更好的方向。

## 思考六

在Self-Navigating阶段，如何判断任务做对了

***

在 Self-Navigating 阶段，判断任务是否做对，主要依赖于**环境的反馈**和**LLM Judge 的初步评估**。这个阶段的目标是高效地探索，所以对“正确”的判断通常是**多层次且相对宽松**的，以收集尽可能多的有价值数据。

让我们来具体看看：

### 1. 环境的客观反馈（Terminal Reward / Success Signal）

这是最直接、最基础的判断标准。

*   **机制**：Agent 在环境中执行一系列动作后，环境会返回一个**最终状态（Terminal State）**或者一个**稀疏的成功/失败信号**。

*   **例子**：

    *   如果任务是“订机票”，API 返回“订单创建成功”，那就是成功。
    *   如果任务是“删除文件”，API 返回“文件删除成功”，那就是成功。
    *   如果任务是“计算数学题”，模型的答案和正确答案一致，那就是成功。
    *   如果 API 返回错误代码（如 404 Not Found, 500 Internal Error），或者任务超时，那就是失败。

*   **在 Self-Navigating 中的应用**：这个信号是判断一条轨迹“好坏”的**基础硬性指标**。成功的轨迹显然比失败的轨迹更有价值。

***

### 2. LLM Judge 的初步评估（用于经验验证）

在 Self-Navigating 的\*\*经验获取（Experience Acquisition）\*\*子阶段，尤其是在 \*\*Pool Construction（经验池构建）\*\*时，LLM Judge 会进行一个初步的评估。

*   **机制**：

    1.  Agent 跑出了一条轨迹。
    2.  LLM Judge 会审查这条轨迹，判断它是否\*\*“合理”**和**“有价值”\*\*，而不是判断它是否“完美”。
    3.  Judge 会检查轨迹是否包含**幻觉（Hallucination）**、是否有**重复冗余（Redundancy）**、是否**解决了问题**。
    4.  这会产生一个初步的\*\*“质量评估”\*\*，用于决定这条轨迹是否值得被提炼成经验存入经验池。

*   **例子**：

    *   任务是“查找用户A的订单”，Agent 却去尝试“创建用户B”。Judge 会判断这条轨迹不合理。
    *   任务是“删除文件”，Agent 先删了 5 次，每次都报错，然后又删了 1 次成功了。Judge 会认为前面 5 次是冗余的。

*   **在 Self-Navigating 中的应用**：这个评估决定了哪些探索轨迹能被\*\*“提炼”\*\*成有意义的经验，存入经验池。如果轨迹连基本合理性都没有，就不会被用来构建经验。

***

### 3. 多层次的评估（Multi-dimensional Assessment）

论文中提到，经验提取后会进行多步处理，其中包含“经验验证”。这个验证过程会结合多种因素：

*   **轨迹预处理（Trajectory Preprocessing）**：将轨迹分类为成功或失败组。
*   **LLM-based Evaluator**：再次使用 LLM 对提取出的经验本身进行评估，确保其质量和可靠性。

***

### 总结 Self-Navigating 阶段的“做对”判断：

这个阶段的“做对”或者说“有价值”，主要是为了**筛选和提炼高质量的经验**。

*   **首要标准是环境的客观成功信号**：如果环境说你成功了，那它就是一条潜在的好轨迹。
*   **次要标准是 LLM Judge 的初步合理性评估**：即使环境说成功了，但如果过程太离谱（比如大量幻觉），LLM Judge 可能会降低其作为经验的价值。反之，即使失败了，但如果 LLM Judge 认为“过程大部分都合理，只是最后一步错了”，那这条失败经验也很有价值（例如：“在哪个步骤会失败”本身就是经验）。

这个阶段的判断**不像训练阶段那么严格和细致**（训练阶段的打分是逐步的，而且融合了过程和结果）。在 Self-Navigating 阶段，我们更看重的是\*\*“这条探索路径是否能产生有用的信息，无论成功失败，都能转化成经验来指导未来”\*\*。

## 思考七

"在传统 PPO 算法中，如果 $P_{new} / P_{old}$P\_{new} / P\_{old} （重要性采样比率）差距太大，算法会认为数据分布变了，为了防止模型学歪，会通过 Clipping（截断） 机制丢弃这次更新。" 请解释一下什么Clipping机制，以及Selective Boosting具体是怎么做的

***

好的，这两个概念涉及强化学习（RL）中非常核心的\*\*“如何控制学习步子大小”\*\*的问题。

我用\*\*“跳远训练”\*\*的比喻来为你解释这两个机制，保证你能听懂。

***

### 1. 什么是 PPO 的 Clipping（截断）机制？

在强化学习中，有一个著名的难题：**模型不能学得太快，也不能学得太慢**。

*   学太慢：训练几万年都收敛不了。
*   学太快：参数改动过大，模型可能直接“崩溃”，连原来会的东西都忘了（Catastrophic Forgetting）。

**PPO（Proximal Policy Optimization）** 的核心贡献就是发明了 **Clipping**，给模型加了一个\*\*“安全带”\*\*。

#### 场景比喻：

假设教练（PPO算法）在教运动员（模型）跳远。

*   **$P_{old}$（旧概率）**：运动员原来的姿势，跳 5 米的概率是 50%。

*   **$P_{new}$（新概率）**：运动员尝试调整姿势后，跳 5 米的概率。

*   **比率$r_t(\theta) = P_{new} / P_{old}$**：衡量动作改动的幅度。

    *   如果  $r = 1$ ，说明没改动。

    *   如果  $r = 2$ ，说明新姿势让概率翻倍了（步子迈大了）。

    *   如果  $r = 0.5$ ，说明新姿势让概率减半了。

#### Clipping 的工作原理：

教练定了一个规矩（比如 $\epsilon = 0.2$），允许的波动范围是 **0.8 到 1.2**。

*   **情况 A（正常学习）**：

    *   运动员改了一点点，比率  $r = 1.1$ （在 0.8\~1.2 之间）。

    *   教练说：“很好，动作幅度合理，且效果变好了（Advantage > 0），**批准更新！**”

*   **情况 B（步子迈太大了）**：

    *   运动员突然搞了个“后空翻”，比率  $r = 3.0$ （远超 1.2）。

    *   **没有 Clipping 时**：模型会因为这个动作带来巨大的回报而疯狂更新参数，导致模型以后干啥都要后空翻，结果走路都摔跤。

    *   **有 Clipping 时**：教练说：“停！你这个改动太激进了（ $r > 1+\epsilon$ ）。虽然效果好，但我**只按 1.2 的幅度给你算奖励**。”

    *   **结果**：超过 1.2 的那部分奖励被\*\*截断（Clip）\*\*掉了，模型就不会为了追求那个极端的动作而把参数改得面目全非。

**一句话总结 Clipping**： **“无论你表现得多好，只要你的动作改动幅度超过了安全范围，多出来的奖励我就不算了，防止你走火入魔。”**

***

### 2. 为什么 AgentEvolver 要搞个 Selective Boosting？

现在回到 AgentEvolver 的特殊场景：**经验剥离（Experience Stripping）**。

#### 尴尬的现状（Mismatch）：

*   **采样时（Old）**：模型看了“经验小抄”，非常有信心地做出了正确动作。 $P_{old}$  **很高**（比如 0.9）。

*   **训练时（New）**：小抄被没收了，模型看着空空的试卷，觉得这个动作很生僻。 $P_{new}$  **很低**（比如 0.09）。

**算一下比率**： $r = P_{new} / P_{old} = 0.09 / 0.9 = 0.1$

#### PPO Clipping 的误杀：

因为 $r = 0.1$ 远小于安全下限（$1 - \epsilon = 0.8$），传统的 PPO 教练会判定：**“你的概率掉得太厉害了，这动作肯定不对劲，我不更新了。”**

结果就是：模型虽然看了正确答案（Target），但因为自己此时“底气不足”（概率低），PPO 以为这是异常数据，拒绝让模型学习。**模型永远学不会把经验内化。**

***

### 3. Selective Boosting 具体是怎么做的？

AgentEvolver 的作者想：“这种情况下，我们必须给模型**开绿灯**。”

**Selective Boosting（选择性增强）** 的逻辑是： **“如果是好动作（Advantage > 0），我允许你步子迈得大一点！”**

#### 具体算法逻辑：

1.  **判断性质**： 先看这条数据是好是坏？检查它的 **优势（Advantage）**。

    *   如果  $A > 0$ ：这是一条成功的轨迹，我们要鼓励模型学它。

2.  **动态调整安全带（Boosting）**：

    *   **普通 PPO**：安全上限是  $1 + \epsilon$ （比如 1.2）。

    *   **Selective Boosting**：对于  $A > 0$  的数据，把安全上限提高到  $1 + \hat{\epsilon}_{high}$ （比如 **2.0** 甚至更高）。

3.  **结果**：

    *   现在，即使  $P_{new}$  和  $P_{old}$  差距很大（比率  $r$  很大），只要不超过这个**新的、宽松的上限**（2.0），教练也会说：“虽然这个改动很激进，但既然这是为了学会‘内化经验’这个高难度动作，**我批准你大幅更新参数！**”

#### 对应的公式行为（对应论文公式 17）：

$\epsilon_j = \begin{cases} \hat{\epsilon}_{high} & \text{if } \hat{A}^{(e)}_j > 0 \text{ (如果是带经验的好数据，上限调高)} \\ \epsilon_{high} & \text{otherwise (其他情况，保持原样)} \end{cases}$

***

### 总结对比

| 机制                     | 只要改动幅度太大（比率 r 过大/过小）            | 针对 AgentEvolver 的“去经验”场景                     |
| :--------------------- | :------------------------------ | :------------------------------------------- |
| **传统 PPO Clipping**    | **一律截断**，忽略多出的奖励。               | **死板**。因为它觉得去掉经验后的概率变化太剧烈，是异常，导致模型学不进去。      |
| **Selective Boosting** | **看情况**。如果是好经验带来的正向改动，**放宽截断**。 | **灵活**。告诉模型：“我知道没小抄很难，允许你参数改动大一点，赶紧把这个动作记住！” |


**Selective Boosting 就是给 PPO 的安全带装了一个“智能松紧扣”，在模型需要跨越式学习（内化经验）的时候，把带子松开，让它能跳过去。**

## 思考八

PPO里的概率具体是指什么？截断的奖励又是什么？

***

这是一个非常棒的基础问题，搞懂了这两个概念，你就真正摸到了 PPO（以及所有现代强化学习）的命门。

别被数学名词吓到，我们用\*\*“发奖金”\*\*的例子来通俗解释。

***

### 1. PPO 里的“概率”具体指什么？

这里的概率，指的是**模型在面对某个特定情况时，选择某个特定动作的信心（Confidence）**。

在 LLM（大模型）的语境下，它就是**生成某个 Token 的概率**。

#### 举个例子：

假设现在的输入（State）是：“**床前明月**”。 模型需要预测下一个字（Action）。

*   **旧模型（$P_{old}$）**：

    *   还没训练前的模型。

    *   它觉得下一个字是“光”的概率是 **20%**（ $0.2$ ）。

    *   它觉得是“亮”的概率是 10%。

    *   它觉得是“香”的概率是 5%。

    *   （实际上它选了“光”）。

*   **新模型（$P_{new}$）**：

    *   稍微更新了一点参数后的模型。

    *   它现在觉得下一个字是“光”的概率变成了 **25%**（ $0.25$ ）。

#### PPO 里的“概率比率（Ratio）”：

PPO 关心的不是 0.2 或 0.25 这个绝对值，而是**你变了多少**。 $\text{Ratio } r = \frac{P_{new} (\text{光})}{P_{old} (\text{光})} = \frac{0.25}{0.20} = 1.25$

*   **含义**：新模型比旧模型，对“光”这个字的选择倾向**增加了 25%**。

*   这就是 PPO 用来衡量\*\*“步子迈得有多大”\*\*的尺子。

***

### 2. “截断的奖励”又是什么？

首先要澄清一个关键误区：**被截断（Clip）的不是环境给的那个“原始奖励分”**（比如做对得 +10 分），而是\*\*“这次更新能给模型参数带来的【激励力度】”\*\*。

在学术上，这叫做**被截断的代理目标函数（Clipped Surrogate Objective）**。

我们继续用“发奖金”来比喻：

#### 场景设定：

*   **动作**：接上“光”这个字。

*   **优势（Advantage）**：这是一个好动作，优势是 **+10**（相当于满分 10 分）。

*   **目标**：因为是好动作，我们希望模型**提高**选“光”的概率。

#### 如果没有截断（Unclipped）：

激励公式大致是：**激励力度 = 比率 × 优势**。

*   如果模型把概率从 20% 提到 40%（比率 = 2.0）：

    *   激励力度 =  $2.0 \times 10 = 20$ 。

    *   模型心想：“哇！改动越大奖金越多，下次我直接把概率提到 99%！”

*   **后果**：模型会为了贪图这个激励，疯狂修改参数，导致下一次遇到别的情况直接傻掉（过拟合/策略坍塌）。

#### 有了截断（Clipped）：

PPO 规定：**激励力度是有上限的**。比如规定比率不能超过 1.2（即 20% 的浮动）。

*   **计算过程**：

    1.  **原本的激励**： $2.0 \times 10 = 20$  （想得美）。

    2.  **截断的比率**：系统看你比率是 2.0，超过了 1.2，所以强制按 **1.2** 算。

    3.  **截断后的激励**： $1.2 \times 10 = 12$ 。

*   **最终结果**： 系统取两者的最小值（Min）。所以最终给模型的激励是 **12**，而不是 20。

#### “截断奖励”的通俗含义：

就像老板对员工说：

> “你业绩好（优势高），我鼓励你进步（提高概率）。 你进步 10%，我给你发 1000 元奖金。 你进步 20%，我给你发 2000 元奖金。 **但是！** 如果你突然进步了 200%（比率过大），我怀疑你走了歪门邪道或者步子太急，**所以我最多只按进步 20% 给你发奖金（截断）**。”

***

### 3. 回到 AgentEvolver 的 Selective Boosting

结合上面两个概念，你就更明白 AgentEvolver 在干什么了。

1.  **普通 PPO 说**：“只要你概率变化超过 20%（比率 > 1.2），我就截断激励，不再多给你奖金了。”

2.  **AgentEvolver 的困境**：因为去掉了“经验小抄”，模型选正确动作的概率从 90% 掉到了 10%。这看起来像是“退步”，但其实我们是希望它在这种困难模式下也能选对。

3.  **Selective Boosting 说**：“老板（PPO），这个员工虽然看起来概率波动巨大（超出了 1.2 的限制），但他是在**尝试内化经验**（这是个好动作，优势 > 0）。**对于这种特殊情况，请把奖金上限打开！** 哪怕他变动了 500%，只要是对的，就给他 500% 的激励，让他赶紧记住！”

**总结：**

*   **概率** = 模型选这个字的信心。

*   **截断的奖励** = 为了防止模型瞎改参数，给“参数更新幅度”人为设定的**奖金封顶**。
