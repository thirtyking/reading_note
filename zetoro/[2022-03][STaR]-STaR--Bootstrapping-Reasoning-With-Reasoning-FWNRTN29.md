---
tags: []
parent: '[STaR] STaR: Bootstrapping Reasoning With Reasoning'
collections:
    - 自进化
$version: 2257
$libraryID: 1
$itemKey: FWNRTN29

---
\[2022-03]\[STaR] STaR: Bootstrapping Reasoning With Reasoning

# \[STaR] STaR: Bootstrapping Reasoning With Reasoning

## 概述

你好！很高兴能为你解读这篇在深度学习领域非常有影响力的论文——**《STaR: Bootstrapping Reasoning with Reasoning》**（通过推理引导推理）。

简单来说，这篇论文教会了 AI 如何通过“自我反省”和“不断练习”来变得更聪明，就像一个学生通过不断做题、纠错来提高数学成绩一样。

### 1. 本论文要解决什么问题？为什么重要？

**要解决的问题：** 大语言模型（LLM）虽然博学，但在处理复杂逻辑推理（比如复杂的数学题、逻辑谜题）时经常掉链子。它们习惯“脱口而出”答案，而不是先思考再回答。

**为什么重要：**

1.  **可靠性**：我们不仅希望 AI 给对答案，还希望它能给出正确的推理步骤（即 Chain of Thought，思维链）。

2.  **数据瓶颈**：要训练 AI 拥有这种推理能力，通常需要大量的“问题-推理步骤-答案”数据集。但高质量的推理步骤数据非常稀缺，让人类去写又太贵了。

***

### 2. 难点在哪里？之前的方案为什么不行？

**难点：** 这是一个“鸡生蛋，蛋生鸡”的问题。

*   你想让模型学会推理，你需要推理数据去训练它。
*   但如果你没有推理数据，你就得让模型自己生成。
*   可是模型如果不经过训练，它生成的推理步骤大多是错的。

**之前的方案：**

1.  **Few-shot Prompting（少量样本提示）**：给模型几个例子，让它模仿。*缺点*：模型能力上限被那几个例子锁死了，没法通过大量练习自我进化。

2.  **Fine-tuning（微调）**：用人类写好的推理过程训练模型。*缺点*：人类标注的数据量太小，覆盖面不够。

***

### 3. 本论文提出了何种解决方案？

作者提出了 **STaR**（Self-Taught Reasoner，自我教导的推理者）。

它的核心逻辑是一个**循环迭代过程**：

1.  **生成（Gen）**：给模型一堆问题，让它自己写推理过程并给出答案。

2.  **过滤（Filter）**：看模型给出的答案对不对。如果答案是对的，就把这个“推理过程+答案”存进“错题本”（训练集）。

3.  **微调（Fine-tune）**：用这个不断扩充的“错题本”去训练模型。

4.  **循环**：训练后的模型变强了，再让它去挑战之前不会做的题，不断重复。

**进阶版：Rationalization（合理化/追溯推理）** 如果模型怎么都做不出某道题怎么办？STaR 会把**正确答案**直接告诉模型，让模型“强行解释”：既然答案是 X，你能不能反推一下逻辑？如果模型反推成功了，这个逻辑也可以作为训练数据。

***

### 4. 方案的核心创新点在哪里？

1.  **自举（Bootstrapping）**：它证明了模型可以“左脚踩右脚”上天。只要模型具备一点点基础能力，它就能通过不断地“尝试-验证-学习”来实现自我进化，不需要额外的人类标注。

2.  **利用正确性作为反馈**：模型不需要知道推理过程每一步对不对，它只需要知道最后答案对不对。只要答案对了，大概率中间的推理也是有价值的。

3.  **加入“反向追溯”机制**：不仅仅学习自己本来就会的，还通过“已知答案求过程”的方式学习原本不会的题。

***

### 5. 本方案有什么局限性？

1.  **初始门槛（Cold Start）**：如果模型太弱，连一道题都做不对，那它就无法生成任何正确的训练数据，整个循环就跑不起来。

2.  **错误过拟合**：有时候模型可能通过“错误的逻辑”碰巧得到了“正确的答案”。如果大量学习这种“歪理”，模型会变得固执且错误。

3.  **多样性瓶颈**：模型生成的推理风格可能比较单一，不如人类写的逻辑那么灵活多样。

***

### 6. 关键流程图描述与理解

论文中有一张核心流程图，描绘了 STaR 的循环：

*   **左侧分支（Rationale Generation）**：

    *   输入问题  $\rightarrow$  模型生成思维链  $\rightarrow$  得到答案。

    *   **判断**：如果答案与真实答案一致，保留这一条。

*   **右侧分支（Rationalization）**：

    *   对于那些左侧分支失败的问题  $\rightarrow$  告诉模型正确答案  $\rightarrow$  模型尝试生成能推导出该答案的思维链。

    *   **判断**：如果反推成功，也保留这一条。

*   **底部（Fine-tuning）**：

    *   汇总所有保留下来的“推理+答案”数据，对模型进行一轮微调。

*   **箭头指向开始**：

    *   微调后的模型回到顶部，开始新一轮的生成。

**这张图的意义**：它清晰地展示了 STaR 是如何通过“自我筛选”和“反向学习”将海量的无标注问题转化为高质量训练数据的。

***

### 总结给学生的话：

STaR 就像是一个正在备考的学生：

*   它先自己做模拟题。
*   做对了的题，它记下思路（自我强化）。
*   做错了的题，它看一眼答案，恍然大悟：“哦！原来是这样推导的！”（追溯推理）。
*   经过一轮复习，它变得更厉害了，再去挑战更难的题。

这种\*\*“自我产生数据 $\rightarrow$ 自我学习”\*\*的模式，正是目前通往 AGI（通用人工智能）的重要路径之一。

## 思考一

本论文和 Recursive Introspection: Teaching Language Model Agents How to Self-Improve 论文中的方法有什么本质区别？

这是一个非常深入且敏锐的问题！这两篇论文都属于\*\*“自我进化”（Self-Improvement）\*\*这个大赛道，但它们在“如何进步”和“进步的目标”上有本质的区别。

如果把 **STaR** 比作一个\*\*“刷题型选手”**，那么 **Recursive Introspection（递归反省，简称 RISE）** 就像一个**“实战型特工”\*\*。

以下是它们的四个本质区别：

### 1. 核心目标的区别：更强的推理 vs 更强的纠错

*   **STaR (专注“想对”)**：它的目标是让模型在**一次尝试**中，通过更完美的思维链（CoT）直接推导出正确答案。它学习的是“正确的思考路径”。

*   **Recursive Introspection (专注“修对”)**：它的目标是让模型学会\*\*“自我察觉”和“迭代修复”\*\*。它不仅仅关注一次能否做对，更关注当第一步走错时，模型能不能通过“反省”发现错误，并在第二步、第三步把它改过来。

### 2. 学习机制的区别：刷“正确答案” vs 练“失败分析”

*   **STaR 的逻辑：过滤成功。**

    *   它只把模型**做对了**的例子（或者通过答案引导后反推成功的例子）加入训练集。

    *   它的逻辑是：只要我只学习正确的路径，我就能变得越来越强。

*   **RISE 的逻辑：递归反省。**

    *   它会记录模型**失败的尝试**。它让模型去对比“我刚才哪里做错了”和“我该怎么改”。

    *   它不仅训练模型做题，还专门训练模型如何执行“反省（Introspection）”这个动作。它是在训练一种\*\*“监控并修正自己行为”\*\*的高级能力。

### 3. 应用场景的区别：静态逻辑 vs 动态代理（Agent）

*   **STaR：** 主要针对的是**数学、逻辑推理**等有明确标准答案、且过程相对静态的任务。

*   **RISE：** 主要针对的是\*\*智能体（Agent）\*\*任务（比如操控软件、调用 API、多步骤的任务执行）。在这些任务中，环境是会变的，模型必须学会根据环境反馈不断调整自己的策略，而不是指望一劳永逸地写出完美代码。

### 4. 反馈循环的区别：离线微调 vs 在线递归

*   **STaR：** 是一个“生成-筛选-再训练”的**离线循环**。模型是在不断的“版本更新”中进步的。

*   **RISE：** 强调的是**递归（Recursive）**。它往往在单次任务中就包含多次反省。它通过一种类似强化学习（RL）的思想，让模型学会：如果当前的思考状态（State）不好，就通过反省动作（Action）迁移到一个更好的状态。

***

### 形象比喻：

*   **STaR (刷题家)**： 小明做数学题，做对了就背下这个解题步骤。做错了，看一眼答案，反推一下步骤，也背下来。小明的目标是：下次遇到类似的题，一眼就能写出标准答案。
*   **Recursive Introspection (特工训练)**： 特工潜入大楼。如果门锁住了（失败），他不会直接放弃，而是停下来**反省**：“刚才用撬棍不行，是不是该用电子干扰？”。他训练的是这种“发现不行 -> 分析原因 -> 换个法子”的**应变能力**。

### 总结对照表

| 特性       | STaR                     | Recursive Introspection (RISE)  |
| :------- | :----------------------- | :------------------------------ |
| **关键词**  | 引导 (Bootstrapping)       | 反省 (Introspection)              |
| **主要目标** | 提高单次推理的正确率               | 提高多轮交互中的自我纠错能力                  |
| **学习对象** | 成功的思维链 (Positive traces) | 从失败到成功的转化过程 (Correction traces) |
| **反馈来源** | 最终答案是否正确                 | 外部环境反馈 + 内部自我监控                 |
| **典型任务** | 数学、常识逻辑谜题                | 智能体操作、复杂 API 调用、长程任务            |


**一句话总结**： STaR 是教会 AI **“如何一次性想清楚”**；而 Recursive Introspection 是教会 AI **“如果发现自己想错了，该如何救回来”**。在通往 AGI 的路上，两者其实是互补的。
