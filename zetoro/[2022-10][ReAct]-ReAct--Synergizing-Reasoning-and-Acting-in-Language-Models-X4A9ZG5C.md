---
tags: []
parent: '[ReAct] ReAct: Synergizing Reasoning and Acting in Language Models'
collections:
    - 提示工程
$version: 2206
$libraryID: 1
$itemKey: X4A9ZG5C

---
\[2022-10]\[ReAct] ReAct: Synergizing Reasoning and Acting in Language Models

# \[ReAct] ReAct: Synergizing Reasoning and Acting in Language Models

你好！今天我们来解读 AI Agent（智能体）领域最核心的奠基之作之一：《ReAct: Synergizing Reasoning and Acting in Language Models》。

如果你对之前讲过的 Chain-of-Thought（思维链）还有印象，那么这篇论文就是它的\*\*“进化版”\*\*。它解决了让大模型真正“动手干活”时的关键痛点。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**核心问题：如何让大模型在解决复杂问题时，既能“动脑子思考”，又能“动手查资料”，并且让这两者互相配合？**

**背景：** 大模型（LLM）通常有两种极端状态：

1.  **只会空想（Reasoning only）：** 比如你问它最近的新闻，它可能因为数据过时而开始一本正经地胡说八道（幻觉）。它有很好的逻辑，但缺乏事实依据。
2.  **只会瞎干（Acting only）：** 比如早期的互联网机器人，你让它买票，它可能只会机械地点击按钮。如果网页报错了，它不知道该怎么办，因为它没有“思考”发生了什么。

**重要性：** 我们想要的 AI Agent，应该像一个人类专家。比如医生看病：

*   先**思考**（Reasoning）：病人发烧可能是感冒。
*   然后**行动**（Acting）：拿体温计测量。
*   观察结果（Observation）：39度。
*   再**思考**（Reasoning）：确实是高烧，需要开退烧药。

如果 AI 不能把“思考”和“行动”结合起来，它要么是“纸上谈兵”的赵括，要么是“有勇无谋”的莽夫。ReAct 就是为了解决这个问题，让 AI **知行合一**。

***

### 2. 解决这个问题的难点在哪里？之前的方案为何无效？

**难点：** 如何在模型的输出中，动态地交替进行“推理”和“行动”，并让行动的结果反过来修正推理？

**之前的解决方案及其局限：**

1.  **Chain-of-Thought (CoT，思维链)：**

    *   *做法：* 让模型在回答前先输出“Let's think step by step...”。
    *   *缺点：* **封闭系统，容易产生幻觉**。模型是在自己的“大脑”里闭门造车。如果它一开始就记错了事实（比如记错了美国总统是谁），后面推理得再逻辑严密，结果也是错的。因为它不与外界交互，无法更新信息。

2.  **Acting-only (纯行动模型)：**

    *   *做法：* 训练模型直接输出 API 调用指令（比如 `Search[iPhone 15]`）。
    *   *缺点：* **缺乏规划和反思**。模型只是根据当前的输入反射性地输出动作。如果搜索结果不理想，它不知道为什么，也不知道下一步该怎么调整策略，容易在复杂任务中迷失方向。

***

### 3. 本论文提出了何种解决方案？为什么能解决？

**解决方案：ReAct (Reason + Act)。**

这是一个非常巧妙的**提示工程（Prompting）** 框架。它要求模型在执行任务时，必须遵循一个循环模式： **Thought（思考） $\rightarrow$ Action（行动） $\rightarrow$ Observation（观察）**

**具体流程：**

1.  **Thought：** 模型先自言自语，分析当前的情况，制定下一步计划。
2.  **Action：** 基于刚才的思考，模型生成一个具体的动作（比如调用 Google Search）。
3.  **Observation：** 这个动作被执行后，外部环境（比如搜索引擎）返回的结果被喂回给模型。
4.  **Loop：** 模型看到结果后，再次进入 Thought 阶段，根据新信息更新自己的认知。

**为什么能解决问题？**

*   **对比 CoT：** ReAct 允许模型通过 Action 获取外部信息（Grounding，接地），**减少幻觉**。
*   **对比 Acting-only：** ReAct 强制模型在行动前先写出 Thought（理由），这赋予了模型**规划和纠错**的能力。如果搜索没搜到，模型会思考“哦，这个关键词不对，我得换一个”，而不是卡死。

***

### 4. 方案的核心创新点在哪里？

1.  **交错式生成（Interleaved Generation）：** 它是第一个明确提出将“推理轨迹”（Reasoning Traces）和“特定动作”（Action Execution）交替生成的框架。

2.  **双向增益：**

    *   **推理辅助行动：** 思考让行动更精准，避免盲目尝试。
    *   **行动辅助推理：** 行动带回来的新知识（Observation）能修正错误的推理，防止模型一条道走到黑。

3.  **可解释性（Interpretability）：** 以前的模型直接给结果或直接做动作，我们不知道它为什么这么做。ReAct 留下了完整的“思考日志”，我们可以清楚地看到 AI 是怎么一步步解决问题的，或者是在哪一步想歪了。

***

### 5. 关键架构图解读

论文中的 **Figure 1** 是理解 ReAct 的一把钥匙。这是一张对比图，展示了不同方法如何回答同一个复杂问题。

#### **图的内容描述：**

假设问题是：“Apple Remote 除了能控制 Apple TV，最初是设计用来和什么设备交互的？”

这张图横向对比了四种方法：

1.  **Standard（标准直接回答）：** 模型瞎猜了一个答案（错误的）。

2.  **CoT（思维链）：** 模型开始推理：“Apple Remote 是... 所以它能控制...”。但因为模型内部知识有误，它一本正经地推理出了一个错误的结论（幻觉）。

3.  **Act-only（纯行动）：** 模型直接搜索 `Search[Apple Remote]`，得到了一堆信息，但因为它没有“思考”过程，它迷失在了海量搜索结果中，不知道该提取哪条信息，最终回答错误。

4.  **ReAct（本文方法）：**

    *   **Thought 1:** “我需要先搜一下 Apple Remote 的历史。”
    *   **Action 1:** `Search[Apple Remote]`
    *   **Observation 1:** （返回了维基百科摘要，提到它最早是为 Front Row 软件设计的）。
    *   **Thought 2:** “哦，结果里提到了 Front Row，我得查查 Front Row 是干嘛的。”
    *   **Action 2:** `Search[Front Row]`
    *   **Observation 2:** （返回结果说 Front Row 允许用户控制 Mac 电脑上的媒体）。
    *   **Thought 3:** “原来如此，那就是能控制 Mac。”
    *   **Action 3:** `Finish[Mac]` （回答正确）。

#### **如何辅助理解方法论：**

这张图非常直观地点明了 ReAct 的精髓：

*   如果没有 **Action**（像 CoT），模型就会因知识不足而产生幻觉。
*   如果没有 **Thought**（像 Act-only），模型就会拿到信息后不知道如何处理，或者无法制定下一步搜索计划。
*   只有 **ReAct** 这种“想一步、做一步、看一步”的模式，才能像人类侦探一样抽丝剥茧，解决需要多步推理和外部知识的复杂问题。

***

### 总结一下给学生的话：

ReAct 这篇论文其实是告诉我们：**AI 不应该只是一个只会说话的“大脑”（LLM），也不应该只是一个没有灵魂的“机械手”（脚本）。**

通过 **ReAct**，我们给 AI 装上了“手”去感知世界（Action/Observation），同时保留了它的“脑”去规划未来（Thought）。这是目前所有构建 **Autonomous Agent（自主智能体）** 最基础、最通用的设计范式。后面你们看到的 AutoGPT、LangChain Agent，本质上都是在跑这个 ReAct 循环。
