---
tags: []
parent: '[SigLIP] Sigmoid Loss for Language Image Pre-Training'
collections:
    - classical
$version: 2279
$libraryID: 1
$itemKey: AUNE36J5

---
\[2023-03]\[SigLIP] Sigmoid Loss for Language Image Pre-Training

# \[SigLIP] Sigmoid Loss for Language Image Pre-Training

## 概述

你好！很高兴能为你解读这篇来自 Google DeepMind 的重量级论文：**《Sigmoid Loss for Language Image Pre-Training》（简称 SigLIP）**。

这篇论文在视觉-语言预训练领域引起了很大的轰动，因为它挑战了像 CLIP 这种主流模型的核心算法。

***

### 1. 本论文要解决什么问题？为什么重要？

**核心问题：如何更高效地训练像 CLIP 这样的“图文匹配”大模型。**

**为什么重要：** 现在的 AI 模型（比如 Stable Diffusion 的绘图，或者 GPT-4o 的识图）很多都依赖于一种叫“图文对比学习”的技术。其代表作是 OpenAI 的 CLIP。CLIP 的目标是让模型学会在海量的互联网图片和描述中，找出哪张图对应哪段话。 但是，CLIP 这种方法在\*\*扩大规模（Scaling）\*\*时非常吃资源。如果你想用更大的批次（Batch Size）来训练以提升性能，硬件开销和计算复杂度的增长会让你非常头疼。

### 2. 解决这个问题的难点在哪里？之前的方案有什么弊端？

**之前的方案：Softmax 对比损失函数（CLIP 的做法）。** CLIP 使用的是一种类似“多选题”的逻辑。对于一批数据中的某张图片，模型要从这一批次（Batch）里所有的文字中，选出唯一正确的那一个。

**难点/弊端：**

1.  **全局依赖（Global View）：** 在计算“多选题”的概率时，图片必须看遍这一批次里**所有的**文字。如果你为了效果好，把 Batch Size 设为 10 万，那么每张图都要计算和 10 万个文字的相似度。这会导致内存消耗巨大。

2.  **计算复杂与不稳定：** 这种“多选题”逻辑（Softmax）在数学实现上需要做减去最大值的操作来保证数值稳定，这需要对全量数据多跑一轮计算，增加了额外的开销。

3.  **分布式难题：** 当你在几百张显卡上训练时，每张显卡都要把自己的数据发给其他所有卡（All-gather 操作），通信压力极大。

### 3. 本论文提出了何种解决方案？

**提出的方案：SigLIP（基于 Sigmoid 的图文预训练）。**

论文作者提出了一个极其简单的思路：把原来的“多选题”变成\*\*“判断题（True/False）”\*\*。

**为什么能解决问题：** 在 SigLIP 中，不再要求图片从几万个选项中选一个，而是把图片和文字组合成“对”。

*   如果是配套的图文，模型就判为 `1`（正样本）。

*   如果不配套（比如图片 A 配了图片 B 的文字），模型就判为 `0`（负样本）。

**这样做的好处：**

*   **解耦（Decoupling）：** 既然是判断题，每一对图文的计算就是**独立**的。我不需要知道这一批里还有哪些文字，就可以开始计算。

*   **内存极省：** 不需要像原来那样为了归一化而存储全局信息。

### 4. 方案的核心创新点在哪里？

1.  **从 Softmax 转向 Sigmoid：** 这是最核心的改变。它将对比学习从“竞争关系”变成了“独立二分类关系”。

2.  **引入可学习的偏置（Bias）和温度系数：** 既然是做判断题，正负样本的比例通常极度不均衡（正样本 1 个，负样本可能有几万个）。作者通过在公式里加入两个可以自动学习的参数（ $b$  和  $t$ ），让模型自己去调整判断的“门槛”，完美解决了样本不平衡导致的训练初期不稳定问题。

3.  **高效的“分块”实现（Chunked Implementation）：** 因为每对图文是独立的，作者设计了一种像拼图一样的分块计算方式（见下文图解），避免了昂贵的全局通信。

### 5. 本方案有什么局限性？

*   **极大规模下的收益递减：** 论文发现，虽然 SigLIP 在 Batch Size 小的时候比 CLIP 强很多，但当 Batch Size 增加到 32k（三万二）以上时，性能提升就变得非常缓慢了。

*   **超参数敏感：** 偏置项  $b$  和温度系数  $t$  的初始值设置对训练非常关键。如果初始值设不好，模型可能一开始就学跑偏了（虽然论文里给出了最佳推荐值）。

*   **对负样本挖掘的依赖：** 因为它是独立的二分类，如果负样本选得太简单，模型可能会变得“懒惰”，学不到细微的区别（不过论文实验证明，只要 Batch Size 够大，这个问题并不明显）。

***

### 6. 关键架构图/流程图解读（Figure 1）

在 PDF 的第 3 页有一个 **Figure 1**（如图所示），它展示了如何通过“分块交换”来实现高效计算：

1.  **图 (a) & (b)：** 假设我们有 3 台设备（卡）。每台卡各拿 4 张图和 4 段文字。它们首先在本地计算自己的图文匹配情况。

2.  **图 (c) & (d)：** 关键点来了！这些设备**不需要**把所有的文字都广播给所有人。相反，它们像传球一样，把文字块在设备之间轮转。

    *   Device 1 把它的文字传给 Device 2，Device 2 传给 Device 3，以此类推。
    *   通过几轮轮转，每一张图都能和所有文字“见一面”，并完成“判断题”计算。

3.  **核心意义：** 这种方式在内存中只需要占用一小块空间（黄色小方块），而不需要像 CLIP 那样变出一个巨大的矩阵。这正是 SigLIP 能够用很少的机器训练出强大模型的秘密武器。

### 总结给学生的话：

你可以把 **CLIP** 想象成一个在考场里做**单项选择题**的学生，他必须看完所有选项才能下笔；而 **SigLIP** 就像是在做**对错判断题**，他看到一对图文就打一个勾或叉。结果证明，做判断题不仅让考试速度变快了（训练更高效），而且最后考出来的成绩（模型效果）甚至比做选择题的还要好！

## 思考一

1.  "当你在几百张显卡上训练时，每张显卡都要把自己的数据发给其他所有卡（All-gather 操作），通信压力极大。" 为什么要做all-gather操作？是否是因为采用了数据并行的训练方式？
2.  展开解释一下引入可学习的偏置（Bias）和温度系数是如何解决样本不平衡导致的训练初期不稳定问题的？

***

这两个问题问得非常深入，正触及了这篇论文最核心的两个技术创新点。我们逐一拆解：

### 1. 为什么 CLIP 要做 All-gather 操作？是数据并行导致的吗？

**结论是：是的，它是“数据并行”与“Softmax 损失函数特性”共同作用的结果。**

#### 为什么要 All-gather？

在 CLIP 的 Softmax 逻辑中，计算损失函数需要一个**全局视野**。 假设我们总共有 1024 张图和 1024 段文字，分布在 4 张显卡上（每张卡有 256 个样本）：

*   **数据并行的现状：** GPU 0 只有图  $1-256$  和文字  $1-256$ 。

*   **Softmax 的要求：** 为了计算图 1 的损失，模型必须知道图 1 与**全量** 1024 段文字的相似度，因为 Softmax 的分母是 `exp(图1·文字1) + exp(图1·文字2) + ... + exp(图1·文字1024)`。

*   **矛盾点：** GPU 0 根本没有文字  $257-1024$  的数据。

为了解决这个问题，在计算 Loss 之前，所有显卡必须执行一次 `All-gather`：每张卡都把自己的文字向量发给其他所有人，同时也接收别人的。最终，**每张显卡手里都拥有一份完整的全局文字向量表**。

#### 通信压力大在哪里？

随着 Batch Size 增加（比如增加到 10 万、甚至百万级），这个 `All-gather` 传输的数据量会呈**几何级数增加**，网络带宽会成为训练的瓶颈，导致显卡大部分时间在“等数据”而不是“算模型”。

**SigLIP 的不同：** 因为 SigLIP 把问题变成了“判断题”（一对一），计算图 1 和文字 500 是否匹配时，**只需要这两个向量**，不需要知道文字 1 到 1000 是什么。这就给“分块计算”留下了空间，可以像传声筒一样轮流交换数据，而不需要一次性把所有数据挤占在网络带宽里。

***

### 2. 可学习的偏置（Bias）和温度系数如何解决“样本不平衡”？

这是 SigLIP 能够跑通的关键。我们先看**样本不平衡**有多严重：

如果 Batch Size 是 32,768（32k），那么在一个 Batch 里：

*   **正样本（图文匹配）：** 32,768 对。

*   **负样本（图文不匹配）：**  $32,768 \times 32,768 - 32,768 \approx 10$  亿对。

*   **比例：** 正负比例大约是 **1 : 32,000**。

#### 初始化的“灾难”

在训练刚开始时，模型是乱猜的，相似度得分接近 0。

*   对于这 10 亿对负样本，模型会发现：“哎呀，我预测得不对，它们相似度怎么不是 0？”
*   由于负样本实在太多了，成千上万个负样本产生的梯度（纠错力量）会瞬间汇聚成一股巨大的洪流。
*   这股洪流会直接把模型“冲垮”，导致模型在训练初期极度不稳定，甚至参数直接变成无效的 NaN。

#### SigLIP 的药方：$l = (sim \cdot t) + b$

作者引入了两个可学习的参数：\*\*$t$（温度/缩放系数）\*\*和 **$b$（偏置项）**。

1.  **偏置项 $b$ 的妙用（解决不平衡）：**

    *   作者把  $b$  初始设置为  $-10$ 。

    *   根据 Sigmoid 函数的特性， $\sigma(-10)$  是一个非常接近  $0$  的数。

    *   这意味着：**模型在还没开始学习时，就先默认“所有图文对都是不匹配的（负样本）”。**

    *   由于这个先验知识（Prior）和现实中“负样本占绝大多数”的情况完全吻合，那 10 亿个负样本产生的梯度就变得非常微弱了（因为模型已经默认它们是负的了）。这就好比给模型装了一个“减震器”，挡住了初期的梯度洪流。

2.  **温度系数 $t$ 的妙用（解决区分度）：**

    *   作者把  $t$  设置为可学习的参数（初始值约等于 10）。

    *   $t$  的作用是放大相似度得分之间的微小差异。如果两个向量只是有点像，乘以  $t$  之后，得分会迅速推向 Sigmoid 函数的高激活区。

    *   这让模型能够更敏锐地捕捉到“到底哪个才是真正的正样本”，提高对比的精度。

**通俗对比：**

*   **没有$b$时：** 老师给学生 3 万道题，其中只有 1 道是对的，剩下的全是错的，但不告诉学生规律。学生一上来被错题淹没了，直接崩溃。

*   **有了$b=-10$时：** 老师先告诉学生：“这卷子里 99.9% 都是错的。” 学生心里有了底，直接把大部分题都填了“错”，心情很平静。他只需要花精力去找那道唯一的“对”题就行了。训练自然就稳定了。

## 思考二

CLIP 的 All-gather 是在每一轮的训练中通讯一次，还是每一层的训练中通讯一次？

***

这是一个非常专业且切中要害的问题。为了让你透彻理解，我们需要区分**两种不同的通信操作**：

直接回答你的结论：**CLIP 这种特有的、为了构建全局相似度矩阵而进行的 All-gather，是在每一轮（Step/Batch）的“前向传播结束后、计算 Loss 前”执行一次，而不是每一层执行一次。**

以下是详细的拆解对比：

### 1. 为什么是“每一轮”而不是“每一层”？

在 CLIP 的训练流程中：

1.  **独立计算（前向传播）：** 图片经过视觉编码器（比如 12 层的 ViT），文字经过文本编码器（比如 12 层的 Transformer）。在这个过程中，各张显卡是“各扫门前雪”，每张卡只管算自己分到的那 256 张图和词。**层与层之间不需要交换数据。**

2.  **提取特征：** 只有当图片和文字走完了**所有层**，拿到了最终的“特征向量（Embedding）”后，我们才需要把这些向量聚在一起。

3.  **构建大矩阵（All-gather）：** 此时，所有显卡进行 **一次 All-gather**，把各自算好的特征向量交换一下，拼成一个巨大的全局矩阵。

4.  **计算 Loss：** 算出 Loss 后开始反向传播。

**所以，这个特有的通信动作每轮只发生一次。**

***

### 2. 容易混淆的概念：梯度的 All-reduce

你可能会在某些地方听说“通信是逐层进行的”，那通常指的不是 CLIP 特有的 All-gather，而是深度学习中**数据并行（DDP）通用的“梯度同步（All-reduce）”**。

*   **梯度同步（All-reduce）：** 在**反向传播**阶段，为了保证所有显卡的模型参数更新是一致的，需要同步梯度。现代框架（如 PyTorch DDP）为了提高效率，会在反向传播计算每一层的梯度时，就立刻异步地开始同步这一层的梯度。这确实是“逐层（或按 Block）”发生的。

*   **特征同步（All-gather）：** 这是 CLIP、SigLIP 这种对比学习**特有**的。它同步的是“数据特征”而不是“参数梯度”。它只发生在 Loss 计算的那一刻。

***

### 3. 既然每轮只通讯一次，为什么通信压力还大？

你可能会想：一轮才通讯一次，这压力能有多大？

问题的核心在于**数据量（Bandwidth）**：

*   **普通模型：** 只需要同步梯度。

*   **CLIP 类模型：** 既要同步梯度（All-reduce），还要在每一轮额外传输巨大的特征矩阵（All-gather）。

*   **规模效应：** 假设你的 Batch Size 是 100 万，特征维度是 512。每轮 All-gather 都要在所有显卡之间交换几 GB 甚至更多的数据。

*   **阻塞效应：** 更糟糕的是，Loss 计算必须等 All-gather 彻底完成后才能开始。如果网络慢一点，几百张显卡就会集体“熄火”等着数据传输完成，这大大降低了硬件的利用率（MFU）。

### 总结

*   **CLIP 的 All-gather：** 每一轮（Step）一次。目的是为了让每一张显卡都能看到全局的“多选题”选项。

*   **SigLIP 的改进：** 它通过“分块循环”的方式，把这一次沉重的、阻塞性的全局通讯，拆分成了多个小的、可以和计算重叠的局部通讯。这就是为什么 SigLIP 能把 Batch Size 推向百万级而机器不卡顿的原因。
