---
tags: []
parent: '[ROME] Locating and Editing Factual Associations in GPT'
collections:
    - '2. 参数记忆'
$version: 2259
$libraryID: 1
$itemKey: F2CIDU62

---
\[2022-02]\[ROME] Locating and Editing Factual Associations in GPT

# \[ROME] Locating and Editing Factual Associations in GPT

## 概述

你好！很高兴能为你解读这篇在深度学习领域非常有影响力的论文《Locating and Editing Factual Associations in GPT》（通常被称为 **ROME** 算法）。

这篇论文通俗点说，就是尝试给大模型做一次“微创手术”：**精准找到大脑里的一条错误记忆，并把它修过来，且不影响其他记忆。**

### 1. 本论文要解决什么问题？为什么很重要？

**要解决的问题：** 大语言模型（如 GPT）在训练过程中吸收了海量知识，但它会面临两个大麻烦：

1.  **知识过时：** 比如模型记得“现任美国总统是特朗普”，但现在换届了。

2.  **事实错误（幻觉）：** 模型可能会言之凿凿地说出一些错误事实。

**为什么重要：** 传统的做法是“重新训练”或者“全量微调”，但这就像为了修好家里一个坏了的水龙头而把整个房子拆了重建，既费钱又费时。如果能像手术一样，直接定位并修改那条特定的知识，模型就能保持准时、准确，且成本极低。

***

### 2. 解决这个问题的难点在哪里？之前有哪些方案？为什么不行？

**难点：**

*   **黑盒困境：** 大模型有几百亿个参数，知识是散落在这些参数里的。我们不知道“埃菲尔铁塔在巴黎”这句话具体存在哪个神经元里。

*   **副作用（灾难性遗忘）：** 如果你通过暴力训练让它记住新事实，它往往会忘记旧知识，或者变得“智障”，连话都不会说了。

**之前的方案及局限：**

*   **微调（Fine-tuning）：** 针对新事实训练。缺点是容易过拟合，且会破坏模型的其他能力。

*   **外挂知识库（Prompting）：** 在提问时把正确答案喂给它。缺点是治标不治本，模型本身的“大脑”还是错的，且每次都要输入长长的背景资料，浪费空间。

*   **知识神经元（Knowledge Neurons）：** 尝试找某个神经元。缺点是找得不够准，修改效果不持久。

***

### 3. 本论文提出了何种解决方案？

这篇论文提出了一个叫 **ROME (Rank-One Model Editing)** 的方法，分为两步走：

**第一步：定位（Causal Tracing，因果追踪）** 作者发明了一种“干扰实验”。他们先给模型输入一个事实，比如“伊隆·马斯克是 SpaceX 的创始人”。

1.  他们先把输入的信息“弄乱”（加噪声），让模型变糊涂。

2.  然后逐个恢复模型内部神经元的激活值，看恢复哪一个部分时，模型突然“想起来”正确答案了。

3.  **结论：** 他们惊奇地发现，事实知识主要存储在模型的 **MLP（多层感知机）** 层中，尤其是中间层的 MLP。

**第二步：修改（Rank-One Update，秩一更新）** 既然找到了位置（特定的 MLP 层），作者把 MLP 想象成一个\*\*“键值对数据库”\*\*（类似 Excel 表格）：

*   **键（Key）：** 主体对象（如“埃菲尔铁塔”）。

*   **值（Value）：** 相关事实（如“在巴黎”）。作者通过数学手段（秩一更新），在不破坏整个矩阵的情况下，精准地修改这一个“键值对”，把“在巴黎”改成“在罗马”。

***

### 4. 方案的核心创新点在哪里？

1.  **发现了知识的“藏身之处”：** 论文通过严谨的实验证明了，GPT 的**中层 MLP 层**是存储事实关联的核心区域。注意力机制（Attention）更多是起搬运作用，而 MLP 才是知识仓库。

2.  **精密的数学更新：** 传统的修改是乱动参数，ROME 是利用数学上的线性代数技巧，只针对一个特定的方向进行微小改动（Rank-One），保证了对其他知识的影响降到最低。

***

### 5. 本方案有什么局限性？与之前的方案相比哪些地方存在劣势？

**局限性：**

1.  **一次只能改一个：** ROME 擅长精准修改一条知识。如果你想一次性修改一万条知识，ROME 的效率和稳定性就会下降（后来作者又发了改进版叫 MEMIT 来解决这个问题）。

2.  **依赖定位的准确性：** 如果因果追踪找错了层，修改就会失败。

3.  **副作用仍存：** 虽然比以前好，但有时候修改一个事实会导致相关的连锁反应（比如改了“现任总统”，但模型在描述该总统的生平时可能还会跳回旧的信息）。

***

### 6. 关键架构图/流程图解读

论文中最核心的一张图是 **Causal Tracing（因果追踪图）**：

*   **横轴：** 句子的输入过程（Token 序列，比如：Steve, Jobs, is, the, founder, of...）。

*   **纵轴：** 模型的层数（从第 1 层到第 48 层）。

*   **热力图颜色：** 颜色越红，代表这个位置的神经元对“记起这个事实”越重要。

**如何辅助理解：** 你会看到图中有一块明显的**红色区域**，集中在句子中\*\*“主体词”（如 Steve Jobs）结束时的中间层 MLP\*\*。 这图就像一张 X 光片，直接告诉学生：看，模型在处理到“Steve Jobs”这个词并到达第 20 层左右时，它的“知识仓库”就被打开了。这为后面的精准手术提供了导航图。

***

**总结：** ROME 论文告诉我们，大模型不是不可理解的浆糊，它的知识是有规律地存在某些特定地方的。通过“因果分析”找准位置，再用“数学手术”进行微改，我们就能在不毁掉模型的前提下，修正它的错误。

## 思考一

1.  "他们先把输入的信息弄乱（加噪声），让模型变糊涂。然后逐个恢复模型内部神经元的激活值"，请解释，什么叫把输入的信息弄乱？输入信息弄乱为什么模型为变糊涂？模型糊涂和激活值有什么关系？激活值的修改不是应该在训练阶段吗？
2.  如何逐个恢复激活值？每恢复一个激活值就要跑一下测试吗？效率是不是太低了？
3.  发现知识存储的位置后，如何能把知识准备修改成我们需要的内容？请进一步解释一下过程和原理。

你的问题非常犀利，直接切中了这篇论文最核心的**工程实现**和**科学逻辑**。

为了让你听懂，我们把 GPT 想象成一个正在考试的学生，而研究员就像是在他大脑里插满电极的科学家。

***

### 问题 1：关于“弄乱信息”和“激活值”的疑惑

#### 什么是“把输入信息弄乱”？

在大模型里，每个词（比如“埃菲尔铁塔”）都会被转换成一串数字，叫 **Embedding（嵌入向量）**。

*   **弄乱（加噪声）**：就是在这些数字里随机加上一些乱码。

*   **具体做法**：研究员只针对“主体”（比如“埃菲尔铁塔”）的向量加噪声，而句子的其他部分（比如“位于哪个城市？”）保持不变。

#### 为什么模型会变糊涂？

模型预测答案是靠向量之间的精确计算。当你把“埃菲尔铁塔”变成了“一堆乱码”，模型在处理到最后时，它的“大脑”里就没有了关于这个塔的任何信息，它就无法预测出“巴黎”，预测概率会瞬间掉到零。这证明了：**如果你破坏了主体的输入，模型的知识提取过程就被阻断了。**

#### 模型糊涂和激活值有什么关系？

*   **激活值（Activation）**：是模型在处理信息时，每一个神经元产生的中间结果。

*   当模型糊涂时，它内部所有的中间激活值都是“错误的”或“无意义的”。

#### 激活值的修改不是应该在训练阶段吗？

这是理解本论文的关键：

*   **训练阶段**：是永久性地改变大脑的“连线”（权重参数  $W$ ）。

*   **推理阶段（即本次实验）**：并不改变参数，而是**人工强行干预**。科学家在模型运行的一瞬间，直接伸手拨动某个神经元的信号，看模型会有什么反应。这是一种**诊断手段**，就像医生给病人做电刺激实验，看刺激哪个部位病人会抬腿。

***

### 问题 2：如何逐个恢复？效率是不是太低了？

#### 怎么恢复？

这是一个叫\*\*“因果追踪”（Causal Tracing）\*\*的精妙设计：

1.  **第一步（干净运行）**：正常输入“埃菲尔铁塔在哪？”，记录下所有神经元的正确激活值（备份）。

2.  **第二步（污染运行）**：输入加了噪声的“乱码铁塔”，模型现在是糊涂的。

3.  **第三步（人工修复）**：在“污染运行”的过程中，科学家强行把某一层、某个词对应的神经元激活值，替换成第一步备份里的“正确值”。

#### 效率是不是太低了？

**是的，非常低！** 科学家需要遍历每一层（比如 48 层）和每一个词（比如 10 个词），要做几百次推理。

**但是（划重点）：**

*   这个“低效率”的过程**不是为了改模型**，而是为了**做地图**。

*   一旦科学家通过这种笨办法发现：“哦！原来不管我怎么弄乱，只要我修复第 20 到 25 层的 MLP，模型就能起死回生！”

*   那么，这个“地图”就画好了。以后我们要修改其他知识，就直接去改 20-25 层就行了，不需要再重新做一遍这个低效的实验。

***

### 问题 3：如何把知识修改成我们需要的内容？

发现位置后，就进入了手术阶段。这里用到了一个核心理论：**MLP 层是一个“键值对（Key-Value）存储器”。**

#### 原理：

你可以把 MLP 的参数矩阵 $W$ 想象成一个**巨大的 Excel 表格**：

*   **Key (键)**：输入特征（比如“埃菲尔铁塔”的特征）。

*   **Value (值)**：输出事实（比如“在巴黎”的特征）。

*   当模型看到“埃菲尔铁塔”时，它会在表格里检索，匹配到“在巴黎”。

#### 修改过程（ROME 算法）：

如果我们想把“在巴黎”改成“在罗马”，步骤如下：

1.  **确定新目标**：我们要让模型在输入“埃菲尔铁塔”的 Key 时，输出“在罗马”对应的那个 Value。

2.  **计算微调量**：利用线性代数里的\*\*“秩一更新”（Rank-One Update）\*\*。这个数学技巧的神奇之处在于：它能精准地计算出  $W$  矩阵应该如何微调，才能让新的 Key 正好指向新的 Value，而对表格里的其他行（比如“长城在哪个城市”）几乎没有改动。

3.  **直接修改权重**：直接把这个微调量加到模型的参数  $W$  上。

**通俗类比：** 这不像传统的“背诵式学习”（全量微调），而是像\*\*“黑客改数据库”\*\*。你不需要重新训练整个模型，你只需要知道那条数据在数据库的哪一行、哪一列，然后用一行代码把 `Paris` 改成 `Rome`。

#### 为什么能成功？

因为 ROME 算法不仅考虑了“要把这一个改对”，还通过数学约束，确保了这种修改**只在“埃菲尔铁塔”这个特定方向上起作用**。这就像是一场微创手术，只切除病灶，不伤及周围组织。

## 思考二

1.  "以后我们要修改其他知识，就直接去改 20-25 层就行了，不需要再重新做一遍这个低效的实验。" 为什么修改其他知识也是修改同一个地方？难道所有的知识都在一个地方吗？如果不在一个地方，修改其他知识时不是还得做一遍实验？
2.  通过公式推导，解释一下ROME 算法

***

好的，你的观察非常敏锐。这两个问题直指 ROME 论文的**普适性**和**数学本质**。我们逐一拆解。

### 问题 1：为什么修改其他知识也是在“同一个地方”？

你说得对，并不是“所有知识都挤在同一个神经元里”，但它们确实都遵循相同的**存储模式**和**分布规律**。

*   **“位置”的两个维度：** 当我们说“位置”时，其实有两个含义：一个是**层数（Layer）**，一个是**Token 位置（Token Index）**。

    *   **层数是普适的：** 论文通过对成千上万个事实进行实验发现，对于 GPT 这种架构，事实性知识总是倾向于在**中间层**（比如 48 层模型里的第 18-25 层）被提取。这是由模型架构决定的“功能分区”，就像人类的大脑，虽然具体的记忆点不同，但语言功能区、视觉功能区的位置对大部分人来说是大致固定的。

    *   **Token 是具体的：** 知识提取总是发生在\*\*“主体词（Subject）”的最后一个词\*\*。比如“埃菲尔铁塔（Eiffel Tower）”的“Tower”这个位置。

*   **不需要每次都做实验：** 正因为研究员发现绝大多数事实都躲在“中间层 MLP”这个大仓库里，所以我们在修改新知识时，可以直接跳过低效率的“因果追踪”实验，直接默认去改那几层就行。这大大提高了修改效率。

*   **就像改 Excel 表：** 你可以把这几层 MLP 想象成一张巨大的 Excel 表格。虽然每一行存的内容不同（一行存铁塔，一行存姚明），但它们**都在同一个文件里**。我们只要锁定这个文件（中间层），剩下的就是找到正确的行（通过输入对应的 Key）去修改它。

***

### 问题 2：ROME 算法的公式推导与原理

ROME 的核心是将 MLP 层看作一个**线性关联存储器（Linear Associative Memory）**。

#### 1. 基本设定

假设我们要修改的 MLP 权重矩阵是 $W$。

*   **Key ($k$)**：是该层对主体词（如“埃菲尔铁塔”）的输入向量。

*   **Value ($v$)**：是该层应该输出的向量（代表“在巴黎”或新的“在罗马”）。

*   原本模型的逻辑是： $W k \approx v_{old}$ 。

*   我们的目标是：求一个新的  $W_{new}$ ，使得  $W_{new} k^* = v^*$ （其中  $k^*$  是主体的特征， $v^*$  是我们想要的新事实）。

#### 2. 优化目标（核心思想）

我们不能随便改 $W$，否则模型其他知识就全乱了。所以我们要满足两个条件：

1.  **精确性**： $W_{new} k^* = v^*$  （新知识必须改对）。

2.  **最小干扰**：对于所有其他的输入  $k$ ，新旧矩阵的输出要尽可能一致。即最小化  $||W_{new} K - W K||^2$ ，这里的  $K$  代表模型平时处理的所有正常数据。

#### 3. 数学推导

为了最小化对其他数据的影响，我们需要考虑数据的分布。设 $C = E[kk^T]$ 为输入向量的**二阶矩矩阵**（可以理解为该层平时看到的各种数据的“统计特征分布”）。

我们要解决的是这样一个约束优化问题： $\min_{W_{new}} \sum_{k \in \text{all data}} \|W_{new} k - W k\|^2 \quad \text{subject to } W_{new} k^* = v^*$

利用**拉格朗日乘子法**，可以推导出 $W$ 的更新公式： $\Delta W = W_{new} - W = (v^* - W k^*) \frac{(C^{-1} k^*)^T}{k^{*T} C^{-1} k^*}$

#### 4. 公式通俗解读

这个公式非常有意思，它由三部分组成：

1.  **误差项$(v^* - W k^*)$**：这代表了“现状”与“理想”之间的差距。我们缺多少，补多少。

2.  **方向项$(C^{-1} k^*)^T$**：这是最精妙的地方。 $C^{-1}$  是输入分布的逆矩阵。乘以它意味着：**我们要把修改的能量，投射到那些平时最不常用的、最不容易干扰其他知识的方向上去。**

3.  **归一化项$k^{*T} C^{-1} k^*$**：这只是一个缩放因子，确保新旧知识能完美对接。

#### 为什么叫“秩一更新”（Rank-One Update）？

在矩阵理论中，如果一个矩阵是由两个向量相乘得到的（即 $\vec{u} \vec{v}^T$），这个矩阵的**秩就是 1**。 你会发现上面的 $\Delta W$ 正是一个向量（误差）乘以另一个向量（投影方向）。这意味着我们**只在矩阵的一个维度上做了修改**，而保持了矩阵其他几千个维度的完整性。

### 总结

*   **定位**：通过实验发现知识都在“中产阶级”（中间层）的“末尾词”位置。

*   **修改**：利用线性代数，计算出一个“精准手术刀”  $\Delta W$ 。这个手术刀只对准特定主体，且通过  $C^{-1}$  巧妙地避开了对其他神经元记忆的误伤。
