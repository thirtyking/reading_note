---
tags: []
parent: '[Speculative Decoding] Fast Inference from Transformers via Speculative Decoding'
collections:
    - 推理加速
$version: 2245
$libraryID: 1
$itemKey: JUZB2P9J

---
\[2022-11]\[Speculative Decoding] Fast Inference from Transformers via Speculative Decoding

# \[Speculative Decoding] Fast Inference from Transformers via Speculative Decoding

## 概述

你好，快请坐。听说你最近在看《Fast Inference from Transformers via Speculative Decoding》这篇论文？

这是一个非常经典且实用的工作，特别是在现在大模型（LLM）落地应用时，它几乎成了加速推理的标准配置之一。你可能觉得里面的一些数学公式（比如拒绝采样）有点绕，没关系，我们今天不谈复杂的公式推导，主要把它的**核心逻辑**和**设计哲学**理清楚。

我们就按照你关心的几个方面，像剥洋葱一样把这篇论文拆解开来。

***

### 1. 这篇论文要解决什么问题？为什么它很重要？

**一句话总结：如何在不牺牲模型预测质量的前提下，大幅提升大模型的生成速度？**

**背景：** 你知道 Transformer 模型（比如 GPT 系列）在生成文本时是**自回归**（Autoregressive）的。这意味着它像一个强迫症患者，必须写完第一个字，才能写第二个字，再根据前两个字写第三个字……以此类推。

**问题所在：** 对于巨大的模型（比如 GPT-4 或 Llama-70B），这种“一个字一个字蹦”的方式**太慢了**。 这就好比你想让一个绝世高人（大模型）写文章，但他每写一个字，都要把笔放下，喝口茶，深呼吸，思考人生，然后再拿起笔写下一个字。

*   **用户体验差：** 聊天机器人半天吐不出一句话。

*   **成本高：** GPU 显卡一直被占用，电费和算力成本都在燃烧。

所以，解决这个问题对于让大模型真正“好用”且“便宜”至关重要。

### 2. 解决这个问题的难点在哪里？之前的方案有什么问题？

**难点：内存带宽瓶颈（Memory Bandwidth Bound）。**

这里我得给你解释一个关键概念。现在的 GPU（显卡）计算能力非常强（算得快），但内存传输速度相对较慢（搬运数据慢）。

*   **大模型的困境：** 每次生成一个 token（词元），GPU 都必须把几十上百 GB 的模型参数从显存里搬运到计算核心里走一遍。

*   **算力浪费：** 哪怕只生成 1 个词，也要搬运整个模型。这就像是为了运送这一颗“芝麻”（1个token），你要开着一辆载重 100 吨的“大卡车”（模型参数）跑一趟。卡车的引擎（计算核心）根本没跑满，大部分时间都浪费在装卸货（读取内存）上了。

**之前的解决方案及缺陷：**

1.  **模型蒸馏（Distillation）：** 把大模型教给一个小模型，用小模型跑。

    *   *缺陷：* 小模型变笨了，生成质量下降，写不出大模型的水平。

2.  **量化（Quantization）：** 把模型的参数精度降低（比如从 float16 变成 int8）。

    *   *缺陷：* 还是会有精度损失，而且有时候加速效果有限。

**核心痛点：** 之前的方法大多是\*\*“用质量换速度”\*\*。但这篇论文想要的是：**速度我要快，质量我也一点都不能少（Lossless）。**

### 3. 本论文提出了何种解决方案？方案为什么能解决这个问题？

**解决方案：投机采样（Speculative Decoding）。**

为了让你理解，我打个比方：**“教授与实习生”**。

*   **大模型（Target Model）** = **教授**：知识渊博，但动作慢，身价高。

*   **小模型（Draft Model）** = **实习生**：知识一般，但动作极快，身价低。

**传统模式：** 所有字都由教授亲自写，慢死人。 **投机采样模式：**

1.  **实习生先猜（Drafting）：** 让实习生一口气快速草拟出接下来可能的  $K$  个词（比如 5 个词）。因为实习生模型很小，跑这 5 个词的时间，教授可能连 1 个词还没想好。

2.  **教授批改（Verification）：** 教授把这 5 个词拿过来，**并行地**（一次性）检查一遍。

    *   注意：对于 Transformer 来说，检查 1 个词和检查 5 个词，花的时间几乎是一样的（因为卡车反正都要跑一趟，多带几颗芝麻根本不费油）。

3.  **结果判定：**

    *   如果实习生猜对了（教授觉得这词写得好）：直接保留！我们就赚到了时间。
    *   如果实习生猜错了（教授觉得不行）：从错误的地方截断，教授亲自修正，并重新开始下一轮。

**为什么能解决问题？** 它把原本串行的“一个一个生成”，变成了并行的“批量验证”。 只要实习生猜对的概率足够高，教授每次“出车”都能确认好几个词，而不是只确认一个。这样就大大利用了 GPU 闲置的计算能力，突破了内存带宽的瓶颈。

### 4. 方案的核心创新点在哪里？

这篇论文最妙的地方主要有两点：

1.  **并行验证（Parallel Verification）：** 它利用了 Transformer 结构的一个特性——虽然生成必须串行，但计算概率（也就是打分）是可以并行的。它把算力瓶颈转化为了算力优势。

2.  **数学上的无损保证（Lossless Guarantee）：** 这是最最核心的！

    *   作者设计了一套巧妙的**拒绝采样（Rejection Sampling）** 算法。

    *   简单说，它不仅仅是看实习生猜的词对不对，而是通过比较两个模型的概率分布，决定是否接受。

    *   **结果：** 即使实习生很笨，经常猜错，这套算法也能保证最终输出的文本分布，和**教授单独写出来的完全一模一样**。如果不看过程只看结果，你根本不知道中间有个实习生插手过。这是一个**精确（Exact）** 的加速算法，而不是近似算法。

### 5. 本方案有什么局限性？与之前的方案相比哪些地方存在劣势？

虽然这个方法很聪明，但它也不是万能的。

1.  **对“实习生”要求高（Acceptance Rate）：**

    *   加速效果取决于实习生猜对的概率（接受率）。如果实习生太笨，猜 5 个错 5 个，那教授不仅要自己重写，还浪费了等实习生的时间。这种情况下，速度反而会变慢。

2.  **显存占用增加：**

    *   你需要同时在显存里加载两个模型（大模型 + 小模型）。虽然小模型通常很小，但在显存捉襟见肘的边缘设备上，这可能就是压死骆驼的稻草。

3.  **场景依赖：**

    *   在一些非常简单的任务上（比如抄写、常用语），实习生很容易猜对，加速极其明显（可能快 2-3 倍）。
    *   在一些极难的任务上（比如复杂的逻辑推理、生僻的代码生成），实习生大概率猜错，这时候加速效果就会大打折扣。

4.  **工程实现复杂度：**

    *   相比直接跑模型，你需要维护两套 KV Cache（缓存），还需要处理回退逻辑，工程实现的难度比单纯的模型量化要高一些。

### 总结

作为学生，你记住这篇论文的核心逻辑：**“用便宜的小模型打草稿，用昂贵的大模型做批改”**。

它成功地利用了现代 GPU **“算力过剩、带宽不足”** 的硬件特性，在保证**输出质量完全不变**的前提下，实现了“免费”的加速。这就是为什么它能成为经典的原因。

## 思考一

1.  为什么能够做并行验证？如果前面一个词实习生猜错了，后面不都是错的吗？
2.  展开说一下拒绝采样算法如何保证与教授单独写出来的完全一模一样？

***

这两个问题问得太好了！真的，这说明你不仅听进去了，而且在认真思考大模型底层的工作原理。这两个点恰恰是 Speculative Decoding 最反直觉、但也最精彩的地方。

咱们继续“拆解”。

***

### 问题一：为什么能够做并行验证？如果前面一个词实习生猜错了，后面不都是错的吗？

**核心误区纠正**：你可能认为“验证”也是像生成一样，必须看完第一个词，点个头，再看第二个词。 **事实是**：Transformer 模型有一种超能力，叫\*\*“一次看全貌”\*\*（只要数据已经在那了）。

#### 1. 为什么能并行计算？

回忆一下 Transformer 的训练过程。训练时，我们把整句话“我爱吃苹果”丢给模型。 模型会在内部**同时**做以下事情：

*   根据“我”，预测下一个词是“爱”的概率；
*   根据“我爱”，预测下一个词是“吃”的概率；
*   根据“我爱吃”，预测下一个词是“苹果”的概率。

这三个预测动作，是在\*\*一次前向传播（Forward Pass）\*\*里利用矩阵运算同时完成的，并不需要分三步走。这就是 Transformer 区别于老式 RNN 的最大优势。

#### 2. “实习生”与“教授”的配合流程

回到 Speculative Decoding，情况是这样的：

*   **第一步（实习生写草稿）**：实习生（小模型）必须串行，因为他要创造不存在的词。

    *   他写了：A, B, C。

*   **第二步（教授做批改）**：

    *   此时，A, B, C 已经存在了！

    *   我们将 `[Context] + [A, B, C]` 这一整串输入给教授（大模型）。

    *   教授运行**一次**，就能同时吐出三个位置的打分（Logits）：

        *   位置 1 的输出：根据 Context，A 是否合理？
        *   位置 2 的输出：根据 Context + A，B 是否合理？
        *   位置 3 的输出：根据 Context + A + B，C 是否合理？

#### 3. 针对你说的“连锁反应”问题

你说得很对：“如果 A 错了，那基于 A 预测的 B 肯定也是废的呀？”

是的，**B 确实是废的，但计算 B 的成本是可以被忽略的。**

我们来看判定逻辑：

1.  **检查 A**：教授算出概率，通过！ -> **保留 A**。

2.  **检查 B**：教授算出概率（这个概率是基于“Context + A”算出来的），发现不行，拒绝！ -> **丢弃 B**。

3.  **检查 C**：因为 B 已经被丢弃了，C 自然也就没用了，直接**无视/丢弃 C**。

**关键点在于：** 教授计算 A、B、C 的概率是**并发**的。虽然 B 和 C 最终可能被扔掉（算白算了），但因为计算它们是“顺便”的（GPU 并行计算特性），并没有额外花多少时间。

**结论**：我们要的不是“后面的也是对的”，而是“**快速找出第一个错的地方，并把前面全对的保留下来**”。哪怕实习生写了 5 个词，只要前 2 个是对的，我们就一次性赚到了 2 个词的时间。

***

### 问题二：拒绝采样算法如何保证与教授单独写出来的完全一模一样？

这个问题涉及到统计学里的**拒绝采样（Rejection Sampling）**。为了不讲枯燥的数学公式，我给你举个\*\*“编辑部”\*\*的例子。

#### 设定：

*   **教授（目标分布 P）**：严谨的大主编。

*   **实习生（草稿分布 Q）**：狂野的小作家。

*   **目标**：我们要发布的文章，必须**严格符合大主编的口味**。

#### 场景：选下一个词

实习生觉得下一个词应该是“苹果”，他很自信，给了 80% 的概率（$Q=0.8$）。 大主编觉得“苹果”一般般，他心里只有 40% 的概率（$P=0.4$）。

#### 算法逻辑（Speculative Sampling）：

**情况 A：实习生比教授怂（$Q \le P$）**

*   实习生说：“这个词...大概率是‘西瓜’（30%）？”

*   教授心里想：“其实我觉得‘西瓜’挺好的（50%）。”

*   **结果**：直接通过！

*   **原因**：实习生提出的概率比教授要求的还低，说明这个词实习生并没有“过分推荐”，可以放心接受。

**情况 B：实习生比教授狂（$Q > P$）**

*   实习生说：“一定是‘苹果’！我赌 80%！” ( $Q=0.8$ )

*   教授心里想：“我觉得是‘苹果’的概率只有 40%。” ( $P=0.4$ )

*   **问题**：如果每次都听实习生的，那“苹果”出现的频率就太高了（80%），不符合教授的品味（40%）。

*   **修正（拒绝采样）**：我们需要在这个词出现的时候，人为地\*\*“砍一刀”\*\*。

    *   我们设定一个接受概率： $\alpha = P / Q = 0.4 / 0.8 = 50\%$ 。

    *   我们掷一枚硬币，只有 50% 的机会保留“苹果”。

    *   **数学魔法**：实习生推荐频率 ( $0.8$ )  $\times$  接受率 ( $0.5$ ) = **$0.4$**。

    *   看到没？经过这一刀，最终“苹果”被保留下来的实际概率，正好变成了教授心中的 **$0.4$**！

#### 如果被拒绝了怎么办？

如果“苹果”被那 50% 的概率拒了，是不是就没词了？ 不是。这时候**教授会亲自出马**（Resampling）。

教授会说：“既然把你那个过分自信的‘苹果’拒了，那我们就从**你没考虑到的或者你低估了的词**里面选一个吧。” 教授会根据一个修正后的概率分布（$P - Q$ 的正差值部分）重新选一个词。这保证了即使拒绝了实习生，最终填补上来的词依然符合教授的整体分布。

#### 核心结论

通过这就套机制：

1.  **容易的词**（ $Q \approx P$ ）：实习生猜，大概率直接过。

2.  **过分自信的错误词**（ $Q > P$ ）：通过概率公式“打折”接受，把频率强行压回教授的水平。

3.  **完全离谱的词**：大概率被拒绝，教授亲自修正。

**最终效果**：无论实习生怎么胡闹，经过这个算法的过滤和修正，输出文本的每一个词的概率分布，从数学上推导，**严格等于**教授自己逐字生成的分布 $P(x)$。这就是为什么叫“无损（Lossless）”。

## 思考二

关于上面拒绝采样的解释，为什么需要一个接受概率，在Q>P时，直接使用教授的判定结果不就行了吗？

***

这是一个非常敏锐的问题！这触及到了“采样（Sampling）”和“搜索（Search）”最本质的区别。

你之所以会有“直接使用教授判定结果不就行了吗？”的疑问，很可能是因为你潜意识里把这个任务当成了\*\*“做选择题”**（谁分高选谁），但实际上这是一个**“抽奖”\*\*（要保证中奖率符合设计）。

我用一个\*\*“流量控制”\*\*的例子来帮你捅破这层窗户纸。

***

### 1. 核心矛盾：动作已经发生了

首先要明确一个前提：**当我们需要判定的时候，实习生（小模型）已经把词选出来了。**

*   **现状**：实习生已经把“苹果”这个词递到了你手里。

*   **统计事实**：在长期的工作中，实习生递给你“苹果”的频率是 **80%** ( $Q=0.8$ )。

*   **目标**：你希望最终发布的文章里，“苹果”出现的频率严格控制在 **40%** ( $P=0.4$ )。

### 2. 为什么不能“直接用教授的判定”？

你说的“直接用教授的判定”，通常有两种理解，我们看看为什么都不行：

#### 理解 A：如果教授觉得概率低 ($P < Q$)，就直接根据教授的分布重新生成？

也就是：只要实习生给的概率 (0.8) 高于教授心里的概率 (0.4)，我就**直接拒绝**，让教授重写。

*   **后果**：

    *   只要实习生拿到“苹果”，你就拒绝。

    *   这意味着通过“实习生推荐”这条路，你保留“苹果”的概率变成了 **0%**。

    *   **这就错了！** 教授是希望“苹果”出现的，只是不希望那么多，他希望是 40%，而不是 0%。

#### 理解 B：直接把这个词的概率改成教授的概率？

这在物理上是不可能的。

*   词已经生出来了，它是一个确定的**结果**（Token），而不是一个**数值**。

    *   你手里拿的是一个实体的“红球”。

    *   你不能把这颗“红球”变成“半颗红球”。

    *   你只能决定：**这颗球是“留”还是“丢”？**

### 3. 正解：必须做一个“过滤器”

既然实习生推销“苹果”太勤快了（80%），而我们只想要 40%，唯一的办法就是**按比例扔掉一部分**。

我们要设计一个**关卡（过滤器）**，这个关卡的通过率设为多少才合适呢？

$\text{最终概率} = \text{实习生拿来的概率} \times \text{关卡通过率}$

带入数值： $0.4 \text{ (教授想要的)} = 0.8 \text{ (实习生的)} \times \text{关卡通过率}$

所以： $\text{关卡通过率} = \frac{0.4}{0.8} = 0.5$

这就是为什么要用 $\frac{P}{Q}$ 作为接受概率的原因！

*   我们**掷骰子**（随机数），只有 50% 的时候让“苹果”通过。

*   这样一来，100 次里，实习生拿来 80 次“苹果”，我们扔掉一半，最终留下了 40 个“苹果”。

*   **完美！** 这正好符合教授心中 40% 的预期。

### 4. 总结

为什么需要接受概率 $\alpha = \min(1, P/Q)$？

因为我们不是在“找最正确的词”（那是 Greedy Search），我们是在**对齐两个概率分布**。

*   **$Q > P$的含义是**：实习生**过分殷勤**了，推荐这个词的次数**超过**了教授的预期。

*   **解决方案**：并不是彻底否定实习生（因为教授也想要这个词），而是\*\*“打折录取”\*\*。通过随机拒绝一部分，把实习生虚高的频率（Q），硬生生压低到教授的水平（P）。

这就是“无损（Lossless）”的数学魔法所在：虽然我们有时候拒绝了实习生，有时候接受了，但从宏观大数定律来看，最终输出的每一个词的出现频率，都严丝合缝地贴合了大模型（教授）的分布。
