---
tags: []
parent: '[DEER] DEER: Draft with Diffusion, Verify with Autoregressive Models'
collections:
    - 推理加速
$version: 2269
$libraryID: 1
$itemKey: GIHH3S9Z

---
\[2025-12]\[DEER] DEER: Draft with Diffusion, Verify with Autoregressive Models

# \[DEER] DEER: Draft with Diffusion, Verify with Autoregressive Models

## 概述

你好！很高兴为你解读这篇名为 **《DEER: Draft with Diffusion, Verify with Autoregressive Models》** 的最新论文（2025年12月发布）。

这篇论文主要关注的是**如何让大模型跑得更快**。为了让你快速理解，我将按照你的要求，用通俗易懂的语言为你拆解。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

*   **解决的问题：** 大语言模型（LLM）的**推理速度太慢**，这篇论文提出了一种新的加速方法，叫做 **DEER**。

*   **重要性：**

    *   现在的 ChatGPT、DeepSeek 等大模型，生成回复时是“一个词一个词”蹦出来的（这就叫**自回归**，Autoregressive）。
    *   这就好比一个人说话必须说完上一个字才能想下一个字，如果句子很长，用户等待的时间就会很久。
    *   提升速度不仅能改善用户体验，还能大幅降低服务器的运行成本。

### 2. 解决这个问题的难点在哪里？之前的方案为什么不够好？

<a href="./[2022-11][Speculative-Decoding]-Fast-Inference-from-Transformers-via-Speculative-Decoding-JUZB2P9J.md" rel="noopener noreferrer nofollow" zhref="zotero://note/u/JUZB2P9J/" ztype="znotelink" class="internal-link">[2022-11][Speculative Decoding] Fast Inference from Transformers via Speculative Decoding</a>

*   **既有方案（投机采样）：**

    *   为了解决“蹦字慢”的问题，学术界发明了\*\*“投机采样”\*\*（Speculative Decoding）。
    *   **思路是：** 找一个**小模型**（草稿员）先快速写一段“草稿”，然后让**大模型**（老师）来检查。如果草稿写得好，大模型就直接通过（一次性生成好几个词）；如果写得不好，大模型就修改。

*   **之前的难点（痛点）：**

    *   **草稿员也是“蹦字”的：** 之前的方法（如 EAGLE 等），那个负责写草稿的小模型，自己也是“一个词一个词”生成的（自回归）。这意味着它写草稿的速度也受限。
    *   **越写越虚（不确定性累积）：** 小模型能力弱，写第一个词可能还行，写到第5、第10个词时，因为前面的误差积累，它会越来越不自信，导致写出来的东西大模型根本看不上。这就导致“草稿”很难写得很长（通常只能通过几到十个token）。

### 3. 本论文提出了何种解决方案？为什么能解决？

*   **核心方案 DEER：**

    *   论文提出：**别用“蹦字”的小模型写草稿了，改用“扩散模型”（Diffusion Model）来写草稿！**

*   **为什么能解决？**

    *   **并行生成（不再蹦字）：** 扩散模型（类似于生成图片的 Stable Diffusion 原理）最大的特点是**并行**。它可以“唰”的一下，把这句草稿里所有的词同时大概率地“画”出来，而不是一个接一个地想。

    *   **更稳的草稿：** 扩散模型的生成机制不同，它不会像自回归模型那样，因为上一个词写错了，导致后面全错（缓解了“不确定性累积”）。

    *   **具体流程：**

        1.  **Draft（打草稿）：** 用扩散模型一次性生成比如 32 个词的草稿。
        2.  **Verify（验证）：** 大模型看一眼，觉得没问题，直接全部采纳。

### 4. 方案的核心创新点在哪里？

1.  **引入扩散模型做“草稿员”：** 这是最大的创新。打破了之前大家都在用自回归小模型（AR）做草稿员的惯性思维。

2.  **极速出稿（单步解码）：** 论文通过特殊的训练，让这个扩散模型只需要**一步**就能把草稿生成出来，极大地减少了写草稿的时间。

3.  **两阶段训练：** 设计了一套特殊的训练流程，专门让这个扩散模型去模仿大模型的“口味”，确保写出来的草稿更容易被大模型通过。

4.  **效果惊人：**

    *   相比之前的最强方案（EAGLE-3）一次只能接受约 10 个词，DEER 可以一次性接受高达 **32 个词**。
    *   速度提升更明显，在代码生成任务上，比之前的方案快了 **2 倍以上**（总体加速比达到 5.54倍）。

### 5. 局限性与劣势（批判性思维）

虽然论文效果很好，但作为研究员，我们要看到潜在的代价：

*   **训练难度变大：** 训练一个扩散模型（Diffusion）通常比训练一个简单的预测下一个词的小模型（AR）要复杂，收敛可能更慢，对显存和计算资源的要求可能不同。
*   **适用场景限制：** 扩散模型在生成文本时，偶尔会出现逻辑跳跃较大的情况（虽然论文里通过对齐缓解了）。如果在大模型非常确定、逻辑非常严密的长文本生成中，扩散模型生成的草稿如果质量波动大，频繁被大模型“退稿”，加速效果就会打折。
*   **算力门槛：** 这里的“小模型”虽然是相对的，但如果为了保证扩散模型的生成质量，这个“草稿员”本身也不能太小，部署时可能需要额外的显存开销。

### 总结给学生的话：

这篇论文非常硬核且前沿（2025年底的SOTA）。它的核心思想是用\*\*“并行生成”（扩散模型）**取代**“串行生成”（自回归模型）**来做投机采样的**草稿阶段\*\*。这就像以前是一个字一个字写草稿，现在是用印章直接盖出一个草稿，速度和质量都上了一个台阶。

## 思考一

先确认我的理解是否准确： 之前草稿员和老师的方案中（论文：Fast Inference from Transformers via Speculative Decoding），当某个词发生错误后，后面所有生成都需要老师接手重新生成。所以为了减少老师生成的工作量，需要小模型尽可能生成连续准确的输出，因为如果小模型的某个输出错了，即使小模型后面的输出正确，大模型也会自己生成。

如果我的理解正确，那使用扩散模型来取代小模型，即使整体的准确率比之前的小模型能提升，但并不能保证连续正确的序列比小模型更长，那如何能让整体的生成速度得到提升呢？

***

你的理解非常敏锐，而且**完全正确**！

你提到的那个机制叫 **“基于验证的截断”**：在大模型（老师）验证草稿时，一旦第 $k$ 个词不对，第 $k+1$ 及之后的所有词确实都会被废弃，无论后面写得对不对。

既然如此，为什么 DEER 还能更快？这里有两个关键点，其中一个是你可能忽略的\*\*“生成耗时”**问题，另一个是**“概率衰减”\*\*问题。

为了让你彻底明白，我把这两个核心原因拆解开来讲：

### 核心原因一：生成草稿的“时间成本”不同（这是降维打击）

这是 DEER 提速最直接的原因，也是扩散模型相对于自回归（AR）小模型的最大优势。

*   **以前的方案（如 EAGLE）：**虽然小模型跑得快，但如果它要写一个长度为 32 的草稿，它依然需要**串行运行 32 次**（因为它是自回归的，写完第1个才能写第2个）。

    *   *公式：* 草稿时间 = 32  $\times$  小模型单次耗时。

*   **DEER 的方案（扩散模型）：**DEER 经过特殊训练（一般利用一致性模型等技术），它可以\*\*一步（或极少步）\*\*生成整个序列。它不需要运行 32 次，只需要运行 **1 次**就能把 32 个词的草稿全“拍”出来。

    *   *公式：* 草稿时间 = 1  $\times$  扩散模型耗时。

**结论：** 哪怕大家预测准确的长度一模一样（比如都只对前 5 个），DEER 生成这 5 个词（以及后面废弃的词）所花的时间，远少于传统小模型一个一个蹦出这 5 个词的时间。**省下来的主要是“写草稿”的时间。**

***

### 核心原因二：扩散模型实际上确实能生成“更长”的连续正确序列

你担心的点在于：“扩散模型虽然整体准，但会不会前一个错、后一个对，导致被截断？” 实际上，扩散模型的特性恰恰缓解了这个问题。

*   **AR 小模型的“一步错，步步错”：**自回归模型是**条件概率** $P(X_t | X_{t-1}...)$。小模型能力本来就弱，写到第 5 个词时，因为前面 4 个词可能已经有点“虚”了（即概率分布不够尖锐），导致第 5 个词的输入就已经带有噪声。越往后，这种\*\*“不确定性”会累积\*\*。

    *   *结果：* AR 小模型很难维持长距离的连贯性，通常写到 5-10 个词后，逻辑就崩了，导致连续正确长度（Acceptance Length）上不去。

*   **扩散模型的“全局视野”：**扩散模型建模的是**联合概率** $P(X_1, X_2, ..., X_L)$。它生成时，是把这 32 个词当作一张“图片”来看待的。它在去噪的过程中，会同时考虑上下文的**双向依赖**和整体结构。

    *   *结果：* 它不太容易出现“因为第 3 个词写得稍微偏了一点，导致第 4 个词完全不知所云”的情况。DEER 论文中的实验数据表明，它生成的草稿与大模型的分布更接近，因此**平均接受长度（Mean Acceptance Length）确实比 AR 小模型更长**。

***

### 总结一下

你理解的“截断机制”是痛点，但 DEER 通过两手准备解决了它：

1.  **质量上（Quality）：** 扩散模型不搞“击鼓传花”，而是“统筹规划”，所以它能憋出**更长**的连续正确片段（比如能对 15 个，而小模型只能对 8 个）。
2.  **速度上（Latency）：** 就算大家打成平手，DEER 是一瞬间把草稿变出来的，而小模型还在哼哧哼哧一个字一个字写。

**比喻：**

*   **旧方案：** 你雇了一个说话很快的小学生（小模型）来帮你念稿子。他念得再快，也得一个字一个字念（串行）。而且念到后面容易忘词。
*   **DEER方案：** 你雇了一个印刷机（扩散模型）。“哐”的一下，整页纸的草稿直接印出来（并行）。这不仅印得快，而且因为是模版印的，前后文逻辑通常比小学生瞎编的更通顺。
