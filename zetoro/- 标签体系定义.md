- **模型结构 (Model Architecture)**
    - **核心组件 (Core Components)**：如何改进 Transformer 的基础单元（如注意力机制、位置编码、激活函数），以提升模型的表达能力或训练稳定性？
    - **长上下文 (Long Context)**：如何让模型处理超长文本（如整本书），解决位置编码外推能力不足或注意力机制复杂度过高的问题？
    - **稀疏化/MoE (Sparse/MoE)**：如何在极大幅度增加模型参数量（提升容量）的同时，保持推理和训练的计算成本（FLOPs）不显著增加？

- **预训练 (Pre-training)**
    - **扩展定律 (Scaling Laws)**：模型性能与参数量、数据量、计算量之间存在怎样的数学关系？如何根据有限的算力预算制定最优的训练计划？
    - **学习范式 (Paradigm)**：大模型在什么条件下会涌现出 Few-shot 或 In-context Learning 能力？模型是如何从无监督数据中习得通用知识的？

- **后训练 (Post-training)**
    - **对齐 (Alignment)**：如何让模型的回答不仅仅是“续写”，而是符合人类的价值观（有用、安全、诚实）？如何利用偏好数据（A比B好）来优化模型？
    - **PEFT (参数高效微调)**：如何在显存有限或需要适配多个下游任务时，只调整极少量的参数（如 <1%）就能达到全量微调的效果？
    - **SFT策略 (SFT Strategy)**：在监督微调阶段，如何筛选高质量的指令数据？如何设计训练策略以避免模型死记硬背（过拟合）或遗忘通用能力？

- **训练系统 (Training System)**
    - **并行计算 (Parallelism)**：当单卡甚至单机无法容纳模型时，如何将模型切分（张量并行、流水线并行）并高效地在数千张 GPU 上协同计算？
    - **显存优化 (Memory Opt)**：如何通过切分状态（ZeRO）、卸载（Offload）或重计算（Checkpointing）等技术，打破单卡显存墙的限制？

- **推理系统 (Inference System)**
    - **服务架构 (Serving Arch)**：在服务端面临高并发请求时，如何设计调度策略（如分离 Prefill 和 Decode 阶段）以最大化系统的吞吐量（Goodput）？
    - **显存/缓存管理 (Memory/Cache Opt)**：如何解决推理过程中 KV Cache 占用显存过大且碎片化的问题（如 PagedAttention），从而支持更大的 Batch Size？
    - **推理加速 (Speed Opt)**：如何在不牺牲质量的前提下加快 Token 的生成速度（如投机采样），或通过量化、算子融合降低延迟？

- **应用与智能体 (App & Agents)**
    - **智能体 (Agents)**：如何让模型具备“手”和“脚”，学会规划任务、调用工具、反思错误，从而独立解决复杂问题？
    - **提示工程 (Prompting)**：在不修改模型参数的前提下，如何设计输入文本（如思维链 CoT、ReAct），激发模型的潜在推理能力？
    - **RAG (检索增强生成)**：如何让模型利用外部知识库（向量数据库、知识图谱），解决幻觉问题并回答它未见过的私有数据？


论文列表如下：
Attention Is All You Need
[Few-shot] Language Models are Few-Shot Learners
Scaling Laws for Neural Language Models
[RLHF] Training language models to follow instructions with human feedback
LoRA: Low-Rank Adaptation of Large Language Models
LLM Powered Autonomous Agents
ReAct: Synergizing Reasoning and Acting in Language Models
Generative Agents: Interactive Simulacra of Human Behavior
[RoPE] RoFormer: Enhanced Transformer with Rotary Position Embedding
[YaRN] YaRN: Efficient Context Window Extension of Large Language Models
[GQA] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
[SwiGLU] GLU Variants Improve Transformer
[MoE] Mixtral of Experts
[Megatron-LM] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
[ZeRO] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models
[Scaling Laws] Training Compute-Optimal Large Language Models
[DPO] Direct Preference Optimization: Your Language Model is Secretly a Reward Model
[NEFTune] NEFTune: Noisy Embeddings Improve Instruction Finetuning
[PagedAttention/vLLM] Efficient Memory Management for Large Language Model Serving with PagedAttention
[PD分离] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving
[Speculative Decoding] Fast Inference from Transformers via Speculative Decoding
[GraphRAG] From Local to Global: A Graph RAG Approach to Query-Focused Summarization