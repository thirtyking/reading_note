# 论文分类
1. 基于大模型的知识体系架构，对以下论文进行分类，分类层级为2级
1. 给每篇论文打上**最多2个**第2级分类的标签

## 论文列表如下：
[2017-06][Transformer]-Attention-Is-All-You-Need-RLSRSZK6.md
[2019-09][Megatron-LM]-Megatron-LM--Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism-YS6U6GSX.md
[2019-10][ZeRO]-ZeRO--Memory-Optimizations-Toward-Training-Trillion-Parameter-Models-6YK6KGB9.md
[2020-01][Scaling-Laws]-Scaling-Laws-for-Neural-Language-Models-W4V7KTID.md
[2020-02][SwiGLU]-GLU-Variants-Improve-Transformer-7599PQWC.md
[2020-05][Few-Shot]-Language-Models-are-Few-Shot-Learners-HGJ5AFJL.md
[2021-03][CLIP]-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision-AL6LDFDM.md
[2021-04][RoPE]-RoFormer--Enhanced-Transformer-with-Rotary-Position-Embedding-D3S8URW6.md
[2021-06][LoRA]-LoRA--Low-Rank-Adaptation-of-Large-Language-Models-XZH8M6F4.md
[2022-02][ROME]-Locating-and-Editing-Factual-Associations-in-GPT-F2CIDU62.md
[2022-03][Chinchilla]-Training-Compute-Optimal-Large-Language-Models-2TU83KZ9.md
[2022-03][RLHF]-Training-language-models-to-follow-instructions-with-human-feedback-BKTCEVNE.md
[2022-03][STaR]-STaR--Bootstrapping-Reasoning-With-Reasoning-FWNRTN29.md
[2022-09][diffusion-survey]-Diffusion-Models--A-Comprehensive-Survey-of-Methods-and-Applications-72GR2YXF.md
[2022-10][ReAct]-ReAct--Synergizing-Reasoning-and-Acting-in-Language-Models-X4A9ZG5C.md
[2022-11][Speculative-Decoding]-Fast-Inference-from-Transformers-via-Speculative-Decoding-JUZB2P9J.md
[2023-03][Reflexion]-Reflexion--Language-Agents-with-Verbal-Reinforcement-Learning-2SMNNHEX.md
[2023-03][Self-Refine]-Self-Refine--Iterative-Refinement-with-Self-Feedback-6GFQX3MW.md
[2023-03][SigLIP]-Sigmoid-Loss-for-Language-Image-Pre-Training-AUNE36J5.md
[2023-04][斯坦福小镇]-Generative-Agents--Interactive-Simulacra-of-Human-Behavior-JB86VHH6.md
[2023-05][DPO]-Direct-Preference-Optimization--Your-Language-Model-is-Secretly-a-Reward-Model-CPXS6W5G.md
[2023-05][GQA]-GQA--Training-Generalized-Multi-Query-Transformer-Models-from-Multi-Head-Checkpoints-4Q76SH7N.md
[2023-05][Voyager]-Voyager--An-Open-Ended-Embodied-Agent-with-Large-Language-Models-4L5KBN9U.md
[2023-06][Agent入门]LLM-Powered-Autonomous-Agents-ZCGVRRYI.md
[2023-06]State-of-GPT-THCR4LF8.md
[2023-09][PagedAttention-vLLM]-Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention-T4FAGSCN.md
[2023-09][SAE]-Sparse-Autoencoders-Find-Highly-Interpretable-Features-in-Language-Models-3TIAZD5H.md
[2023-09][YaRN]-YaRN--Efficient-Context-Window-Extension-of-Large-Language-Models-W4VILBN4.md
[2023-10][NEFTune]-NEFTune--Noisy-Embeddings-Improve-Instruction-Finetuning-JAMD6IXD.md
[2024-01][DS-Coder]-DeepSeek-Coder--When-the-Large-Language-Model-Meets-Programming----The-Rise-of-Code-Intelligence-3LUUP9S4.md
[2024-01][DS-LLM]-DeepSeek-LLM--Scaling-Open-Source-Language-Models-with-Longtermism-4BNYVQMJ.md
[2024-01][MoE]-Mixtral-of-Experts-WM4IYMS3.md
[2024-01][PD分离]-DistServe--Disaggregating-Prefill-and-Decoding-for-Goodput-optimized-Large-Language-Model-Serving-XXNTLH95.md
[2024-01][Self-Play]-Self-Play-Fine-Tuning-Converts-Weak-Language-Models-to-Strong-Language-Models-JPVT886T.md
[2024-01][Self-Rewarding]-Self-Rewarding-Language-Models-H592247Z.md
[2024-02][DS-Math]-DeepSeekMath--Pushing-the-Limits-of-Mathematical-Reasoning-in-Open-Language-Models-RJW3TI8J.md
[2024-03][DS-VL]-DeepSeek-VL--Towards-Real-World-Vision-Language-Understanding-NSGAFJZL.md
[2024-04][GraphRAG]-From-Local-to-Global--A-Graph-RAG-Approach-to-Query-Focused-Summarization-MIPE8G3T.md
[2024-07][Agent-RI]-Recursive-Introspection--Teaching-Language-Model-Agents-How-to-Self-Improve-2GV3CNN9.md
[2024-09][HC]-Hyper-Connections-BIRLCCH3.md
[2025-08][Memory-R1]-Memory-R1--Enhancing-Large-Language-Model-Agents-to-Manage-and-Utilize-Memories-via-Reinforcement-L-XXYAWEIL.md
[2025-11][通义Agent]AgentEvolver--Towards-Efficient-Self-Evolving-Agent-System-22LJHKWM.md
[2025-12][DEER]-DEER--Draft-with-Diffusion,-Verify-with-Autoregressive-Models-GIHH3S9Z.md
[2025-12][mHC]-mHC--Manifold-Constrained-Hyper-Connections-5FEIWN7F.md
[2025-12][SAE-Latent-Attribution]-Debugging-misaligned-completions-with-sparse-autoencoder-latent-attribution-E5BU4YHN.md
[2025-12][Survey]-Memory-in-the-Age-of-AI-Agents-LJG2Q2EL.md

# 分类结果
基于大模型的知识体系架构，我将这些论文分为 **6个一级分类** 和 **16个二级分类**。每篇论文根据其核心贡献标注了最多2个二级分类标签。
---

### 一、 知识体系分类架构 (2级)

1.  **核心架构与底层组件 (Core Architecture & Components)**
    *   1.1 基础模型架构 (Transformer/MoE/连接结构)
    *   1.2 注意力机制与核心组件 (Attention/Activation Functions)
    *   1.3 位置编码与长文本扩展 (Positional Encoding & Context Extension)
2.  **训练理论与工程系统 (Training Theory & Systems)**
    *   2.1 缩放法则与计算最优 (Scaling Laws)
    *   2.2 分布式训练与内存优化 (Parallelism & Optimization)
    *   2.3 高效微调与对齐技术 (PEFT & Alignment Training)
3.  **能力演进与特定领域 (Capabilities & Specialized Domains)**
    *   3.1 逻辑推理 (数学/代码/推理链)
    *   3.2 多模态学习 (Vision-Language/Diffusion)
4.  **智能体框架与交互 (Agentic Frameworks & Interaction)**
    *   4.1 智能体规划与反思 (Planning, Reasoning & Acting)
    *   4.2 智能体记忆与自我演化 (Memory & Self-Evolution)
    *   4.3 智能体系统与社会模拟 (Agent Systems & Simulation)
5.  **推理加速与部署服务 (Inference & Serving)**
    *   5.1 推理优化与解码技术 (Speculative Decoding/Quantization)
    *   5.2 推理系统与服务架构 (Serving Systems/VRAM Management)
6.  **可解释性、安全与编辑 (Interpretability & Safety)**
    *   6.1 机制可解释性 (Mechanistic Interpretability/SAE)
    *   6.2 知识编辑与安全 (Model Editing & Safety)

---

### 二、 论文分类清单

| 论文简标 | 论文标题 | 一级分类 | 二级分类标签 (最多2个) |
| :--- | :--- | :--- | :--- |
| **[Transformer]** | Attention Is All You Need | 核心架构 | 1.1 基础模型架构, 1.2 注意力机制 |
| **[Megatron-LM]** | Training Multi-Billion Parameter... | 训练工程 | 2.2 分布式训练与内存优化 |
| **[ZeRO]** | Memory Optimizations Toward... | 训练工程 | 2.2 分布式训练与内存优化 |
| **[Scaling-Laws]** | Scaling Laws for Neural LM | 训练工程 | 2.1 缩放法则与计算最优 |
| **[SwiGLU]** | GLU Variants Improve Transformer | 核心架构 | 1.2 注意力机制与核心组件 |
| **[Few-Shot]** | Language Models are Few-Shot Learners | 核心架构 | 1.1 基础模型架构, 2.1 缩放法则 |
| **[CLIP]** | Learning Transferable Visual Models... | 能力演进 | 3.2 多模态学习 |
| **[RoPE]** | RoFormer: Rotary Position Embedding | 核心架构 | 1.3 位置编码与长文本扩展 |
| **[LoRA]** | Low-Rank Adaptation of LLMs | 训练工程 | 2.3 高效微调与对齐技术 |
| **[ROME]** | Locating and Editing Factual... | 可解释性 | 6.1 机制可解释性, 6.2 知识编辑 |
| **[Chinchilla]** | Training Compute-Optimal LLMs | 训练工程 | 2.1 缩放法则与计算最优 |
| **[RLHF]** | Training LMs to follow instructions | 训练工程 | 2.3 高效微调与对齐技术 |
| **[STaR]** | Bootstrapping Reasoning With... | 能力演进 | 3.1 逻辑推理, 2.3 高效微调 |
| **[diffusion-survey]** | Diffusion Models: A Survey | 能力演进 | 3.2 多模态学习 |
| **[ReAct]** | Synergizing Reasoning and Acting | 智能体 | 4.1 智能体规划与反思, 3.1 逻辑推理 |
| **[Speculative-Decoding]** | Fast Inference via Speculative... | 推理加速 | 5.1 推理优化与解码技术 |
| **[Reflexion]** | Agents with Verbal Reinforcement | 智能体 | 4.1 智能体规划与反思 |
| **[Self-Refine]** | Iterative Refinement with Self-Feedback | 智能体 | 4.1 智能体规划与反思 |
| **[SigLIP]** | Sigmoid Loss for Language-Image | 能力演进 | 3.2 多模态学习 |
| **[斯坦福小镇]** | Generative Agents: Simulacra... | 智能体 | 4.3 智能体系统与社会模拟 |
| **[DPO]** | Direct Preference Optimization | 训练工程 | 2.3 高效微调与对齐技术 |
| **[GQA]** | GQA: Generalized Multi-Query... | 核心架构 | 1.2 注意力机制与核心组件 |
| **[Voyager]** | An Open-Ended Embodied Agent | 智能体 | 4.2 智能体记忆与自我演化, 4.3 智能体系统 |
| **[Agent入门]** | LLM Powered Autonomous Agents | 智能体 | 4.1 智能体规划与反思, 4.2 智能体记忆 |
| **[State-of-GPT]** | State of GPT (Andrej Karpathy) | 训练工程 | 2.3 高效微调与对齐技术 |
| **[PagedAttention]** | Efficient Memory Management (vLLM) | 推理加速 | 5.2 推理系统与服务架构, 1.2 注意力机制 |
| **[SAE]** | Sparse Autoencoders Find Features | 可解释性 | 6.1 机制可解释性 |
| **[YaRN]** | Efficient Context Window Extension | 核心架构 | 1.3 位置编码与长文本扩展 |
| **[NEFTune]** | Noisy Embeddings Improve Finetuning | 训练工程 | 2.3 高效微调与对齐技术 |
| **[DS-Coder]** | DeepSeek-Coder | 能力演进 | 3.1 逻辑推理 |
| **[DS-LLM]** | DeepSeek-LLM | 核心架构 | 1.1 基础模型架构, 2.1 缩放法则 |
| **[MoE]** | Mixtral of Experts | 核心架构 | 1.1 基础模型架构 |
| **[PD分离]** | DistServe: Disaggregating Prefill... | 推理加速 | 5.2 推理系统与服务架构 |
| **[Self-Play]** | Self-Play Fine-Tuning (SPIN) | 训练工程 | 2.3 高效微调与对齐技术 |
| **[Self-Rewarding]** | Self-Rewarding Language Models | 训练工程 | 2.3 高效微调与对齐技术, 4.2 自我演化 |
| **[DS-Math]** | DeepSeekMath | 能力演进 | 3.1 逻辑推理 |
| **[DS-VL]** | DeepSeek-VL | 能力演进 | 3.2 多模态学习 |
| **[GraphRAG]** | From Local to Global: Graph RAG | 智能体 | 4.2 智能体记忆与自我演化 |
| **[Agent-RI]** | Recursive Introspection | 智能体 | 4.1 智能体规划与反思, 4.2 智能体记忆 |
| **[HC]** | Hyper-Connections | 核心架构 | 1.1 基础模型架构 |
| **[Memory-R1]** | Memory-R1: LLM Agents Memory | 智能体 | 4.2 智能体记忆与自我演化 |
| **[通义Agent]** | AgentEvolver | 智能体 | 4.2 智能体记忆与自我演化 |
| **[DEER]** | Draft with Diffusion, Verify with AR | 推理加速 | 5.1 推理优化与解码技术, 3.2 多模态学习 |
| **[mHC]** | Manifold Constrained Hyper-Conn | 核心架构 | 1.1 基础模型架构 |
| **[SAE-Latent]** | Debugging with SAE Latent Attribution | 可解释性 | 6.1 机制可解释性 |
| **[Survey]** | Memory in the Age of AI Agents | 智能体 | 4.2 智能体记忆与自我演化 |