---
tags: []
parent: '[RLHF] Training language models to follow instructions with human feedback'
collections:
    - 对齐
$version: 2251
$libraryID: 1
$itemKey: BKTCEVNE

---
\[2022-03]\[RLHF] Training language models to follow instructions with human feedback

# \[RLHF] Training language models to follow instructions with human feedback

## 概述

你好！非常高兴能为你解读这篇论文：《Training language models to follow instructions with human feedback》，也就是大名鼎鼎的 **InstructGPT**，它是 **ChatGPT** 的孪生兄弟和技术前身。

如果说 GPT-3 是一个“博学但由于不懂规矩而经常胡言乱语的天才”，那么这篇论文就是要把这个天才送进“礼仪学校”，把他调教成一个“彬彬有礼、言听计从的得力助手”。

咱们直入正题。

***

### 1. 本论文要解决什么问题？为什么这个问题很重要？

**核心问题：如何解决“对齐”（Alignment）问题？即让 AI 的目标与人类的真实意图保持一致。**

**背景：** 在 GPT-3 时代，虽然模型读了很多书，但它本质上只会做一件事：**“预测下一个字”**。 如果你给它一个指令：“请把这段话翻译成英文。” GPT-3 可能会以为你在玩文字接龙，它可能会接着续写：“请把这段话翻译成法文。请把这段话翻译成德文……” 它并没有真正**理解并执行**你的命令，它只是在机械地模仿它在训练数据里见过的文本格式。

**重要性：** 如果不解决这个问题，模型越大，它可能只是在“错误的道路上狂奔”。它会生成更有说服力的谎言（幻觉），或者输出有害的、带有偏见的内容。我们要的不是一个只会“瞎编”的复读机，而是一个能真正**遵循指令（Follow Instructions）**、**有用（Helpful）**、**诚实（Honest）** 且 **无害（Harmless）** 的助手（这也就是著名的 3H 原则）。

***

### 2. 解决这个问题的难点在哪里？之前的方案为何无效？

**难点：人类的意图很难用数学公式（Loss Function）来定义。**

*   **以前的方案：** 也就是我们之前讲的 GPT-3 的方案——**不仅要把模型做大，还要喂更多的数据**。

*   **为什么无效：**

    1.  **目标不匹配（Objective Mismatch）：** 预训练的目标是“猜下一个词准确率高”，而人类的目标是“回答得好”。这两个目标经常是冲突的。

    2.  **无法量化：** 比如“请写一首感人的诗”。什么是“感人”？计算机不知道。你没法写一个代码来自动判断一首诗是否感人。

    3.  **规模失效：** 论文中有一个惊人的发现：简单的把 GPT-3 变大，并不能让它更听话。甚至在某些情况下，模型越大，因为它越自信，反而越容易一本正经地胡说八道。

***

### 3. 本论文提出了何种解决方案？为什么能解决？

**解决方案：RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）。**

这篇论文的核心思想是：**既然机器不知道什么是“好”，那我们就让人类老师亲自来打分，教机器什么是“好”。**

**具体步骤（三部曲）：**

1.  **找老师示范（SFT）：** 先找人写好“问题”和标准的“答案”，让模型照着学。

2.  **让老师打分（Reward Model）：** 让模型对一个问题生成好几个不同的回答，让人来给这些回答排名（比如 A 比 B 好）。机器通过学习这些排名，学会了“判卷子”，建立了一个**奖励模型（Reward Model）**。

3.  **让模型刷题（PPO）：** 让模型自己做新题，然后用上面的“奖励模型”给它打分。如果分数高，就奖励它；分数低，就惩罚它。这用到了**强化学习（Reinforcement Learning）**。

**为什么能解决？** 因为它引入了\*\*“人的主观判断”\*\*作为信号。通过这套流程，我们将无法量化的“人类偏好”，转化为了机器可以优化的“数学分数”。

***

### 4. 方案的核心创新点在哪里？

1.  **确立了 RLHF 的技术路线：** 虽然 RLHF 这个概念不是这篇论文发明的，但它是第一次成功地在大语言模型上大规模应用并取得惊人效果的。它证明了**强化学习**可以用来微调语言模型。

2.  **以小博大：** 这是最震撼的一点。论文通过评测发现，**仅有 13 亿参数的 InstructGPT，在人类喜欢程度上，打败了 1750 亿参数的 GPT-3。** 这说明，“懂规矩”比“脑容量大”更重要。

3.  **数据效率：** 它发现了一个秘密——让人类写完美的答案很累（SFT），但让人类给两个答案\*\*排序（Ranking）\*\*却很快很简单。利用排序数据来训练奖励模型，极大地提高了人类反馈的利用效率。

***

### 5. 关键架构图解读

这篇论文中最著名的是那张\*\*“三阶段训练图”\*\*（Three Steps of training InstructGPT）。这可以说是现代 AI 助手诞生的“出生证明”。

#### **图的内容描述：**

这张图分为左、中、右三列，分别代表三个步骤：

*   **第一步（Step 1）：SFT（Supervised Fine-Tuning，有监督微调）**

    *   **图示：** 一个人类标注员在看 Prompt（提示），并写出完美的 Answer（答案）。

    *   **含义：** 这是\*\*“言传身教”\*\*。通过少量高质量的人工手写数据，先让 GPT-3 学会“原来我需要回答问题，而不是续写”。

*   **第二步（Step 2）：Reward Model（奖励模型训练）**

    *   **图示：** 模型对同一个 Prompt 生成了 A、B、C、D 四个回答。人类标注员对它们进行**排序**（比如 D > C > A = B）。

    *   **含义：** 这是\*\*“建立家规”\*\*。通过排序，训练一个能模拟人类口味的裁判员（Reward Model）。它不生成文本，只负责打分。

*   **第三步（Step 3）：PPO（Proximal Policy Optimization，强化学习优化）**

    *   **图示：** 这是一个循环图。模型生成回答 -> 奖励模型打分 -> 根据分数更新模型参数。

    *   **含义：** 这是\*\*“自我修炼”\*\*。模型通过不断的尝试，试图去讨好那个“奖励模型”以获得高分。最终，它变成了 InstructGPT。

#### **如何辅助理解方法论：**

这张图清晰地展示了**数据流向**和**人类介入**的节点：

*   人类不仅是数据的提供者（Step 1），更是价值观的裁判者（Step 2）。
*   它解释了为什么我们可以不用一直让人盯着模型改——因为我们在 Step 2 训练了一个\*\*“电子裁判”\*\*（奖励模型），在 Step 3 里，这个电子裁判可以不知疲倦地指导模型训练。

***

### 总结一下给学生的话：

InstructGPT 这篇论文告诉我们：**光有“知识”（Pre-training）是不够的，还需要有“教养”（Alignment）。**

通过引入**人类反馈（Human Feedback）**，OpenAI 成功地将大模型从一个不可控的“文本生成器”变成了一个听话的“智能助手”。这个 **RLHF** 的流程，直到今天依然是训练 ChatGPT、Llama 等所有主流大模型的核心秘籍。
